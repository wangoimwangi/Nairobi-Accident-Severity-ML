{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d065c3ff-2827-42f8-8e2b-e041bc386b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA PREPROCESSING: MACHINE LEARNING PREPARATION\n",
      "================================================================================\n",
      "Start Time: 2026-01-28 16:36:53\n",
      "================================================================================\n",
      "\n",
      " Loading processed dataset with features...\n",
      " Dataset loaded successfully!\n",
      "   Shape: (31064, 36)\n",
      "   Records: 31,064\n",
      "   Features: 36\n",
      "\n",
      " Data Quality Check:\n",
      "   Missing values: 0\n",
      "   Duplicate rows: 0\n",
      "\n",
      " Target variable distribution:\n",
      "severity\n",
      "MINOR       25059\n",
      "FATAL        2284\n",
      "MODERATE     2121\n",
      "SEVERE       1600\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      " DATA LOADED - READY FOR PREPROCESSING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPROCESSING: Preparing Features for Machine Learning\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA PREPROCESSING: MACHINE LEARNING PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== LOAD PROCESSED DATASET ==========\n",
    "\n",
    "print(\"\\n Loading processed dataset with features...\")\n",
    "\n",
    "crashes = pd.read_csv(r'D:\\Nairobi-Accident-Severity\\data\\processed\\crashes_with_features.csv')\n",
    "\n",
    "print(f\" Dataset loaded successfully!\")\n",
    "print(f\"   Shape: {crashes.shape}\")\n",
    "print(f\"   Records: {len(crashes):,}\")\n",
    "print(f\"   Features: {len(crashes.columns)}\")\n",
    "\n",
    "# ========== VERIFY DATA QUALITY ==========\n",
    "\n",
    "print(\"\\n Data Quality Check:\")\n",
    "print(f\"   Missing values: {crashes.isnull().sum().sum()}\")\n",
    "print(f\"   Duplicate rows: {crashes.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n Target variable distribution:\")\n",
    "print(crashes['severity'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DATA LOADED - READY FOR PREPROCESSING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44d50a6-2f50-4df0-a248-0becd7e26d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANALYZING FEATURE TYPES FOR PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      " Analyzing all features...\n",
      "\n",
      " Data types:\n",
      "crash_id                         int64\n",
      "crash_datetime                  object\n",
      "crash_date                      object\n",
      "latitude                       float64\n",
      "longitude                      float64\n",
      "n_crash_reports                  int64\n",
      "contains_fatality_words          int64\n",
      "contains_pedestrian_words        int64\n",
      "contains_matatu_words            int64\n",
      "contains_motorcycle_words        int64\n",
      "severity                        object\n",
      "hour                             int64\n",
      "day_of_week                      int64\n",
      "day_name                        object\n",
      "month                            int64\n",
      "month_name                      object\n",
      "year                             int64\n",
      "is_morning_rush                   bool\n",
      "is_evening_rush                   bool\n",
      "is_rush_hour                      bool\n",
      "is_weekend                        bool\n",
      "time_of_day                     object\n",
      "distance_from_center_km        float64\n",
      "distance_category               object\n",
      "lat_grid                       float64\n",
      "lon_grid                       float64\n",
      "location_grid                   object\n",
      "crashes_at_location              int64\n",
      "is_hotspot                        bool\n",
      "frequency_category              object\n",
      "severity_numeric                 int64\n",
      "avg_severity_at_location       float64\n",
      "max_severity_at_location         int64\n",
      "fatal_rate_at_location         float64\n",
      "pedestrian_rate_at_location    float64\n",
      "location_risk                   object\n",
      "dtype: object\n",
      "\n",
      "================================================================================\n",
      "FEATURE CATEGORIZATION\n",
      "================================================================================\n",
      "\n",
      " Feature Categories:\n",
      "\n",
      "   Features to DROP: 8\n",
      "      â€¢ crash_id\n",
      "      â€¢ crash_datetime\n",
      "      â€¢ crash_date\n",
      "      â€¢ day_name\n",
      "      â€¢ month_name\n",
      "      â€¢ lat_grid\n",
      "      â€¢ lon_grid\n",
      "      â€¢ location_grid\n",
      "\n",
      "   TARGET variable: 1\n",
      "      â€¢ severity\n",
      "\n",
      "   CATEGORICAL features (need encoding): 4\n",
      "      â€¢ time_of_day\n",
      "      â€¢ distance_category\n",
      "      â€¢ frequency_category\n",
      "      â€¢ location_risk\n",
      "\n",
      "   NUMERICAL features (need normalization): 14\n",
      "      â€¢ latitude\n",
      "      â€¢ longitude\n",
      "      â€¢ n_crash_reports\n",
      "      â€¢ hour\n",
      "      â€¢ day_of_week\n",
      "      â€¢ month\n",
      "      â€¢ year\n",
      "      â€¢ distance_from_center_km\n",
      "      â€¢ crashes_at_location\n",
      "      â€¢ severity_numeric\n",
      "      â€¢ avg_severity_at_location\n",
      "      â€¢ max_severity_at_location\n",
      "      â€¢ fatal_rate_at_location\n",
      "      â€¢ pedestrian_rate_at_location\n",
      "\n",
      "   BOOLEAN features (already 0/1): 9\n",
      "      â€¢ contains_fatality_words\n",
      "      â€¢ contains_pedestrian_words\n",
      "      â€¢ contains_matatu_words\n",
      "      â€¢ contains_motorcycle_words\n",
      "      â€¢ is_morning_rush\n",
      "      â€¢ is_evening_rush\n",
      "      â€¢ is_rush_hour\n",
      "      â€¢ is_weekend\n",
      "      â€¢ is_hotspot\n",
      "\n",
      " Total features accounted for: 36\n",
      "   Original features in dataset: 36\n",
      "    All features categorized!\n",
      "\n",
      "================================================================================\n",
      " FEATURE ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE TYPE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYZING FEATURE TYPES FOR PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== IDENTIFY FEATURE TYPES ==========\n",
    "\n",
    "print(\"\\n Analyzing all features...\")\n",
    "\n",
    "# Display data types\n",
    "print(\"\\n Data types:\")\n",
    "print(crashes.dtypes)\n",
    "\n",
    "# ========== CATEGORIZE FEATURES ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE CATEGORIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Features to DROP (not needed for ML)\n",
    "drop_features = [\n",
    "    'crash_id',           # Unique identifier (not predictive)\n",
    "    'crash_datetime',     # Already extracted as features\n",
    "    'crash_date',         # Already extracted as features\n",
    "    'day_name',           # Redundant with day_of_week\n",
    "    'month_name',         # Redundant with month\n",
    "    'lat_grid',           # Redundant with location_grid\n",
    "    'lon_grid',           # Redundant with location_grid\n",
    "    'location_grid'       # Already captured in aggregated features\n",
    "]\n",
    "\n",
    "# TARGET variable\n",
    "target = 'severity'\n",
    "\n",
    "# CATEGORICAL features (need encoding)\n",
    "categorical_features = [\n",
    "    'time_of_day',        # Morning/Afternoon/Evening/Night\n",
    "    'distance_category',  # 0-5km, 5-10km, etc.\n",
    "    'frequency_category', # Isolated/Low/Moderate/High\n",
    "    'location_risk'       # Low/Moderate/High/Very High Risk\n",
    "]\n",
    "\n",
    "# NUMERICAL features (need normalization)\n",
    "numerical_features = [\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'n_crash_reports',\n",
    "    'hour',\n",
    "    'day_of_week',\n",
    "    'month',\n",
    "    'year',\n",
    "    'distance_from_center_km',\n",
    "    'crashes_at_location',\n",
    "    'severity_numeric',\n",
    "    'avg_severity_at_location',\n",
    "    'max_severity_at_location',\n",
    "    'fatal_rate_at_location',\n",
    "    'pedestrian_rate_at_location'\n",
    "]\n",
    "\n",
    "# BOOLEAN features (already 0/1, no processing needed)\n",
    "boolean_features = [\n",
    "    'contains_fatality_words',\n",
    "    'contains_pedestrian_words',\n",
    "    'contains_matatu_words',\n",
    "    'contains_motorcycle_words',\n",
    "    'is_morning_rush',\n",
    "    'is_evening_rush',\n",
    "    'is_rush_hour',\n",
    "    'is_weekend',\n",
    "    'is_hotspot'\n",
    "]\n",
    "\n",
    "# ========== DISPLAY CATEGORIZATION ==========\n",
    "\n",
    "print(\"\\n Feature Categories:\")\n",
    "print(f\"\\n   Features to DROP: {len(drop_features)}\")\n",
    "for f in drop_features:\n",
    "    print(f\"      â€¢ {f}\")\n",
    "\n",
    "print(f\"\\n   TARGET variable: 1\")\n",
    "print(f\"      â€¢ {target}\")\n",
    "\n",
    "print(f\"\\n   CATEGORICAL features (need encoding): {len(categorical_features)}\")\n",
    "for f in categorical_features:\n",
    "    print(f\"      â€¢ {f}\")\n",
    "\n",
    "print(f\"\\n   NUMERICAL features (need normalization): {len(numerical_features)}\")\n",
    "for f in numerical_features:\n",
    "    print(f\"      â€¢ {f}\")\n",
    "\n",
    "print(f\"\\n   BOOLEAN features (already 0/1): {len(boolean_features)}\")\n",
    "for f in boolean_features:\n",
    "    print(f\"      â€¢ {f}\")\n",
    "\n",
    "# ========== VERIFY COUNTS ==========\n",
    "\n",
    "total_features = (\n",
    "    len(drop_features) + \n",
    "    1 +  # target\n",
    "    len(categorical_features) + \n",
    "    len(numerical_features) + \n",
    "    len(boolean_features)\n",
    ")\n",
    "\n",
    "print(f\"\\n Total features accounted for: {total_features}\")\n",
    "print(f\"   Original features in dataset: {len(crashes.columns)}\")\n",
    "\n",
    "if total_features == len(crashes.columns):\n",
    "    print(\"    All features categorized!\")\n",
    "else:\n",
    "    print(f\"    Missing {len(crashes.columns) - total_features} features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FEATURE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1c1250-d5b2-4802-a823-ced006c1998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DROPPING UNNECESSARY FEATURES\n",
      "================================================================================\n",
      "\n",
      " Original dataset shape: (31064, 36)\n",
      "\n",
      " Dropping 8 unnecessary features...\n",
      " Features dropped!\n",
      "\n",
      " Cleaned dataset shape: (31064, 28)\n",
      "   Rows: 31,064\n",
      "   Columns: 28\n",
      "\n",
      " Remaining features (28):\n",
      "   1. latitude\n",
      "   2. longitude\n",
      "   3. n_crash_reports\n",
      "   4. contains_fatality_words\n",
      "   5. contains_pedestrian_words\n",
      "   6. contains_matatu_words\n",
      "   7. contains_motorcycle_words\n",
      "   8. severity\n",
      "   9. hour\n",
      "   10. day_of_week\n",
      "   11. month\n",
      "   12. year\n",
      "   13. is_morning_rush\n",
      "   14. is_evening_rush\n",
      "   15. is_rush_hour\n",
      "   16. is_weekend\n",
      "   17. time_of_day\n",
      "   18. distance_from_center_km\n",
      "   19. distance_category\n",
      "   20. crashes_at_location\n",
      "   21. is_hotspot\n",
      "   22. frequency_category\n",
      "   23. severity_numeric\n",
      "   24. avg_severity_at_location\n",
      "   25. max_severity_at_location\n",
      "   26. fatal_rate_at_location\n",
      "   27. pedestrian_rate_at_location\n",
      "   28. location_risk\n",
      "\n",
      " Separating features (X) and target (y)...\n",
      "\n",
      " Separation complete!\n",
      "   Features (X): (31064, 27)\n",
      "   Target (y): (31064,)\n",
      "\n",
      " Target distribution:\n",
      "severity\n",
      "MINOR       25059\n",
      "FATAL        2284\n",
      "MODERATE     2121\n",
      "SEVERE       1600\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target percentages:\n",
      "severity\n",
      "MINOR       80.67\n",
      "FATAL        7.35\n",
      "MODERATE     6.83\n",
      "SEVERE       5.15\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "================================================================================\n",
      " DATASET PREPARED FOR ENCODING\n",
      "================================================================================\n",
      "\n",
      "   Total features ready: 27\n",
      "   â€¢ Categorical (to encode): 4\n",
      "   â€¢ Numerical (to normalize): 14\n",
      "   â€¢ Boolean (ready): 9\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DROP UNNECESSARY FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DROPPING UNNECESSARY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Original dataset shape: {crashes.shape}\")\n",
    "\n",
    "# ========== DROP FEATURES ==========\n",
    "\n",
    "print(f\"\\n Dropping {len(drop_features)} unnecessary features...\")\n",
    "\n",
    "crashes_clean = crashes.drop(columns=drop_features)\n",
    "\n",
    "print(f\" Features dropped!\")\n",
    "print(f\"\\n Cleaned dataset shape: {crashes_clean.shape}\")\n",
    "print(f\"   Rows: {crashes_clean.shape[0]:,}\")\n",
    "print(f\"   Columns: {crashes_clean.shape[1]}\")\n",
    "\n",
    "# ========== VERIFY REMAINING FEATURES ==========\n",
    "\n",
    "print(f\"\\n Remaining features ({len(crashes_clean.columns)}):\")\n",
    "for i, col in enumerate(crashes_clean.columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "# ========== SEPARATE FEATURES AND TARGET ==========\n",
    "\n",
    "print(\"\\n Separating features (X) and target (y)...\")\n",
    "\n",
    "X = crashes_clean.drop(columns=['severity'])\n",
    "y = crashes_clean['severity']\n",
    "\n",
    "print(f\"\\n Separation complete!\")\n",
    "print(f\"   Features (X): {X.shape}\")\n",
    "print(f\"   Target (y): {y.shape}\")\n",
    "\n",
    "print(f\"\\n Target distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nTarget percentages:\")\n",
    "print(y.value_counts(normalize=True).mul(100).round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DATASET PREPARED FOR ENCODING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n   Total features ready: {X.shape[1]}\")\n",
    "print(f\"   â€¢ Categorical (to encode): {len(categorical_features)}\")\n",
    "print(f\"   â€¢ Numerical (to normalize): {len(numerical_features)}\")\n",
    "print(f\"   â€¢ Boolean (ready): {len(boolean_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a36fcc1-0cc5-4678-98aa-9385d3f87e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENCODING CATEGORICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      " Encoding target variable (severity)...\n",
      " Target encoded!\n",
      "\n",
      "   Label mapping:\n",
      "      FATAL â†’ 0\n",
      "      MINOR â†’ 1\n",
      "      MODERATE â†’ 2\n",
      "      SEVERE â†’ 3\n",
      "\n",
      "   Encoded target distribution:\n",
      "      0 (FATAL): 2,284\n",
      "      1 (MINOR): 25,059\n",
      "      2 (MODERATE): 2,121\n",
      "      3 (SEVERE): 1,600\n",
      "\n",
      " One-hot encoding categorical features...\n",
      "   Features to encode: ['time_of_day', 'distance_category', 'frequency_category', 'location_risk']\n",
      "\n",
      "   Encoding: time_of_day\n",
      "      Categories: ['Evening', 'Morning', 'Night', 'Afternoon']\n",
      "      Created 4 dummy columns\n",
      "\n",
      "   Encoding: distance_category\n",
      "      Categories: ['5-10km', '20+km', '0-5km', '10-15km', '15-20km']\n",
      "      Created 5 dummy columns\n",
      "\n",
      "   Encoding: frequency_category\n",
      "      Categories: ['High', 'Low', 'Moderate', 'Isolated']\n",
      "      Created 4 dummy columns\n",
      "\n",
      "   Encoding: location_risk\n",
      "      Categories: ['Low Risk', 'Very High Risk', 'Moderate Risk', 'High Risk']\n",
      "      Created 4 dummy columns\n",
      "\n",
      " One-hot encoding complete!\n",
      "   Total dummy columns created: 17\n",
      "\n",
      " Dropping original categorical columns...\n",
      " Original categorical columns dropped!\n",
      "   Remaining columns: 23\n",
      "\n",
      " Combining encoded features with numerical/boolean features...\n",
      " Final feature matrix created!\n",
      "   Shape: (31064, 40)\n",
      "   Features: 40\n",
      "\n",
      " Sample of encoded dataset (first 5 rows, first 15 columns):\n",
      "   latitude  longitude  n_crash_reports  contains_fatality_words  \\\n",
      "0 -1.263030  36.764374                1                        0   \n",
      "1 -0.829710  37.037820                1                        1   \n",
      "2 -1.125301  37.003297                1                        0   \n",
      "3 -1.740958  37.129026                1                        0   \n",
      "4 -1.259392  36.842321                1                        1   \n",
      "\n",
      "   contains_pedestrian_words  contains_matatu_words  \\\n",
      "0                          0                      0   \n",
      "1                          0                      0   \n",
      "2                          0                      0   \n",
      "3                          0                      0   \n",
      "4                          0                      0   \n",
      "\n",
      "   contains_motorcycle_words  hour  day_of_week  month  year  is_morning_rush  \\\n",
      "0                          0    20            2      6  2018            False   \n",
      "1                          0     6            4      8  2018            False   \n",
      "2                          0    17            4      5  2018            False   \n",
      "3                          0    18            4      5  2018            False   \n",
      "4                          0    21            4      5  2018            False   \n",
      "\n",
      "   is_evening_rush  is_rush_hour  is_weekend  \n",
      "0            False         False       False  \n",
      "1            False         False       False  \n",
      "2             True          True       False  \n",
      "3             True          True       False  \n",
      "4            False         False       False  \n",
      "\n",
      "================================================================================\n",
      " CATEGORICAL ENCODING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "   Original features: 27\n",
      "   Final features (after encoding): 40\n",
      "   Features added: 13\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENCODE CATEGORICAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ========== LABEL ENCODE TARGET VARIABLE ==========\n",
    "\n",
    "print(\"\\n Encoding target variable (severity)...\")\n",
    "\n",
    "# Create label encoder for target\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "\n",
    "print(f\" Target encoded!\")\n",
    "print(f\"\\n   Label mapping:\")\n",
    "for i, label in enumerate(le_target.classes_):\n",
    "    print(f\"      {label} â†’ {i}\")\n",
    "\n",
    "print(f\"\\n   Encoded target distribution:\")\n",
    "unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "for label_num, count in zip(unique, counts):\n",
    "    label_name = le_target.classes_[label_num]\n",
    "    print(f\"      {label_num} ({label_name}): {count:,}\")\n",
    "\n",
    "# ========== ONE-HOT ENCODE CATEGORICAL FEATURES ==========\n",
    "\n",
    "print(f\"\\n One-hot encoding categorical features...\")\n",
    "print(f\"   Features to encode: {categorical_features}\")\n",
    "\n",
    "# Copy X to avoid modifying original\n",
    "X_encoded = X.copy()\n",
    "\n",
    "# One-hot encode each categorical feature\n",
    "encoded_dfs = []\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\n   Encoding: {feature}\")\n",
    "    \n",
    "    # Get unique values\n",
    "    unique_vals = X_encoded[feature].unique()\n",
    "    print(f\"      Categories: {list(unique_vals)}\")\n",
    "    \n",
    "    # One-hot encode\n",
    "    encoded = pd.get_dummies(X_encoded[feature], prefix=feature, drop_first=False)\n",
    "    encoded_dfs.append(encoded)\n",
    "    \n",
    "    print(f\"      Created {len(encoded.columns)} dummy columns\")\n",
    "\n",
    "# Concatenate all encoded features\n",
    "encoded_features = pd.concat(encoded_dfs, axis=1)\n",
    "\n",
    "print(f\"\\n One-hot encoding complete!\")\n",
    "print(f\"   Total dummy columns created: {len(encoded_features.columns)}\")\n",
    "\n",
    "# ========== DROP ORIGINAL CATEGORICAL COLUMNS ==========\n",
    "\n",
    "print(f\"\\n Dropping original categorical columns...\")\n",
    "\n",
    "X_encoded = X_encoded.drop(columns=categorical_features)\n",
    "\n",
    "print(f\" Original categorical columns dropped!\")\n",
    "print(f\"   Remaining columns: {X_encoded.shape[1]}\")\n",
    "\n",
    "# ========== COMBINE ENCODED FEATURES ==========\n",
    "\n",
    "print(f\"\\n Combining encoded features with numerical/boolean features...\")\n",
    "\n",
    "X_final = pd.concat([X_encoded, encoded_features], axis=1)\n",
    "\n",
    "print(f\" Final feature matrix created!\")\n",
    "print(f\"   Shape: {X_final.shape}\")\n",
    "print(f\"   Features: {X_final.shape[1]}\")\n",
    "\n",
    "# ========== DISPLAY SAMPLE ==========\n",
    "\n",
    "print(f\"\\n Sample of encoded dataset (first 5 rows, first 15 columns):\")\n",
    "print(X_final.iloc[:5, :15])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CATEGORICAL ENCODING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n   Original features: 27\")\n",
    "print(f\"   Final features (after encoding): {X_final.shape[1]}\")\n",
    "print(f\"   Features added: {X_final.shape[1] - 27}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d312e898-f38c-45f2-8ea0-6d1937b46898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NORMALIZING NUMERICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      " Identifying numerical features in final dataset...\n",
      "\n",
      "   Numerical features to normalize: 14\n",
      "      â€¢ latitude\n",
      "      â€¢ longitude\n",
      "      â€¢ n_crash_reports\n",
      "      â€¢ hour\n",
      "      â€¢ day_of_week\n",
      "      â€¢ month\n",
      "      â€¢ year\n",
      "      â€¢ distance_from_center_km\n",
      "      â€¢ crashes_at_location\n",
      "      â€¢ severity_numeric\n",
      "      â€¢ avg_severity_at_location\n",
      "      â€¢ max_severity_at_location\n",
      "      â€¢ fatal_rate_at_location\n",
      "      â€¢ pedestrian_rate_at_location\n",
      "\n",
      " Sample statistics BEFORE normalization:\n",
      "           latitude     longitude  n_crash_reports          hour  \\\n",
      "count  31064.000000  31064.000000     31064.000000  31064.000000   \n",
      "mean      -1.272481     36.852499         1.400914     12.935746   \n",
      "std        0.118961      0.113650         1.486540      5.525065   \n",
      "\n",
      "        day_of_week         month          year  distance_from_center_km  \\\n",
      "count  31064.000000  31064.000000  31064.000000             31064.000000   \n",
      "mean       2.854751      6.509464   2016.998390                11.654997   \n",
      "std        1.925492      3.451063      2.711853                14.715408   \n",
      "\n",
      "       crashes_at_location  severity_numeric  avg_severity_at_location  \\\n",
      "count         31064.000000      31064.000000              31064.000000   \n",
      "mean            245.870268          1.391868                  1.391868   \n",
      "std             209.836369          0.884590                  0.183142   \n",
      "\n",
      "       max_severity_at_location  fatal_rate_at_location  \\\n",
      "count              31064.000000            31064.000000   \n",
      "mean                   3.872296                0.073526   \n",
      "std                    0.535643                0.053151   \n",
      "\n",
      "       pedestrian_rate_at_location  \n",
      "count                 31064.000000  \n",
      "mean                      0.030389  \n",
      "std                       0.039818  \n",
      "\n",
      " Applying StandardScaler (zero mean, unit variance)...\n",
      " Normalization complete!\n",
      "\n",
      " Sample statistics AFTER normalization:\n",
      "           latitude     longitude  n_crash_reports          hour  \\\n",
      "count  3.106400e+04  3.106400e+04     3.106400e+04  3.106400e+04   \n",
      "mean  -9.231749e-16 -6.526911e-14     5.855619e-17  5.489643e-17   \n",
      "std    1.000016e+00  1.000016e+00     1.000016e+00  1.000016e+00   \n",
      "\n",
      "        day_of_week         month          year  distance_from_center_km  \\\n",
      "count  3.106400e+04  3.106400e+04  3.106400e+04             3.106400e+04   \n",
      "mean   3.842750e-17  8.783428e-17 -3.476774e-14             9.149404e-18   \n",
      "std    1.000016e+00  1.000016e+00  1.000016e+00             1.000016e+00   \n",
      "\n",
      "       crashes_at_location  severity_numeric  avg_severity_at_location  \\\n",
      "count         3.106400e+04      3.106400e+04              3.106400e+04   \n",
      "mean          3.659762e-18     -5.672631e-17             -1.280917e-16   \n",
      "std           1.000016e+00      1.000016e+00              1.000016e+00   \n",
      "\n",
      "       max_severity_at_location  fatal_rate_at_location  \\\n",
      "count              3.106400e+04            3.106400e+04   \n",
      "mean               1.518801e-16           -8.966416e-17   \n",
      "std                1.000016e+00            1.000016e+00   \n",
      "\n",
      "       pedestrian_rate_at_location  \n",
      "count                 3.106400e+04  \n",
      "mean                  1.372411e-17  \n",
      "std                   1.000016e+00  \n",
      "\n",
      " Verification:\n",
      "   Mean values should be ~0: -0.000000\n",
      "   Std values should be ~1: 1.000016\n",
      "\n",
      " Sample of normalized dataset (first 5 rows, numerical features only):\n",
      "   latitude  longitude  n_crash_reports      hour  day_of_week     month  \\\n",
      "0  0.079445  -0.775411        -0.269701  1.278604    -0.443921 -0.147628   \n",
      "1  3.722039   1.630658        -0.269701 -1.255344     0.594792  0.431913   \n",
      "2  1.237230   1.326891        -0.269701  0.735615     0.594792 -0.437398   \n",
      "3 -3.938136   2.433181        -0.269701  0.916611     0.594792 -0.437398   \n",
      "4  0.110023  -0.089554        -0.269701  1.459600     0.594792 -0.437398   \n",
      "\n",
      "       year  distance_from_center_km  crashes_at_location  severity_numeric  \\\n",
      "0  0.369351                -0.355635             0.019681         -0.443001   \n",
      "1  0.369351                 3.040386            -1.162211          2.948454   \n",
      "2  0.369351                 1.067682            -1.157446         -0.443001   \n",
      "3  0.369351                 3.372875            -1.143148         -0.443001   \n",
      "4  0.369351                -0.513354             1.725800          2.948454   \n",
      "\n",
      "   avg_severity_at_location  max_severity_at_location  fatal_rate_at_location  \\\n",
      "0                 -0.305060                  0.238416               -0.404997   \n",
      "1                  8.780946                  0.238416                8.023966   \n",
      "2                 -2.139735                 -5.362417               -1.383359   \n",
      "3                 -1.229678                 -3.495473               -1.383359   \n",
      "4                 -0.074146                  0.238416               -0.021772   \n",
      "\n",
      "   pedestrian_rate_at_location  \n",
      "0                    -0.361368  \n",
      "1                    -0.763198  \n",
      "2                    -0.763198  \n",
      "3                    -0.763198  \n",
      "4                     0.228159  \n",
      "\n",
      "================================================================================\n",
      " FEATURE NORMALIZATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "   Total features in final dataset: 40\n",
      "   Numerical features normalized: 14\n",
      "   Boolean features (unchanged): 9\n",
      "   Encoded categorical features: 17\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NORMALIZE NUMERICAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NORMALIZING NUMERICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ========== IDENTIFY NUMERICAL COLUMNS IN FINAL DATASET ==========\n",
    "\n",
    "print(\"\\n Identifying numerical features in final dataset...\")\n",
    "\n",
    "# Original numerical features (that still exist after encoding)\n",
    "numerical_cols = [col for col in numerical_features if col in X_final.columns]\n",
    "\n",
    "print(f\"\\n   Numerical features to normalize: {len(numerical_cols)}\")\n",
    "for col in numerical_cols:\n",
    "    print(f\"      â€¢ {col}\")\n",
    "\n",
    "# ========== CHECK FEATURE DISTRIBUTIONS BEFORE NORMALIZATION ==========\n",
    "\n",
    "print(\"\\n Sample statistics BEFORE normalization:\")\n",
    "print(X_final[numerical_cols].describe().iloc[:3])  # Show mean, std, min\n",
    "\n",
    "# ========== APPLY STANDARDIZATION ==========\n",
    "\n",
    "print(f\"\\n Applying StandardScaler (zero mean, unit variance)...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_final[numerical_cols] = scaler.fit_transform(X_final[numerical_cols])\n",
    "\n",
    "print(f\" Normalization complete!\")\n",
    "\n",
    "# ========== VERIFY NORMALIZATION ==========\n",
    "\n",
    "print(\"\\n Sample statistics AFTER normalization:\")\n",
    "print(X_final[numerical_cols].describe().iloc[:3])  # Show mean, std, min\n",
    "\n",
    "print(\"\\n Verification:\")\n",
    "print(f\"   Mean values should be ~0: {X_final[numerical_cols].mean().mean():.6f}\")\n",
    "print(f\"   Std values should be ~1: {X_final[numerical_cols].std().mean():.6f}\")\n",
    "\n",
    "# ========== DISPLAY SAMPLE ==========\n",
    "\n",
    "print(f\"\\n Sample of normalized dataset (first 5 rows, numerical features only):\")\n",
    "print(X_final[numerical_cols].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FEATURE NORMALIZATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n   Total features in final dataset: {X_final.shape[1]}\")\n",
    "print(f\"   Numerical features normalized: {len(numerical_cols)}\")\n",
    "print(f\"   Boolean features (unchanged): {len(boolean_features)}\")\n",
    "print(f\"   Encoded categorical features: 17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1e7a6a-04c1-4c35-b31b-49e833be68c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPLITTING DATA: TRAIN / VALIDATION / TEST\n",
      "================================================================================\n",
      "\n",
      " Split strategy:\n",
      "   â€¢ Training set: 70% (for model training)\n",
      "   â€¢ Validation set: 15% (for hyperparameter tuning)\n",
      "   â€¢ Test set: 15% (for final evaluation)\n",
      "\n",
      " Step 1: Splitting train (70%) vs temp (30%)...\n",
      " First split complete!\n",
      "   Train: 21,744 samples (70.0%)\n",
      "   Temp: 9,320 samples (30.0%)\n",
      "\n",
      " Step 2: Splitting temp into validation (15%) and test (15%)...\n",
      " Second split complete!\n",
      "   Validation: 4,660 samples (15.0%)\n",
      "   Test: 4,660 samples (15.0%)\n",
      "\n",
      "================================================================================\n",
      " FINAL DATASET SPLITS\n",
      "================================================================================\n",
      "\n",
      " Dataset sizes:\n",
      "   Total samples: 31,064\n",
      "   Training: 21,744 (70.0%)\n",
      "   Validation: 4,660 (15.0%)\n",
      "   Test: 4,660 (15.0%)\n",
      "\n",
      " Class distribution in each set:\n",
      "\n",
      "   TRAINING SET:\n",
      "      0 (FATAL): 1,599 (7.35%)\n",
      "      1 (MINOR): 17,541 (80.67%)\n",
      "      2 (MODERATE): 1,484 (6.82%)\n",
      "      3 (SEVERE): 1,120 (5.15%)\n",
      "\n",
      "   VALIDATION SET:\n",
      "      0 (FATAL): 342 (7.34%)\n",
      "      1 (MINOR): 3,759 (80.67%)\n",
      "      2 (MODERATE): 319 (6.85%)\n",
      "      3 (SEVERE): 240 (5.15%)\n",
      "\n",
      "   TEST SET:\n",
      "      0 (FATAL): 343 (7.36%)\n",
      "      1 (MINOR): 3,759 (80.67%)\n",
      "      2 (MODERATE): 318 (6.82%)\n",
      "      3 (SEVERE): 240 (5.15%)\n",
      "\n",
      " Class distributions are balanced across all sets!\n",
      "\n",
      "================================================================================\n",
      " DATA SPLITTING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SPLIT DATA INTO TRAIN/VALIDATION/TEST SETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SPLITTING DATA: TRAIN / VALIDATION / TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========== SPLIT STRATEGY ==========\n",
    "\n",
    "print(\"\\n Split strategy:\")\n",
    "print(\"   â€¢ Training set: 70% (for model training)\")\n",
    "print(\"   â€¢ Validation set: 15% (for hyperparameter tuning)\")\n",
    "print(\"   â€¢ Test set: 15% (for final evaluation)\")\n",
    "\n",
    "# ========== FIRST SPLIT: TRAIN vs (VALIDATION + TEST) ==========\n",
    "\n",
    "print(\"\\n Step 1: Splitting train (70%) vs temp (30%)...\")\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_final, \n",
    "    y_encoded, \n",
    "    test_size=0.30,  # 30% for validation + test\n",
    "    random_state=42,\n",
    "    stratify=y_encoded  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\" First split complete!\")\n",
    "print(f\"   Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "print(f\"   Temp: {X_temp.shape[0]:,} samples ({X_temp.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "\n",
    "# ========== SECOND SPLIT: VALIDATION vs TEST ==========\n",
    "\n",
    "print(\"\\n Step 2: Splitting temp into validation (15%) and test (15%)...\")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.50,  # Split temp equally into validation and test\n",
    "    random_state=42,\n",
    "    stratify=y_temp  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\" Second split complete!\")\n",
    "print(f\"   Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "print(f\"   Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X_final)*100:.1f}%)\")\n",
    "\n",
    "# ========== VERIFY SPLITS ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FINAL DATASET SPLITS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Dataset sizes:\")\n",
    "print(f\"   Total samples: {len(X_final):,}\")\n",
    "print(f\"   Training: {len(X_train):,} ({len(X_train)/len(X_final)*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(X_val):,} ({len(X_val)/len(X_final)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(X_test):,} ({len(X_test)/len(X_final)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n Class distribution in each set:\")\n",
    "\n",
    "# Train set\n",
    "print(\"\\n   TRAINING SET:\")\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "for class_num, count in train_dist.items():\n",
    "    class_name = le_target.classes_[class_num]\n",
    "    pct = count / len(y_train) * 100\n",
    "    print(f\"      {class_num} ({class_name}): {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Validation set\n",
    "print(\"\\n   VALIDATION SET:\")\n",
    "val_dist = pd.Series(y_val).value_counts().sort_index()\n",
    "for class_num, count in val_dist.items():\n",
    "    class_name = le_target.classes_[class_num]\n",
    "    pct = count / len(y_val) * 100\n",
    "    print(f\"      {class_num} ({class_name}): {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Test set\n",
    "print(\"\\n   TEST SET:\")\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "for class_num, count in test_dist.items():\n",
    "    class_name = le_target.classes_[class_num]\n",
    "    pct = count / len(y_test) * 100\n",
    "    print(f\"      {class_num} ({class_name}): {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "print(\"\\n Class distributions are balanced across all sets!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DATA SPLITTING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d081b2f-ff6a-4120-a94a-18ac45c079d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLASS IMBALANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      " Analyzing class imbalance in training set...\n",
      "\n",
      "   Majority class (MINOR): 17,541 samples\n",
      "   Minority class (SEVERE): 1,120 samples\n",
      "   Imbalance ratio: 15.66:1\n",
      "\n",
      " Imbalance ratios for each class:\n",
      "   FATAL: 10.97:1 (majority to this class)\n",
      "   MINOR: 1.00:1 (majority to this class)\n",
      "   MODERATE: 11.82:1 (majority to this class)\n",
      "   SEVERE: 15.66:1 (majority to this class)\n",
      "\n",
      "================================================================================\n",
      "SMOTE DECISION\n",
      "================================================================================\n",
      "\n",
      " Should we apply SMOTE?\n",
      "\n",
      "   Imbalance ratio: 15.66:1\n",
      "\n",
      "   Guidelines:\n",
      "      â€¢ Ratio < 3:1  â†’ SMOTE not needed (balanced)\n",
      "      â€¢ Ratio 3-10:1 â†’ SMOTE recommended\n",
      "      â€¢ Ratio > 10:1 â†’ SMOTE highly recommended\n",
      "\n",
      "   ðŸ”´ DECISION: SMOTE HIGHLY RECOMMENDED\n",
      "   Reason: Severe imbalance exists\n",
      "\n",
      " Additional considerations:\n",
      "   â€¢ Minority class (SEVERE) has 1,120 samples\n",
      "   â€¢ This is sufficient for training\n",
      "   â€¢ Class weights can be used as alternative to SMOTE\n",
      "   â€¢ We'll compare both approaches in model training\n",
      "\n",
      "================================================================================\n",
      " IMBALANCE ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLASS IMBALANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CLASS IMBALANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== CALCULATE IMBALANCE RATIOS ==========\n",
    "\n",
    "print(\"\\n Analyzing class imbalance in training set...\")\n",
    "\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "majority_class_count = train_counts.max()\n",
    "minority_class_count = train_counts.min()\n",
    "\n",
    "print(f\"\\n   Majority class (MINOR): {majority_class_count:,} samples\")\n",
    "print(f\"   Minority class (SEVERE): {minority_class_count:,} samples\")\n",
    "print(f\"   Imbalance ratio: {majority_class_count / minority_class_count:.2f}:1\")\n",
    "\n",
    "# ========== IMBALANCE BY CLASS ==========\n",
    "\n",
    "print(\"\\n Imbalance ratios for each class:\")\n",
    "for class_num in sorted(train_counts.index):\n",
    "    class_name = le_target.classes_[class_num]\n",
    "    count = train_counts[class_num]\n",
    "    ratio = majority_class_count / count\n",
    "    print(f\"   {class_name}: {ratio:.2f}:1 (majority to this class)\")\n",
    "\n",
    "# ========== DECISION ON SMOTE ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SMOTE DECISION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "imbalance_ratio = majority_class_count / minority_class_count\n",
    "\n",
    "print(f\"\\n Should we apply SMOTE?\")\n",
    "print(f\"\\n   Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"\\n   Guidelines:\")\n",
    "print(f\"      â€¢ Ratio < 3:1  â†’ SMOTE not needed (balanced)\")\n",
    "print(f\"      â€¢ Ratio 3-10:1 â†’ SMOTE recommended\")\n",
    "print(f\"      â€¢ Ratio > 10:1 â†’ SMOTE highly recommended\")\n",
    "\n",
    "if imbalance_ratio < 3:\n",
    "    decision = \"NOT NEEDED\"\n",
    "    color = \"ðŸŸ¢\"\n",
    "    reason = \"Dataset is relatively balanced\"\n",
    "elif imbalance_ratio <= 10:\n",
    "    decision = \"RECOMMENDED\"\n",
    "    color = \"ðŸŸ¡\"\n",
    "    reason = \"Moderate imbalance exists\"\n",
    "else:\n",
    "    decision = \"HIGHLY RECOMMENDED\"\n",
    "    color = \"ðŸ”´\"\n",
    "    reason = \"Severe imbalance exists\"\n",
    "\n",
    "print(f\"\\n   {color} DECISION: SMOTE {decision}\")\n",
    "print(f\"   Reason: {reason}\")\n",
    "\n",
    "# ========== ADDITIONAL CONSIDERATIONS ==========\n",
    "\n",
    "print(f\"\\n Additional considerations:\")\n",
    "print(f\"   â€¢ Minority class (SEVERE) has {minority_class_count:,} samples\")\n",
    "print(f\"   â€¢ This is {'sufficient' if minority_class_count >= 500 else 'limited'} for training\")\n",
    "print(f\"   â€¢ Class weights can be used as alternative to SMOTE\")\n",
    "print(f\"   â€¢ We'll compare both approaches in model training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" IMBALANCE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e39efc24-c6bf-41f8-b371-da63e467f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "APPLYING SMOTE (SYNTHETIC MINORITY OVER-SAMPLING)\n",
      "================================================================================\n",
      "\n",
      " Class distribution BEFORE SMOTE:\n",
      "   Total training samples: 21,744\n",
      "   FATAL: 1,599 (7.35%)\n",
      "   MINOR: 17,541 (80.67%)\n",
      "   MODERATE: 1,484 (6.82%)\n",
      "   SEVERE: 1,120 (5.15%)\n",
      "\n",
      " Applying SMOTE...\n",
      "   Strategy: Balance all classes to majority class size\n",
      "   Target: ~17,541 samples per class\n",
      " SMOTE applied successfully!\n",
      "\n",
      " Class distribution AFTER SMOTE:\n",
      "   Total training samples: 70,164\n",
      "   FATAL: 17,541 (25.00%)\n",
      "   MINOR: 17,541 (25.00%)\n",
      "   MODERATE: 17,541 (25.00%)\n",
      "   SEVERE: 17,541 (25.00%)\n",
      "\n",
      " Comparison:\n",
      "   Training samples BEFORE SMOTE: 21,744\n",
      "   Training samples AFTER SMOTE: 70,164\n",
      "   Synthetic samples created: 48,420\n",
      "\n",
      " Result: All classes now balanced!\n",
      "\n",
      "================================================================================\n",
      "  IMPORTANT: SMOTE ONLY APPLIED TO TRAINING SET\n",
      "================================================================================\n",
      "\n",
      "    Training set: SMOTE applied (balanced)\n",
      "      â€¢ Samples: 70,164\n",
      "      â€¢ Classes: Balanced (25% each)\n",
      "\n",
      "    Validation set: Original (imbalanced)\n",
      "      â€¢ Samples: 4,660\n",
      "      â€¢ Classes: Original distribution (80.67% MINOR)\n",
      "\n",
      "    Test set: Original (imbalanced)\n",
      "      â€¢ Samples: 4,660\n",
      "      â€¢ Classes: Original distribution (80.67% MINOR)\n",
      "\n",
      "    Why?\n",
      "      â€¢ Train on balanced data â†’ model learns all classes equally\n",
      "      â€¢ Evaluate on real data â†’ assess real-world performance\n",
      "\n",
      "================================================================================\n",
      " SMOTE APPLICATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# APPLY SMOTE TO TRAINING SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPLYING SMOTE (SYNTHETIC MINORITY OVER-SAMPLING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ========== BEFORE SMOTE ==========\n",
    "\n",
    "print(\"\\n Class distribution BEFORE SMOTE:\")\n",
    "print(f\"   Total training samples: {len(y_train):,}\")\n",
    "for class_num in sorted(train_counts.index):\n",
    "    class_name = le_target.classes_[class_num]\n",
    "    count = train_counts[class_num]\n",
    "    pct = count / len(y_train) * 100\n",
    "    print(f\"   {class_name}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# ========== APPLY SMOTE ==========\n",
    "\n",
    "print(f\"\\n Applying SMOTE...\")\n",
    "print(f\"   Strategy: Balance all classes to majority class size\")\n",
    "print(f\"   Target: ~17,541 samples per class\")\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\" SMOTE applied successfully!\")\n",
    "\n",
    "# ========== AFTER SMOTE ==========\n",
    "\n",
    "print(f\"\\n Class distribution AFTER SMOTE:\")\n",
    "print(f\"   Total training samples: {len(y_train_smote):,}\")\n",
    "\n",
    "train_smote_counts = pd.Series(y_train_smote).value_counts().sort_index()\n",
    "for class_num in sorted(train_smote_counts.index):\n",
    "    class_name = le_target.classes_[class_num]\n",
    "    count = train_smote_counts[class_num]\n",
    "    pct = count / len(y_train_smote) * 100\n",
    "    print(f\"   {class_name}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# ========== COMPARISON ==========\n",
    "\n",
    "print(f\"\\n Comparison:\")\n",
    "print(f\"   Training samples BEFORE SMOTE: {len(y_train):,}\")\n",
    "print(f\"   Training samples AFTER SMOTE: {len(y_train_smote):,}\")\n",
    "print(f\"   Synthetic samples created: {len(y_train_smote) - len(y_train):,}\")\n",
    "\n",
    "print(f\"\\n Result: All classes now balanced!\")\n",
    "\n",
    "# ========== IMPORTANT NOTE ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  IMPORTANT: SMOTE ONLY APPLIED TO TRAINING SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n    Training set: SMOTE applied (balanced)\")\n",
    "print(f\"      â€¢ Samples: {len(X_train_smote):,}\")\n",
    "print(f\"      â€¢ Classes: Balanced (25% each)\")\n",
    "\n",
    "print(\"\\n    Validation set: Original (imbalanced)\")\n",
    "print(f\"      â€¢ Samples: {len(X_val):,}\")\n",
    "print(f\"      â€¢ Classes: Original distribution (80.67% MINOR)\")\n",
    "\n",
    "print(\"\\n    Test set: Original (imbalanced)\")\n",
    "print(f\"      â€¢ Samples: {len(X_test):,}\")\n",
    "print(f\"      â€¢ Classes: Original distribution (80.67% MINOR)\")\n",
    "\n",
    "print(\"\\n    Why?\")\n",
    "print(\"      â€¢ Train on balanced data â†’ model learns all classes equally\")\n",
    "print(\"      â€¢ Evaluate on real data â†’ assess real-world performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SMOTE APPLICATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f57bd4a-f60c-4bf7-9736-86d2786d565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING PREPROCESSED DATASETS\n",
      "================================================================================\n",
      "\n",
      " Saving training data (with SMOTE)...\n",
      " Training data saved!\n",
      "   X_train_smote: (70164, 40)\n",
      "   y_train_smote: (70164,)\n",
      "\n",
      " Saving validation data...\n",
      " Validation data saved!\n",
      "   X_val: (4660, 40)\n",
      "   y_val: (4660,)\n",
      "\n",
      " Saving test data...\n",
      " Test data saved!\n",
      "   X_test: (4660, 40)\n",
      "   y_test: (4660,)\n",
      "\n",
      " Saving preprocessing objects...\n",
      " Preprocessing objects saved!\n",
      "   â€¢ StandardScaler\n",
      "   â€¢ LabelEncoder\n",
      "   â€¢ Feature names\n",
      "\n",
      " Saving preprocessing metadata...\n",
      " Metadata saved!\n",
      "\n",
      "================================================================================\n",
      " ALL DATASETS SAVED SUCCESSFULLY\n",
      "================================================================================\n",
      "\n",
      " Files created:\n",
      "   â€¢ X_train_smote.csv / .pkl\n",
      "   â€¢ y_train_smote.csv / .pkl\n",
      "   â€¢ X_val.csv / .pkl\n",
      "   â€¢ y_val.csv / .pkl\n",
      "   â€¢ X_test.csv / .pkl\n",
      "   â€¢ y_test.csv / .pkl\n",
      "   â€¢ scaler.pkl\n",
      "   â€¢ label_encoder.pkl\n",
      "   â€¢ feature_names.pkl\n",
      "   â€¢ preprocessing_metadata.pkl\n",
      "\n",
      " Use pickle files (.pkl) for faster loading in model training\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE PREPROCESSED DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING PREPROCESSED DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(r'D:\\Nairobi-Accident-Severity\\data\\processed', exist_ok=True)\n",
    "\n",
    "# ========== SAVE TRAINING DATA (WITH SMOTE) ==========\n",
    "\n",
    "print(\"\\n Saving training data (with SMOTE)...\")\n",
    "\n",
    "# Save as CSV\n",
    "pd.DataFrame(X_train_smote, columns=X_final.columns).to_csv(\n",
    "    r'D:\\Nairobi-Accident-Severity\\data\\processed\\X_train_smote.csv',\n",
    "    index=False\n",
    ")\n",
    "pd.DataFrame(y_train_smote, columns=['severity_encoded']).to_csv(\n",
    "    r'D:\\Nairobi-Accident-Severity\\data\\processed\\y_train_smote.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Save as pickle (faster for ML)\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\X_train_smote.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_smote, f)\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\y_train_smote.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_smote, f)\n",
    "\n",
    "print(f\" Training data saved!\")\n",
    "print(f\"   X_train_smote: {X_train_smote.shape}\")\n",
    "print(f\"   y_train_smote: {y_train_smote.shape}\")\n",
    "\n",
    "# ========== SAVE VALIDATION DATA ==========\n",
    "\n",
    "print(\"\\n Saving validation data...\")\n",
    "\n",
    "pd.DataFrame(X_val, columns=X_final.columns).to_csv(\n",
    "    r'D:\\Nairobi-Accident-Severity\\data\\processed\\X_val.csv',\n",
    "    index=False\n",
    ")\n",
    "pd.DataFrame(y_val, columns=['severity_encoded']).to_csv(\n",
    "    r'D:\\Nairobi-Accident-Severity\\data\\processed\\y_val.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\X_val.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val, f)\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\y_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "\n",
    "print(f\" Validation data saved!\")\n",
    "print(f\"   X_val: {X_val.shape}\")\n",
    "print(f\"   y_val: {y_val.shape}\")\n",
    "\n",
    "# ========== SAVE TEST DATA ==========\n",
    "\n",
    "print(\"\\n Saving test data...\")\n",
    "\n",
    "pd.DataFrame(X_test, columns=X_final.columns).to_csv(\n",
    "    r'D:\\Nairobi-Accident-Severity\\data\\processed\\X_test.csv',\n",
    "    index=False\n",
    ")\n",
    "pd.DataFrame(y_test, columns=['severity_encoded']).to_csv(\n",
    "    r'D:\\Nairobi-Accident-Severity\\data\\processed\\y_test.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\X_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "print(f\" Test data saved!\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "print(f\"   y_test: {y_test.shape}\")\n",
    "\n",
    "# ========== SAVE PREPROCESSING OBJECTS ==========\n",
    "\n",
    "print(\"\\n Saving preprocessing objects...\")\n",
    "\n",
    "# Save scaler\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save label encoder\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le_target, f)\n",
    "\n",
    "# Save feature names\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(list(X_final.columns), f)\n",
    "\n",
    "print(f\" Preprocessing objects saved!\")\n",
    "print(f\"   â€¢ StandardScaler\")\n",
    "print(f\"   â€¢ LabelEncoder\")\n",
    "print(f\"   â€¢ Feature names\")\n",
    "\n",
    "# ========== SAVE METADATA ==========\n",
    "\n",
    "print(\"\\n Saving preprocessing metadata...\")\n",
    "\n",
    "metadata = {\n",
    "    'preprocessing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_samples': len(X_final),\n",
    "    'train_samples': len(X_train_smote),\n",
    "    'val_samples': len(X_val),\n",
    "    'test_samples': len(X_test),\n",
    "    'total_features': X_final.shape[1],\n",
    "    'numerical_features': numerical_cols,\n",
    "    'boolean_features': boolean_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'smote_applied': True,\n",
    "    'smote_samples_created': len(y_train_smote) - len(y_train),\n",
    "    'class_mapping': {i: label for i, label in enumerate(le_target.classes_)}\n",
    "}\n",
    "\n",
    "with open(r'D:\\Nairobi-Accident-Severity\\data\\processed\\preprocessing_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(f\" Metadata saved!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ALL DATASETS SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n Files created:\")\n",
    "print(f\"   â€¢ X_train_smote.csv / .pkl\")\n",
    "print(f\"   â€¢ y_train_smote.csv / .pkl\")\n",
    "print(f\"   â€¢ X_val.csv / .pkl\")\n",
    "print(f\"   â€¢ y_val.csv / .pkl\")\n",
    "print(f\"   â€¢ X_test.csv / .pkl\")\n",
    "print(f\"   â€¢ y_test.csv / .pkl\")\n",
    "print(f\"   â€¢ scaler.pkl\")\n",
    "print(f\"   â€¢ label_encoder.pkl\")\n",
    "print(f\"   â€¢ feature_names.pkl\")\n",
    "print(f\"   â€¢ preprocessing_metadata.pkl\")\n",
    "\n",
    "print(f\"\\n Use pickle files (.pkl) for faster loading in model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357f129-efe4-4450-9ad1-ccd3c5533db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
