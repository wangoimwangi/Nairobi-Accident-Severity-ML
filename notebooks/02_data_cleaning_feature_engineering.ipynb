{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab7705a-7ae0-44a0-b7c5-e0caee4cddbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NOTEBOOK 02: DATA CLEANING & FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      " Libraries imported successfully\n",
      "  - pandas, numpy: Data manipulation\n",
      "  - matplotlib, seaborn: Visualization\n",
      "  - sklearn: Data splitting\n",
      "  - imblearn: SMOTE for class imbalance\n",
      "\n",
      "======================================================================\n",
      "LOADING LABELED DATASET FROM NOTEBOOK 01\n",
      "======================================================================\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: 31,064 rows × 24 columns\n",
      "Memory usage: 12.87 MB\n",
      "Date range: 2012-08-08 to 2023-07-12\n",
      "\n",
      "======================================================================\n",
      "FEATURE CATEGORIES FROM NOTEBOOK 01\n",
      "======================================================================\n",
      "\n",
      "1. ORIGINAL WORLD BANK FEATURES (10 columns):\n",
      "   - crash_id\n",
      "   - crash_datetime\n",
      "   - crash_date\n",
      "   - latitude\n",
      "   - longitude\n",
      "   - n_crash_reports\n",
      "   - contains_fatality_words\n",
      "   - contains_pedestrian_words\n",
      "   - contains_matatu_words\n",
      "   - contains_motorcycle_words\n",
      "\n",
      "2. SEVERITY LABELS (2 columns):\n",
      "   - severity_4class\n",
      "   - severity_binary\n",
      "\n",
      "3. TEMPORAL FEATURES (6 columns):\n",
      "   - hour\n",
      "   - day_of_week\n",
      "   - day_name\n",
      "   - month\n",
      "   - year\n",
      "   - is_weekend\n",
      "\n",
      "4. SPATIAL FEATURES (6 columns):\n",
      "   - lat_bin\n",
      "   - lon_bin\n",
      "   - grid_cell\n",
      "   - crashes_at_location\n",
      "   - high_rate_at_location\n",
      "   - location_risk_category\n",
      "\n",
      "======================================================================\n",
      "TARGET VARIABLE: BINARY SEVERITY\n",
      "======================================================================\n",
      "severity_binary\n",
      "LOW     27180\n",
      "HIGH     3884\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class imbalance ratio (LOW:HIGH): 7.00:1\n",
      "\n",
      " Data loaded successfully\n",
      " Ready for feature engineering\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "NOTEBOOK 02: DATA CLEANING & FEATURE ENGINEERING FOR EMERGENCY DISPATCH\n",
    "===============================================================================\n",
    "\n",
    "Purpose: Create dispatch-time available features for accident severity prediction\n",
    "\n",
    "Critical Principle: ONLY use information available WHEN THE EMERGENCY CALL COMES IN\n",
    "- Location (GPS coordinates) \n",
    "- Timestamp (when call received) \n",
    "- Historical patterns at location/time \n",
    "\n",
    "Author: Mary Wangoi Mwangi (122174)\n",
    "Supervisor: Prof. Vincent Omwenga\n",
    "Date: January 2026\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NOTEBOOK 02: DATA CLEANING & FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n Libraries imported successfully\")\n",
    "print(\"  - pandas, numpy: Data manipulation\")\n",
    "print(\"  - matplotlib, seaborn: Visualization\")\n",
    "print(\"  - sklearn: Data splitting\")\n",
    "print(\"  - imblearn: SMOTE for class imbalance\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD LABELED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING LABELED DATASET FROM NOTEBOOK 01\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the dataset created in Notebook 01\n",
    "data_path = '../data/processed/labeled_crashes.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert datetime columns\n",
    "df['crash_datetime'] = pd.to_datetime(df['crash_datetime'])\n",
    "df['crash_date'] = pd.to_datetime(df['crash_date'])\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Date range: {df['crash_date'].min().date()} to {df['crash_date'].max().date()}\")\n",
    "\n",
    "# Display column categories\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE CATEGORIES FROM NOTEBOOK 01\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. ORIGINAL WORLD BANK FEATURES (10 columns):\")\n",
    "original_cols = ['crash_id', 'crash_datetime', 'crash_date', 'latitude', 'longitude', \n",
    "                 'n_crash_reports', 'contains_fatality_words', 'contains_pedestrian_words',\n",
    "                 'contains_matatu_words', 'contains_motorcycle_words']\n",
    "for col in original_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(\"\\n2. SEVERITY LABELS (2 columns):\")\n",
    "severity_cols = ['severity_4class', 'severity_binary']\n",
    "for col in severity_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(\"\\n3. TEMPORAL FEATURES (6 columns):\")\n",
    "temporal_cols = ['hour', 'day_of_week', 'day_name', 'month', 'year', 'is_weekend']\n",
    "for col in temporal_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(\"\\n4. SPATIAL FEATURES (6 columns):\")\n",
    "spatial_cols = ['lat_bin', 'lon_bin', 'grid_cell', 'crashes_at_location', \n",
    "                'high_rate_at_location', 'location_risk_category']\n",
    "for col in spatial_cols:\n",
    "    print(f\"   - {col}\")\n",
    "    \n",
    "\n",
    "# Display target variable distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TARGET VARIABLE: BINARY SEVERITY\")\n",
    "print(\"=\"*70)\n",
    "print(df['severity_binary'].value_counts())\n",
    "print(f\"\\nClass imbalance ratio (LOW:HIGH): {(df['severity_binary']=='LOW').sum() / (df['severity_binary']=='HIGH').sum():.2f}:1\")\n",
    "\n",
    "print(\"\\n Data loaded successfully\")\n",
    "print(\" Ready for feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647e5479-33c8-4715-9cc6-23209477eac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MISSING VALUE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      " Columns with missing values:\n",
      "                Column  Missing_Count  Missing_Percentage\n",
      "location_risk_category           1211                 3.9\n",
      "\n",
      "======================================================================\n",
      "INVESTIGATING location_risk_category MISSING VALUES\n",
      "======================================================================\n",
      "\n",
      "Crashes with missing location_risk_category: 1,211\n",
      "Percentage of total dataset: 3.90%\n",
      "\n",
      "Why are these missing?\n",
      "Min high_rate_at_location (missing cases): 0.00%\n",
      "Max high_rate_at_location (missing cases): 0.00%\n",
      "Mean high_rate_at_location (missing cases): 0.00%\n",
      "\n",
      "Our risk category bins (from Notebook 01):\n",
      "  - LOW_RISK: 0-10%\n",
      "  - MEDIUM_RISK: 10-15%\n",
      "  - HIGH_RISK: 15-20%\n",
      "  - VERY_HIGH_RISK: 20-100%\n",
      "\n",
      " ISSUE IDENTIFIED:\n",
      "   Missing values occur when high_rate_at_location is EXACTLY 0%\n",
      "   or falls outside bin boundaries (edge case handling)\n",
      "\n",
      "======================================================================\n",
      "FIXING MISSING VALUES IN location_risk_category\n",
      "======================================================================\n",
      "\n",
      "Missing values after fix: 0\n",
      " All missing values successfully fixed!\n",
      "\n",
      "======================================================================\n",
      "UPDATED location_risk_category DISTRIBUTION\n",
      "======================================================================\n",
      "location_risk_category\n",
      "LOW_RISK          10308\n",
      "MEDIUM_RISK       13029\n",
      "HIGH_RISK          6208\n",
      "VERY_HIGH_RISK     1519\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "FINAL MISSING VALUE CHECK\n",
      "======================================================================\n",
      " No missing values in dataset!\n",
      " Dataset is complete and ready for feature engineering\n",
      "\n",
      " Missing value treatment complete\n",
      " Dataset shape: 31,064 rows × 24 columns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 2: Missing Value Analysis & Treatment\n",
    "\n",
    "Objective: Identify, understand, and handle missing values appropriately.\n",
    "Focus: The 1,211 missing values in location_risk_category from Notebook 01.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for missing values across all columns\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"\\n Columns with missing values:\")\n",
    "    print(missing_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n No missing values found!\")\n",
    "\n",
    "    \n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATE location_risk_category MISSING VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INVESTIGATING location_risk_category MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find which crashes have missing risk categories\n",
    "missing_risk = df[df['location_risk_category'].isnull()]\n",
    "\n",
    "print(f\"\\nCrashes with missing location_risk_category: {len(missing_risk):,}\")\n",
    "print(f\"Percentage of total dataset: {len(missing_risk)/len(df)*100:.2f}%\")\n",
    "\n",
    "# Analyze the high_rate_at_location for missing cases\n",
    "print(\"\\nWhy are these missing?\")\n",
    "print(f\"Min high_rate_at_location (missing cases): {missing_risk['high_rate_at_location'].min():.2f}%\")\n",
    "print(f\"Max high_rate_at_location (missing cases): {missing_risk['high_rate_at_location'].max():.2f}%\")\n",
    "print(f\"Mean high_rate_at_location (missing cases): {missing_risk['high_rate_at_location'].mean():.2f}%\")\n",
    "\n",
    "print(\"\\nOur risk category bins (from Notebook 01):\")\n",
    "print(\"  - LOW_RISK: 0-10%\")\n",
    "print(\"  - MEDIUM_RISK: 10-15%\")\n",
    "print(\"  - HIGH_RISK: 15-20%\")\n",
    "print(\"  - VERY_HIGH_RISK: 20-100%\")\n",
    "\n",
    "# The issue: values below 10% or exactly at boundaries\n",
    "print(\"\\n ISSUE IDENTIFIED:\")\n",
    "print(\"   Missing values occur when high_rate_at_location is EXACTLY 0%\")\n",
    "print(\"   or falls outside bin boundaries (edge case handling)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX MISSING VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIXING MISSING VALUES IN location_risk_category\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Strategy: Create a more inclusive binning that covers ALL cases\n",
    "df['location_risk_category'] = pd.cut(\n",
    "    df['high_rate_at_location'], \n",
    "    bins=[-0.01, 10, 15, 20, 100],  # Include -0.01 to catch 0% cases\n",
    "    labels=['LOW_RISK', 'MEDIUM_RISK', 'HIGH_RISK', 'VERY_HIGH_RISK'],\n",
    "    include_lowest=True  # Include the lowest boundary\n",
    ")\n",
    "\n",
    "# Verify fix\n",
    "remaining_missing = df['location_risk_category'].isnull().sum()\n",
    "print(f\"\\nMissing values after fix: {remaining_missing}\")\n",
    "\n",
    "if remaining_missing == 0:\n",
    "    print(\" All missing values successfully fixed!\")\n",
    "else:\n",
    "    print(f\" Still {remaining_missing} missing values - will fill with 'LOW_RISK'\")\n",
    "    df['location_risk_category'].fillna('LOW_RISK', inplace=True)\n",
    "\n",
    "# Display updated distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UPDATED location_risk_category DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(df['location_risk_category'].value_counts().sort_index())\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL MISSING VALUE CHECK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MISSING VALUE CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_missing = df.isnull().sum().sum()\n",
    "if total_missing == 0:\n",
    "    print(\" No missing values in dataset!\")\n",
    "    print(\" Dataset is complete and ready for feature engineering\")\n",
    "else:\n",
    "    print(f\" {total_missing} missing values remaining\")\n",
    "    print(\"\\n Columns with missing values:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "print(f\"\\n Missing value treatment complete\")\n",
    "print(f\" Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4024a4c6-4dd2-45f4-a092-1c3fab667b13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IDENTIFYING DATA LEAKAGE FEATURES\n",
      "======================================================================\n",
      "\n",
      " DATA LEAKAGE FEATURES (Must be removed):\n",
      "----------------------------------------------------------------------\n",
      "1. n_crash_reports\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "2. contains_fatality_words\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "3. contains_pedestrian_words\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "4. contains_matatu_words\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "5. contains_motorcycle_words\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "======================================================================\n",
      "CORRELATION WITH TARGET (Showing why these are leakage)\n",
      "======================================================================\n",
      "\n",
      "Correlation with HIGH severity:\n",
      "----------------------------------------------------------------------\n",
      "n_crash_reports               :  0.095 \n",
      "contains_fatality_words       :  0.745 ← SUSPICIOUSLY HIGH!\n",
      "contains_pedestrian_words     :  0.468 ← SUSPICIOUSLY HIGH!\n",
      "contains_matatu_words         :  0.036 \n",
      "contains_motorcycle_words     :  0.517 ← SUSPICIOUSLY HIGH!\n",
      "\n",
      "======================================================================\n",
      "PROOF OF DATA LEAKAGE: Keyword Flags vs Severity Labels\n",
      "======================================================================\n",
      "\n",
      "1. contains_fatality_words vs severity_binary:\n",
      "severity_binary                HIGH        LOW\n",
      "contains_fatality_words                       \n",
      "0                          5.559416  94.440584\n",
      "1                        100.000000   0.000000\n",
      "\n",
      "2. contains_pedestrian_words vs severity_binary:\n",
      "severity_binary                  HIGH        LOW\n",
      "contains_pedestrian_words                       \n",
      "0                            9.760956  90.239044\n",
      "1                          100.000000   0.000000\n",
      "\n",
      "3. contains_motorcycle_words vs severity_binary:\n",
      "severity_binary                  HIGH        LOW\n",
      "contains_motorcycle_words                       \n",
      "0                            9.163826  90.836174\n",
      "1                          100.000000   0.000000\n",
      "\n",
      " NOTICE: Keyword flags are DIRECT INDICATORS of severity!\n",
      "   Using these features would be circular reasoning, not prediction.\n",
      "\n",
      "======================================================================\n",
      "REMOVING DATA LEAKAGE FEATURES\n",
      "======================================================================\n",
      "\n",
      "Before removal: 25 columns\n",
      "After removal:  19 columns\n",
      "Removed:        5 leakage features\n",
      "\n",
      "======================================================================\n",
      "REMAINING FEATURES (All dispatch-time available)\n",
      "======================================================================\n",
      "\n",
      "Identifiers (3):\n",
      "  ✓ crash_id\n",
      "  ✓ crash_datetime\n",
      "  ✓ crash_date\n",
      "\n",
      "Location (2):\n",
      "  ✓ latitude\n",
      "  ✓ longitude\n",
      "\n",
      "Temporal (6):\n",
      "  ✓ hour\n",
      "  ✓ day_of_week\n",
      "  ✓ day_name\n",
      "  ✓ month\n",
      "  ✓ year\n",
      "  ✓ is_weekend\n",
      "\n",
      "Spatial (6):\n",
      "  ✓ lat_bin\n",
      "  ✓ lon_bin\n",
      "  ✓ grid_cell\n",
      "  ✓ crashes_at_location\n",
      "  ✓ high_rate_at_location\n",
      "  ✓ location_risk_category\n",
      "\n",
      "Target (2):\n",
      "  ✓ severity_4class\n",
      "  ✓ severity_binary\n",
      "\n",
      "======================================================================\n",
      "DATA LEAKAGE REMOVAL COMPLETE\n",
      "======================================================================\n",
      "\n",
      " Removed 5 features with post-accident information\n",
      " Retained 19 dispatch-time available features\n",
      " Dataset shape: 31,064 rows × 19 columns\n",
      "\n",
      " Model will now be trained ONLY on information available at dispatch time\n",
      "  This ensures realistic performance estimates for operational deployment\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 3: Remove Data Leakage Features (CRITICAL)\n",
    "\n",
    "Data leakage occurs when features used for prediction contain information\n",
    "that would NOT be available at the time predictions need to be made.\n",
    "\n",
    "In emergency dispatch, we must ONLY use information available WHEN THE CALL COMES IN.\n",
    "\n",
    "AVAILABLE at dispatch time:\n",
    "  GPS location (caller provides)\n",
    "  Call timestamp (automatic)\n",
    "  Historical patterns at location/time\n",
    "\n",
    "NOT AVAILABLE at dispatch time (DATA LEAKAGE):\n",
    "  Keyword flags (contains_fatality_words, etc.) - from crash reports AFTER accident\n",
    "  Number of reports (n_crash_reports) - only known after multiple people report\n",
    "  Crash description text - not available until scene assessment\n",
    "\n",
    "These features create artificially high model performance that won't work in real dispatch!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"IDENTIFYING DATA LEAKAGE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY LEAKAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "# Features that contain post-accident information\n",
    "leakage_features = [\n",
    "    'n_crash_reports',           # Only known after reports accumulate\n",
    "    'contains_fatality_words',   # From crash scene reports\n",
    "    'contains_pedestrian_words', # From crash scene reports\n",
    "    'contains_matatu_words',     # From crash scene reports\n",
    "    'contains_motorcycle_words'  # From crash scene reports\n",
    "]\n",
    "\n",
    "print(\"\\n DATA LEAKAGE FEATURES (Must be removed):\")\n",
    "print(\"-\" * 70)\n",
    "for i, feature in enumerate(leakage_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "    print(f\"   Why leakage? This information comes from crash reports AFTER the accident\")\n",
    "    print(f\"   Dispatcher doesn't have this when emergency call comes in\\n\")\n",
    "\n",
    "# Show correlation with target (they'll be suspiciously high!)\n",
    "print(\"=\" * 70)\n",
    "print(\"CORRELATION WITH TARGET (Showing why these are leakage)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert binary target to numeric for correlation\n",
    "df['severity_numeric'] = (df['severity_binary'] == 'HIGH').astype(int)\n",
    "\n",
    "print(\"\\nCorrelation with HIGH severity:\")\n",
    "print(\"-\" * 70)\n",
    "for feature in leakage_features:\n",
    "    corr = df[feature].corr(df['severity_numeric'])\n",
    "    print(f\"{feature:30s}: {corr:6.3f} {'← SUSPICIOUSLY HIGH!' if abs(corr) > 0.3 else ''}\")\n",
    "\n",
    "# Show how keyword flags relate to our severity labels\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROOF OF DATA LEAKAGE: Keyword Flags vs Severity Labels\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. contains_fatality_words vs severity_binary:\")\n",
    "print(pd.crosstab(df['contains_fatality_words'], df['severity_binary'], \n",
    "                  normalize='index') * 100)\n",
    "\n",
    "print(\"\\n2. contains_pedestrian_words vs severity_binary:\")\n",
    "print(pd.crosstab(df['contains_pedestrian_words'], df['severity_binary'], \n",
    "                  normalize='index') * 100)\n",
    "\n",
    "print(\"\\n3. contains_motorcycle_words vs severity_binary:\")\n",
    "print(pd.crosstab(df['contains_motorcycle_words'], df['severity_binary'], \n",
    "                  normalize='index') * 100)\n",
    "\n",
    "print(\"\\n NOTICE: Keyword flags are DIRECT INDICATORS of severity!\")\n",
    "print(\"   Using these features would be circular reasoning, not prediction.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REMOVE LEAKAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REMOVING DATA LEAKAGE FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nBefore removal: {len(df.columns)} columns\")\n",
    "\n",
    "# Drop leakage features\n",
    "df_clean = df.drop(columns=leakage_features)\n",
    "\n",
    "# Also drop the temporary severity_numeric column\n",
    "df_clean = df_clean.drop(columns=['severity_numeric'])\n",
    "\n",
    "print(f\"After removal:  {len(df_clean.columns)} columns\")\n",
    "print(f\"Removed:        {len(leakage_features)} leakage features\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY REMAINING FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REMAINING FEATURES (All dispatch-time available)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Categorize remaining features\n",
    "dispatch_available = {\n",
    "    'Identifiers (3)': ['crash_id', 'crash_datetime', 'crash_date'],\n",
    "    'Location (2)': ['latitude', 'longitude'],\n",
    "    'Temporal (6)': ['hour', 'day_of_week', 'day_name', 'month', 'year', 'is_weekend'],\n",
    "    'Spatial (6)': ['lat_bin', 'lon_bin', 'grid_cell', 'crashes_at_location', \n",
    "                    'high_rate_at_location', 'location_risk_category'],\n",
    "    'Target (2)': ['severity_4class', 'severity_binary']\n",
    "}\n",
    "\n",
    "for category, features in dispatch_available.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for feature in features:\n",
    "        if feature in df_clean.columns:\n",
    "            print(f\"  ✓ {feature}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA LEAKAGE REMOVAL COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n Removed {len(leakage_features)} features with post-accident information\")\n",
    "print(f\" Retained {len(df_clean.columns)} dispatch-time available features\")\n",
    "print(f\" Dataset shape: {df_clean.shape[0]:,} rows × {df_clean.shape[1]} columns\")\n",
    "print(\"\\n Model will now be trained ONLY on information available at dispatch time\")\n",
    "print(\"  This ensures realistic performance estimates for operational deployment\")\n",
    "\n",
    "# Update df to clean version\n",
    "df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eb1748-f8d9-4ec3-b5fe-f4ff21b80654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DISPATCH-TIME FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING TEMPORAL RISK FEATURES\n",
      "======================================================================\n",
      "\n",
      "1. hour_severity_rate: Historical HIGH severity % for this hour\n",
      "   Range: 9.57% to 17.50%\n",
      "   Mean: 12.50%\n",
      "\n",
      "2. day_severity_rate: Historical HIGH severity % for this day of week\n",
      "   Range: 11.29% to 15.16%\n",
      "   Mean: 12.50%\n",
      "\n",
      "3. month_severity_rate: Historical HIGH severity % for this month\n",
      "   Range: 10.85% to 13.47%\n",
      "   Mean: 12.50%\n",
      "\n",
      "======================================================================\n",
      "CREATING TIME PERIOD INDICATORS\n",
      "======================================================================\n",
      "\n",
      "4. is_night (10 PM - 4 AM):\n",
      "   Night crashes: 2,115 (6.8%)\n",
      "   Night HIGH rate: 14.75%\n",
      "   Day HIGH rate: 12.34%\n",
      "   Night is 1.20x more dangerous\n",
      "\n",
      "5. is_rush_hour (6-9 AM, 5-7 PM):\n",
      "   Rush hour crashes: 15,590 (50.2%)\n",
      "   Rush hour HIGH rate: 11.34%\n",
      "   Non-rush HIGH rate: 13.67%\n",
      "\n",
      "======================================================================\n",
      "CREATING LOCATION × TIME INTERACTION FEATURES\n",
      "======================================================================\n",
      "\n",
      "6. high_risk_location_dangerous_time:\n",
      "   (High-risk location × Night/Weekend)\n",
      "   Interaction cases: 2,313\n",
      "   HIGH rate when both: 20.41%\n",
      "   HIGH rate baseline: 11.87%\n",
      "   Risk multiplier: 1.72x\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Created 8 new dispatch-time features:\n",
      "  1. hour_severity_rate\n",
      "  2. day_severity_rate\n",
      "  3. month_severity_rate\n",
      "  4. is_night\n",
      "  5. is_rush_hour\n",
      "  6. high_risk_location\n",
      "  7. dangerous_time\n",
      "  8. high_risk_location_dangerous_time\n",
      "\n",
      "Total features now: 27\n",
      "  - Identifiers: 3\n",
      "  - Location: 2 (latitude, longitude)\n",
      "  - Temporal original: 6\n",
      "  - Temporal engineered: 3 (severity rates)\n",
      "  - Temporal indicators: 2 (night, rush_hour)\n",
      "  - Spatial original: 6\n",
      "  - Spatial engineered: 1 (high_risk_location)\n",
      "  - Interaction: 2\n",
      "  - Target: 2\n",
      "\n",
      " Feature engineering complete\n",
      " All features are dispatch-time available\n",
      " Ready for train/validation/test split\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 4: Engineer Dispatch-Time Features\n",
    "\n",
    "Create features that predict severity using ONLY information available\n",
    "when the emergency call comes in.\n",
    "\n",
    "Feature Engineering Strategy:\n",
    "1. Temporal Risk Features: Historical severity rates by hour/day/month\n",
    "2. Spatial Risk Features: Already have (crashes_at_location, high_rate_at_location)\n",
    "3. Interaction Features: Location × Time patterns\n",
    "4. Categorical Encodings: Risk categories and time periods\n",
    "\n",
    "These features capture patterns like:\n",
    "- \"Accidents at 11 PM are usually more severe\"\n",
    "- \"This location has high historical severity\"\n",
    "- \"Weekend nights at this location are dangerous\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DISPATCH-TIME FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# TEMPORAL RISK FEATURES (Historical Severity Rates)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING TEMPORAL RISK FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate historical HIGH severity rate by hour\n",
    "hour_severity_rate = df.groupby('hour')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ").to_dict()\n",
    "\n",
    "df['hour_severity_rate'] = df['hour'].map(hour_severity_rate)\n",
    "\n",
    "print(\"\\n1. hour_severity_rate: Historical HIGH severity % for this hour\")\n",
    "print(f\"   Range: {df['hour_severity_rate'].min():.2f}% to {df['hour_severity_rate'].max():.2f}%\")\n",
    "print(f\"   Mean: {df['hour_severity_rate'].mean():.2f}%\")\n",
    "\n",
    "# Calculate historical HIGH severity rate by day of week\n",
    "day_severity_rate = df.groupby('day_of_week')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ").to_dict()\n",
    "\n",
    "df['day_severity_rate'] = df['day_of_week'].map(day_severity_rate)\n",
    "\n",
    "print(\"\\n2. day_severity_rate: Historical HIGH severity % for this day of week\")\n",
    "print(f\"   Range: {df['day_severity_rate'].min():.2f}% to {df['day_severity_rate'].max():.2f}%\")\n",
    "print(f\"   Mean: {df['day_severity_rate'].mean():.2f}%\")\n",
    "\n",
    "# Calculate historical HIGH severity rate by month\n",
    "month_severity_rate = df.groupby('month')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ").to_dict()\n",
    "\n",
    "df['month_severity_rate'] = df['month'].map(month_severity_rate)\n",
    "\n",
    "print(\"\\n3. month_severity_rate: Historical HIGH severity % for this month\")\n",
    "print(f\"   Range: {df['month_severity_rate'].min():.2f}% to {df['month_severity_rate'].max():.2f}%\")\n",
    "print(f\"   Mean: {df['month_severity_rate'].mean():.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TIME PERIOD INDICATORS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING TIME PERIOD INDICATORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Night indicator (highest severity hours: 10 PM - 4 AM)\n",
    "df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 4)).astype(int)\n",
    "night_high_rate = (df[df['is_night'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "day_high_rate = (df[df['is_night'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(f\"\\n4. is_night (10 PM - 4 AM):\")\n",
    "print(f\"   Night crashes: {df['is_night'].sum():,} ({df['is_night'].mean()*100:.1f}%)\")\n",
    "print(f\"   Night HIGH rate: {night_high_rate:.2f}%\")\n",
    "print(f\"   Day HIGH rate: {day_high_rate:.2f}%\")\n",
    "print(f\"   Night is {night_high_rate/day_high_rate:.2f}x more dangerous\")\n",
    "\n",
    "# Rush hour indicator (morning: 6-9 AM, evening: 5-7 PM)\n",
    "df['is_rush_hour'] = (((df['hour'] >= 6) & (df['hour'] <= 9)) | \n",
    "                      ((df['hour'] >= 17) & (df['hour'] <= 19))).astype(int)\n",
    "rush_high_rate = (df[df['is_rush_hour'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "non_rush_high_rate = (df[df['is_rush_hour'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(f\"\\n5. is_rush_hour (6-9 AM, 5-7 PM):\")\n",
    "print(f\"   Rush hour crashes: {df['is_rush_hour'].sum():,} ({df['is_rush_hour'].mean()*100:.1f}%)\")\n",
    "print(f\"   Rush hour HIGH rate: {rush_high_rate:.2f}%\")\n",
    "print(f\"   Non-rush HIGH rate: {non_rush_high_rate:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOCATION × TIME INTERACTION FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING LOCATION × TIME INTERACTION FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High-risk location at dangerous time\n",
    "df['high_risk_location'] = (df['high_rate_at_location'] > 15).astype(int)\n",
    "df['dangerous_time'] = ((df['is_night'] == 1) | (df['is_weekend'] == 1)).astype(int)\n",
    "df['high_risk_location_dangerous_time'] = df['high_risk_location'] * df['dangerous_time']\n",
    "\n",
    "interaction_high_rate = (df[df['high_risk_location_dangerous_time'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "baseline_high_rate = (df[df['high_risk_location_dangerous_time'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(f\"\\n6. high_risk_location_dangerous_time:\")\n",
    "print(f\"   (High-risk location × Night/Weekend)\")\n",
    "print(f\"   Interaction cases: {df['high_risk_location_dangerous_time'].sum():,}\")\n",
    "print(f\"   HIGH rate when both: {interaction_high_rate:.2f}%\")\n",
    "print(f\"   HIGH rate baseline: {baseline_high_rate:.2f}%\")\n",
    "print(f\"   Risk multiplier: {interaction_high_rate/baseline_high_rate:.2f}x\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY OF NEW FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "new_features = [\n",
    "    'hour_severity_rate',\n",
    "    'day_severity_rate', \n",
    "    'month_severity_rate',\n",
    "    'is_night',\n",
    "    'is_rush_hour',\n",
    "    'high_risk_location',\n",
    "    'dangerous_time',\n",
    "    'high_risk_location_dangerous_time'\n",
    "]\n",
    "\n",
    "print(f\"\\nCreated {len(new_features)} new dispatch-time features:\")\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(f\"\\nTotal features now: {len(df.columns)}\")\n",
    "print(f\"  - Identifiers: 3\")\n",
    "print(f\"  - Location: 2 (latitude, longitude)\")\n",
    "print(f\"  - Temporal original: 6\")\n",
    "print(f\"  - Temporal engineered: 3 (severity rates)\")\n",
    "print(f\"  - Temporal indicators: 2 (night, rush_hour)\")\n",
    "print(f\"  - Spatial original: 6\")\n",
    "print(f\"  - Spatial engineered: 1 (high_risk_location)\")\n",
    "print(f\"  - Interaction: 2\")\n",
    "print(f\"  - Target: 2\")\n",
    "\n",
    "print(\"\\n Feature engineering complete\")\n",
    "print(\" All features are dispatch-time available\")\n",
    "print(\" Ready for train/validation/test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9704b046-6b53-4f6c-846f-b66669fc06a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WEATHER FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING SEASONAL WEATHER FEATURES\n",
      "======================================================================\n",
      "\n",
      "1. season (Nairobi seasonal patterns):\n",
      "season\n",
      "DRY_SEASON     15193\n",
      "LONG_RAINS      7982\n",
      "SHORT_RAINS     7889\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. rain_risk_score (0-1, higher = more likely rain):\n",
      "   Range: 0.15 to 0.85\n",
      "   Mean: 0.46\n",
      "\n",
      "   HIGH severity rate by season:\n",
      "   - DRY_SEASON     : 12.58%\n",
      "   - LONG_RAINS     : 12.55%\n",
      "   - SHORT_RAINS    : 12.31%\n",
      "\n",
      "======================================================================\n",
      "CREATING VISIBILITY FEATURES\n",
      "======================================================================\n",
      "\n",
      "3. daylight_status (visibility conditions):\n",
      "daylight_status\n",
      "DAYLIGHT    20756\n",
      "DARKNESS    10308\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   HIGH severity rate by visibility:\n",
      "   - DAYLIGHT  : 12.21%\n",
      "   - DARKNESS  : 13.10%\n",
      "\n",
      "======================================================================\n",
      "CREATING COMPOUND WEATHER RISK FEATURES\n",
      "======================================================================\n",
      "\n",
      "4. rain_darkness_risk (rainy season + night):\n",
      "   Compound risk cases: 3,549\n",
      "   HIGH rate with compound risk: 13.38%\n",
      "   HIGH rate baseline: 12.39%\n",
      "   Risk multiplier: 1.08x\n",
      "\n",
      "======================================================================\n",
      "WEATHER FEATURES SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Created 5 weather features:\n",
      "  1. season\n",
      "  2. rain_risk_score\n",
      "  3. daylight_status\n",
      "  4. high_rain_risk\n",
      "  5. rain_darkness_risk\n",
      "\n",
      " Weather features engineered\n",
      " Total features now: 32\n",
      " Ready for road infrastructure features\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 5A: Weather Feature Engineering\n",
    "\n",
    "Weather conditions significantly impact accident severity:\n",
    "- Rain → reduced visibility, slippery roads\n",
    "- Night + rain → compounded risk\n",
    "- Temperature extremes → driver fatigue, vehicle issues\n",
    "\n",
    "Data Source Strategy:\n",
    "11 years of historical data (2012-2023), I'll create weather-proxy features based on:\n",
    "\n",
    "1. Season (rainy vs dry season in Nairobi)\n",
    "2. Time-of-day visibility (daylight vs darkness)\n",
    "3. Month-specific rain patterns\n",
    "\n",
    "Note: In operational deployment, these would come from weather API at dispatch time. \n",
    "      For historical analysis, I derive from temporal patterns.\n",
    "\n",
    "Nairobi Climate:\n",
    "- Long rains: March-May\n",
    "- Short rains: October-December\n",
    "- Dry seasons: January-February, June-September\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"WEATHER FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# SEASON-BASED WEATHER PROXY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING SEASONAL WEATHER FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def get_season(month):\n",
    "    \"\"\"\n",
    "    Classify month into Nairobi's seasonal patterns\n",
    "    \"\"\"\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'LONG_RAINS'\n",
    "    elif month in [10, 11, 12]:\n",
    "        return 'SHORT_RAINS'\n",
    "    else:\n",
    "        return 'DRY_SEASON'\n",
    "\n",
    "def get_rain_risk(month):\n",
    "    \"\"\"\n",
    "    Rain probability by month (0-1 scale)\n",
    "    Based on Nairobi historical rainfall patterns\n",
    "    \"\"\"\n",
    "    rain_risk_map = {\n",
    "        1: 0.15,  # Jan - Dry\n",
    "        2: 0.20,  # Feb - Dry\n",
    "        3: 0.70,  # Mar - Long rains peak\n",
    "        4: 0.85,  # Apr - Long rains peak\n",
    "        5: 0.65,  # May - Long rains\n",
    "        6: 0.25,  # Jun - Dry\n",
    "        7: 0.20,  # Jul - Dry\n",
    "        8: 0.25,  # Aug - Dry\n",
    "        9: 0.30,  # Sep - Transition\n",
    "        10: 0.60, # Oct - Short rains\n",
    "        11: 0.75, # Nov - Short rains peak\n",
    "        12: 0.55  # Dec - Short rains\n",
    "    }\n",
    "    return rain_risk_map[month]\n",
    "\n",
    "# Create season feature\n",
    "df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "# Create rain risk score\n",
    "df['rain_risk_score'] = df['month'].apply(get_rain_risk)\n",
    "\n",
    "print(\"\\n1. season (Nairobi seasonal patterns):\")\n",
    "print(df['season'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n2. rain_risk_score (0-1, higher = more likely rain):\")\n",
    "print(f\"   Range: {df['rain_risk_score'].min():.2f} to {df['rain_risk_score'].max():.2f}\")\n",
    "print(f\"   Mean: {df['rain_risk_score'].mean():.2f}\")\n",
    "\n",
    "# Analyze severity by season\n",
    "season_severity = df.groupby('season')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ")\n",
    "print(\"\\n   HIGH severity rate by season:\")\n",
    "for season in ['DRY_SEASON', 'LONG_RAINS', 'SHORT_RAINS']:\n",
    "    if season in season_severity.index:\n",
    "        print(f\"   - {season:15s}: {season_severity[season]:.2f}%\")\n",
    "\n",
    "\n",
    "        \n",
    "# ============================================================================\n",
    "# VISIBILITY FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING VISIBILITY FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def get_daylight_status(hour, month):\n",
    "    \"\"\"\n",
    "    Estimate daylight vs darkness in Nairobi\n",
    "    Nairobi (near equator): sunrise ~6:15 AM, sunset ~6:45 PM (varies slightly)\n",
    "    \"\"\"\n",
    "    # Nairobi is near equator, so sunrise/sunset very consistent\n",
    "    if 7 <= hour <= 18:\n",
    "        return 'DAYLIGHT'\n",
    "    else:\n",
    "        return 'DARKNESS'\n",
    "\n",
    "df['daylight_status'] = df.apply(lambda row: get_daylight_status(row['hour'], row['month']), axis=1)\n",
    "\n",
    "print(\"\\n3. daylight_status (visibility conditions):\")\n",
    "print(df['daylight_status'].value_counts())\n",
    "\n",
    "# Analyze severity by daylight\n",
    "daylight_severity = df.groupby('daylight_status')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ")\n",
    "print(\"\\n   HIGH severity rate by visibility:\")\n",
    "for status in ['DAYLIGHT', 'DARKNESS']:\n",
    "    print(f\"   - {status:10s}: {daylight_severity[status]:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPOUND RISK: RAIN + DARKNESS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING COMPOUND WEATHER RISK FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High rain risk (>60% probability)\n",
    "df['high_rain_risk'] = (df['rain_risk_score'] > 0.6).astype(int)\n",
    "\n",
    "# Rain + darkness compound risk\n",
    "df['rain_darkness_risk'] = ((df['high_rain_risk'] == 1) & \n",
    "                             (df['daylight_status'] == 'DARKNESS')).astype(int)\n",
    "\n",
    "compound_risk_high_rate = (df[df['rain_darkness_risk'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "baseline_high_rate = (df[df['rain_darkness_risk'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(\"\\n4. rain_darkness_risk (rainy season + night):\")\n",
    "print(f\"   Compound risk cases: {df['rain_darkness_risk'].sum():,}\")\n",
    "print(f\"   HIGH rate with compound risk: {compound_risk_high_rate:.2f}%\")\n",
    "print(f\"   HIGH rate baseline: {baseline_high_rate:.2f}%\")\n",
    "if baseline_high_rate > 0:\n",
    "    print(f\"   Risk multiplier: {compound_risk_high_rate/baseline_high_rate:.2f}x\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY OF WEATHER FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEATHER FEATURES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "weather_features = [\n",
    "    'season',\n",
    "    'rain_risk_score',\n",
    "    'daylight_status',\n",
    "    'high_rain_risk',\n",
    "    'rain_darkness_risk'\n",
    "]\n",
    "\n",
    "print(f\"\\nCreated {len(weather_features)} weather features:\")\n",
    "for i, feature in enumerate(weather_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(f\"\\n Weather features engineered\")\n",
    "print(f\" Total features now: {len(df.columns)}\")\n",
    "print(f\" Ready for road infrastructure features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b2df58-d977-4f86-9ab4-3daf2a428f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ROAD INFRASTRUCTURE FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ROAD TYPE CLASSIFICATION (Based on Crash Patterns)\n",
      "======================================================================\n",
      "\n",
      "1. road_type_proxy (derived from crash volume):\n",
      "road_type_proxy\n",
      "MAIN_ROAD          4752\n",
      "MAJOR_HIGHWAY     15376\n",
      "RESIDENTIAL        2601\n",
      "SECONDARY_ROAD     8335\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   HIGH severity rate by road type:\n",
      "   - RESIDENTIAL      :  13.30%\n",
      "   - SECONDARY_ROAD   :  12.11%\n",
      "   - MAIN_ROAD        :  12.44%\n",
      "   - MAJOR_HIGHWAY    :  12.60%\n",
      "\n",
      "======================================================================\n",
      "INTERSECTION DETECTION (Based on Spatial Clustering)\n",
      "======================================================================\n",
      "\n",
      "2. likely_intersection (crash density >50):\n",
      "   Intersection locations: 24,979 crashes (80.4%)\n",
      "   Intersection HIGH rate: 12.33%\n",
      "   Non-intersection HIGH rate: 13.23%\n",
      "\n",
      "======================================================================\n",
      "SPEED-RELATED RISK INDICATORS\n",
      "======================================================================\n",
      "\n",
      "3. high_speed_road (major highways + main roads):\n",
      "   High-speed road crashes: 20,128 (64.8%)\n",
      "   High-speed HIGH rate: 12.56%\n",
      "   Low-speed HIGH rate: 12.39%\n",
      "\n",
      "======================================================================\n",
      "GEOGRAPHIC RISK ZONES\n",
      "======================================================================\n",
      "\n",
      "4. geographic_zone (distance from Nairobi CBD):\n",
      "geographic_zone\n",
      "CBD_CORE          9905\n",
      "INNER_SUBURBS    14771\n",
      "OUTER_SUBURBS     4155\n",
      "PERIPHERAL        2233\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   HIGH severity rate by zone:\n",
      "   - CBD_CORE         :  12.67%\n",
      "   - INNER_SUBURBS    :  12.13%\n",
      "   - OUTER_SUBURBS    :  12.06%\n",
      "   - PERIPHERAL       :  15.09%\n",
      "\n",
      "5. distance_from_cbd_km (continuous):\n",
      "   Range: 0.02 to 225.07 km\n",
      "   Mean: 11.65 km\n",
      "   Median: 7.55 km\n",
      "\n",
      "======================================================================\n",
      "COMPOUND INFRASTRUCTURE RISK\n",
      "======================================================================\n",
      "\n",
      "6. high_risk_infrastructure (intersection × high-speed road):\n",
      "   High-risk infrastructure: 20,128 crashes\n",
      "   HIGH rate with risk: 12.56%\n",
      "   HIGH rate baseline: 12.39%\n",
      "   Risk multiplier: 1.01x\n",
      "\n",
      "======================================================================\n",
      "ROAD INFRASTRUCTURE FEATURES SUMMARY\n",
      "======================================================================\n",
      "\n",
      " Created 6 road infrastructure features:\n",
      "  1. road_type_proxy\n",
      "  2. likely_intersection\n",
      "  3. high_speed_road\n",
      "  4. geographic_zone\n",
      "  5. distance_from_cbd_km\n",
      "  6. high_risk_infrastructure\n",
      "\n",
      " Road infrastructure features engineered\n",
      " Total features now: 38\n",
      " Feature engineering complete!\n",
      "\n",
      " Ready for train/validation/test split\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 5B: Road Infrastructure Feature Engineering\n",
    "\n",
    "Road infrastructure significantly impacts accident severity:\n",
    "- Major highways → higher speeds → more severe crashes\n",
    "- Intersections → conflict points → higher crash risk\n",
    "- Residential roads → lower speeds → less severe crashes\n",
    "\n",
    "Data Source Strategy:\n",
    "Derive road characteristics from:\n",
    "1. Crash density patterns (high-volume roads vs low-volume)\n",
    "2. Location clustering (intersections have more crashes)\n",
    "3. Geographic patterns (highway corridors vs neighborhoods)\n",
    "\n",
    "In operational deployment, these would come from:\n",
    "- OpenStreetMap API (road type, intersection presence)\n",
    "- Google Maps API (speed limits, traffic patterns)\n",
    "- Pre-loaded GIS database\n",
    "\n",
    "For this analysis, I used crash patterns as proxies for road types.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ROAD INFRASTRUCTURE FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# CRASH VOLUME AS ROAD TYPE PROXY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROAD TYPE CLASSIFICATION (Based on Crash Patterns)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def classify_road_type(crashes_at_location):\n",
    "    \"\"\"\n",
    "    Classify road type based on crash volume at location\n",
    "    \n",
    "    Logic:\n",
    "    - Very high crash volume (>200) → Major highway/arterial\n",
    "    - High crash volume (100-200) → Main road\n",
    "    - Medium crash volume (20-100) → Secondary road\n",
    "    - Low crash volume (<20) → Residential/minor road\n",
    "    \"\"\"\n",
    "    if crashes_at_location >= 200:\n",
    "        return 'MAJOR_HIGHWAY'\n",
    "    elif crashes_at_location >= 100:\n",
    "        return 'MAIN_ROAD'\n",
    "    elif crashes_at_location >= 20:\n",
    "        return 'SECONDARY_ROAD'\n",
    "    else:\n",
    "        return 'RESIDENTIAL'\n",
    "\n",
    "df['road_type_proxy'] = df['crashes_at_location'].apply(classify_road_type)\n",
    "\n",
    "print(\"\\n1. road_type_proxy (derived from crash volume):\")\n",
    "print(df['road_type_proxy'].value_counts().sort_index())\n",
    "\n",
    "# Analyze severity by road type\n",
    "road_type_severity = df.groupby('road_type_proxy')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ")\n",
    "print(\"\\n   HIGH severity rate by road type:\")\n",
    "for road_type in ['RESIDENTIAL', 'SECONDARY_ROAD', 'MAIN_ROAD', 'MAJOR_HIGHWAY']:\n",
    "    if road_type in road_type_severity.index:\n",
    "        print(f\"   - {road_type:17s}: {road_type_severity[road_type]:6.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTERSECTION INDICATOR (Crash Clustering)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERSECTION DETECTION (Based on Spatial Clustering)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Locations with very high crash density likely indicate intersections\n",
    "# We already have crashes_at_location from Notebook 01\n",
    "\n",
    "# Intersection proxy: locations with >50 crashes (statistical hotspot)\n",
    "df['likely_intersection'] = (df['crashes_at_location'] > 50).astype(int)\n",
    "\n",
    "intersection_high_rate = (df[df['likely_intersection'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "non_intersection_high_rate = (df[df['likely_intersection'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(\"\\n2. likely_intersection (crash density >50):\")\n",
    "print(f\"   Intersection locations: {df['likely_intersection'].sum():,} crashes ({df['likely_intersection'].mean()*100:.1f}%)\")\n",
    "print(f\"   Intersection HIGH rate: {intersection_high_rate:.2f}%\")\n",
    "print(f\"   Non-intersection HIGH rate: {non_intersection_high_rate:.2f}%\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SPEED-RELATED RISK (High-Volume Roads)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPEED-RELATED RISK INDICATORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High-speed road indicator (major highways + main roads)\n",
    "df['high_speed_road'] = df['road_type_proxy'].isin(['MAJOR_HIGHWAY', 'MAIN_ROAD']).astype(int)\n",
    "\n",
    "high_speed_high_rate = (df[df['high_speed_road'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "low_speed_high_rate = (df[df['high_speed_road'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(\"\\n3. high_speed_road (major highways + main roads):\")\n",
    "print(f\"   High-speed road crashes: {df['high_speed_road'].sum():,} ({df['high_speed_road'].mean()*100:.1f}%)\")\n",
    "print(f\"   High-speed HIGH rate: {high_speed_high_rate:.2f}%\")\n",
    "print(f\"   Low-speed HIGH rate: {low_speed_high_rate:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# GEOGRAPHIC RISK ZONES (Central vs Peripheral)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GEOGRAPHIC RISK ZONES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate distance from Nairobi CBD (approximately -1.2864, 36.8172)\n",
    "nairobi_cbd_lat = -1.2864\n",
    "nairobi_cbd_lon = 36.8172\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate distance between two points in kilometers\n",
    "    \"\"\"\n",
    "    from math import radians, sin, cos, sqrt, atan2\n",
    "    \n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "df['distance_from_cbd_km'] = df.apply(\n",
    "    lambda row: haversine_distance(row['latitude'], row['longitude'], \n",
    "                                   nairobi_cbd_lat, nairobi_cbd_lon),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Classify into zones\n",
    "def classify_zone(distance_km):\n",
    "    \"\"\"\n",
    "    Classify location into geographic zones\n",
    "    \"\"\"\n",
    "    if distance_km < 5:\n",
    "        return 'CBD_CORE'\n",
    "    elif distance_km < 15:\n",
    "        return 'INNER_SUBURBS'\n",
    "    elif distance_km < 30:\n",
    "        return 'OUTER_SUBURBS'\n",
    "    else:\n",
    "        return 'PERIPHERAL'\n",
    "\n",
    "df['geographic_zone'] = df['distance_from_cbd_km'].apply(classify_zone)\n",
    "\n",
    "print(\"\\n4. geographic_zone (distance from Nairobi CBD):\")\n",
    "print(df['geographic_zone'].value_counts().sort_index())\n",
    "\n",
    "# Analyze severity by zone\n",
    "zone_severity = df.groupby('geographic_zone')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ")\n",
    "print(\"\\n   HIGH severity rate by zone:\")\n",
    "for zone in ['CBD_CORE', 'INNER_SUBURBS', 'OUTER_SUBURBS', 'PERIPHERAL']:\n",
    "    if zone in zone_severity.index:\n",
    "        print(f\"   - {zone:17s}: {zone_severity[zone]:6.2f}%\")\n",
    "\n",
    "print(\"\\n5. distance_from_cbd_km (continuous):\")\n",
    "print(f\"   Range: {df['distance_from_cbd_km'].min():.2f} to {df['distance_from_cbd_km'].max():.2f} km\")\n",
    "print(f\"   Mean: {df['distance_from_cbd_km'].mean():.2f} km\")\n",
    "print(f\"   Median: {df['distance_from_cbd_km'].median():.2f} km\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPOUND INFRASTRUCTURE RISK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPOUND INFRASTRUCTURE RISK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High-risk infrastructure: intersection + high-speed road\n",
    "df['high_risk_infrastructure'] = ((df['likely_intersection'] == 1) & \n",
    "                                   (df['high_speed_road'] == 1)).astype(int)\n",
    "\n",
    "infra_risk_high_rate = (df[df['high_risk_infrastructure'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "baseline_infra_rate = (df[df['high_risk_infrastructure'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(\"\\n6. high_risk_infrastructure (intersection × high-speed road):\")\n",
    "print(f\"   High-risk infrastructure: {df['high_risk_infrastructure'].sum():,} crashes\")\n",
    "print(f\"   HIGH rate with risk: {infra_risk_high_rate:.2f}%\")\n",
    "print(f\"   HIGH rate baseline: {baseline_infra_rate:.2f}%\")\n",
    "if baseline_infra_rate > 0:\n",
    "    print(f\"   Risk multiplier: {infra_risk_high_rate/baseline_infra_rate:.2f}x\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY OF ROAD INFRASTRUCTURE FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROAD INFRASTRUCTURE FEATURES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "road_features = [\n",
    "    'road_type_proxy',\n",
    "    'likely_intersection',\n",
    "    'high_speed_road',\n",
    "    'geographic_zone',\n",
    "    'distance_from_cbd_km',\n",
    "    'high_risk_infrastructure'\n",
    "]\n",
    "\n",
    "print(f\"\\n Created {len(road_features)} road infrastructure features:\")\n",
    "for i, feature in enumerate(road_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(f\"\\n Road infrastructure features engineered\")\n",
    "print(f\" Total features now: {len(df.columns)}\")\n",
    "print(f\" Feature engineering complete!\")\n",
    "print(f\"\\n Ready for train/validation/test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c5c997-af43-4571-95c5-0e131959ebeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING FINAL FEATURE SET FOR MODELING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "FEATURE CATEGORIZATION\n",
      "======================================================================\n",
      "\n",
      " Features to EXCLUDE: 8\n",
      "  - crash_id\n",
      "  - crash_datetime\n",
      "  - crash_date\n",
      "  - day_name\n",
      "  - lat_bin\n",
      "  - lon_bin\n",
      "  - grid_cell\n",
      "  - severity_4class\n",
      "\n",
      "NUMERIC features: 24\n",
      "  latitude\n",
      "  longitude\n",
      "  hour\n",
      "  day_of_week\n",
      "  month\n",
      "  year\n",
      "  is_weekend\n",
      "  hour_severity_rate\n",
      "  day_severity_rate\n",
      "  month_severity_rate\n",
      "  is_night\n",
      "  is_rush_hour\n",
      "  crashes_at_location\n",
      "  high_rate_at_location\n",
      "  high_risk_location\n",
      "  dangerous_time\n",
      "  high_risk_location_dangerous_time\n",
      "  rain_risk_score\n",
      "  high_rain_risk\n",
      "  rain_darkness_risk\n",
      "  likely_intersection\n",
      "  high_speed_road\n",
      "  distance_from_cbd_km\n",
      "  high_risk_infrastructure\n",
      "\n",
      "CATEGORICAL features: 5\n",
      " location_risk_category (4 categories)\n",
      " season (3 categories)\n",
      " daylight_status (2 categories)\n",
      " road_type_proxy (4 categories)\n",
      " geographic_zone (4 categories)\n",
      "\n",
      " TARGET variable: severity_binary\n",
      "\n",
      "======================================================================\n",
      "ENCODING CATEGORICAL FEATURES\n",
      "======================================================================\n",
      "\n",
      "Before encoding: 38 columns\n",
      "After encoding:  50 columns\n",
      "\n",
      "Encoding details:\n",
      "  location_risk_category: 4 categories → 4 binary columns\n",
      "  season: 3 categories → 3 binary columns\n",
      "  daylight_status: 2 categories → 2 binary columns\n",
      "  road_type_proxy: 4 categories → 4 binary columns\n",
      "  geographic_zone: 4 categories → 4 binary columns\n",
      "\n",
      "======================================================================\n",
      "CREATING FINAL FEATURE MATRIX\n",
      "======================================================================\n",
      "\n",
      "Final feature matrix shape: (31064, 41)\n",
      "  - Samples: 31,064\n",
      "  - Features: 41\n",
      "\n",
      "Target distribution:\n",
      "  - LOW (0):  27,180 (87.50%)\n",
      "  - HIGH (1): 3,884 (12.50%)\n",
      "  - Imbalance ratio: 7.00:1\n",
      "\n",
      "======================================================================\n",
      "DATA QUALITY FINAL CHECK\n",
      "======================================================================\n",
      "\n",
      "Missing values in features (X): 0\n",
      "Missing values in target (y): 0\n",
      "\n",
      " No missing values - dataset is clean!\n",
      "\n",
      "======================================================================\n",
      "FEATURE INVENTORY\n",
      "======================================================================\n",
      "\n",
      " Total predictive features: 41\n",
      "  - Numeric features: 24\n",
      "  - Encoded categorical: 17\n",
      "\n",
      " Feature preparation complete\n",
      " Ready for train/validation/test split\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 6: Prepare Final Feature Set for Modeling\n",
    "\n",
    "1. Select only predictive features (remove identifiers)\n",
    "2. Encode categorical variables\n",
    "3. Verify no data leakage\n",
    "4. Prepare for train/validation/test split\n",
    "\n",
    "Final feature set will include:\n",
    "- Numeric features (continuous and binary indicators)\n",
    "- Encoded categorical features (one-hot encoding)\n",
    "- Target variable (severity_binary)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING FINAL FEATURE SET FOR MODELING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY FEATURE CATEGORIES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE CATEGORIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Features to EXCLUDE (identifiers, not predictive)\n",
    "exclude_features = [\n",
    "    'crash_id',           # Unique identifier\n",
    "    'crash_datetime',     # Redundant (hour/day/month extracted)\n",
    "    'crash_date',         # Redundant\n",
    "    'day_name',           # Redundant (day_of_week is numeric version)\n",
    "    'lat_bin',            # Redundant (crashes_at_location derived from this)\n",
    "    'lon_bin',            # Redundant\n",
    "    'grid_cell',          # Redundant (string version of lat_bin/lon_bin)\n",
    "    'severity_4class'     # Not using 4-class, only binary\n",
    "]\n",
    "\n",
    "# Numeric features (already in usable format)\n",
    "numeric_features = [\n",
    "    # Location\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    # Temporal original\n",
    "    'hour',\n",
    "    'day_of_week',\n",
    "    'month',\n",
    "    'year',\n",
    "    'is_weekend',\n",
    "    # Temporal engineered\n",
    "    'hour_severity_rate',\n",
    "    'day_severity_rate',\n",
    "    'month_severity_rate',\n",
    "    'is_night',\n",
    "    'is_rush_hour',\n",
    "    # Spatial original\n",
    "    'crashes_at_location',\n",
    "    'high_rate_at_location',\n",
    "    # Spatial engineered\n",
    "    'high_risk_location',\n",
    "    'dangerous_time',\n",
    "    'high_risk_location_dangerous_time',\n",
    "    # Weather\n",
    "    'rain_risk_score',\n",
    "    'high_rain_risk',\n",
    "    'rain_darkness_risk',\n",
    "    # Road infrastructure\n",
    "    'likely_intersection',\n",
    "    'high_speed_road',\n",
    "    'distance_from_cbd_km',\n",
    "    'high_risk_infrastructure'\n",
    "]\n",
    "\n",
    "# Categorical features (need encoding)\n",
    "categorical_features = [\n",
    "    'location_risk_category',  # 4 categories\n",
    "    'season',                  # 3 categories\n",
    "    'daylight_status',         # 2 categories\n",
    "    'road_type_proxy',         # 4 categories\n",
    "    'geographic_zone'          # 4 categories\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'severity_binary'\n",
    "\n",
    "print(f\"\\n Features to EXCLUDE: {len(exclude_features)}\")\n",
    "for feature in exclude_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\nNUMERIC features: {len(numeric_features)}\")\n",
    "for feature in numeric_features:\n",
    "    print(f\"  {feature}\")\n",
    "\n",
    "print(f\"\\nCATEGORICAL features: {len(categorical_features)}\")\n",
    "for feature in categorical_features:\n",
    "    print(f\" {feature} ({df[feature].nunique()} categories)\")\n",
    "\n",
    "print(f\"\\n TARGET variable: {target}\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENCODE CATEGORICAL FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=False)\n",
    "\n",
    "print(f\"\\nBefore encoding: {len(df.columns)} columns\")\n",
    "print(f\"After encoding:  {len(df_encoded.columns)} columns\")\n",
    "\n",
    "# Show encoding results\n",
    "print(\"\\nEncoding details:\")\n",
    "for feature in categorical_features:\n",
    "    original_categories = df[feature].nunique()\n",
    "    encoded_cols = [col for col in df_encoded.columns if col.startswith(feature + '_')]\n",
    "    print(f\"  {feature}: {original_categories} categories → {len(encoded_cols)} binary columns\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE FINAL FEATURE SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING FINAL FEATURE MATRIX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get all encoded categorical column names\n",
    "encoded_categorical_cols = []\n",
    "for feature in categorical_features:\n",
    "    encoded_categorical_cols.extend([col for col in df_encoded.columns if col.startswith(feature + '_')])\n",
    "\n",
    "# Combine numeric and encoded categorical features\n",
    "all_features = numeric_features + encoded_categorical_cols\n",
    "\n",
    "# Create final feature matrix X and target y\n",
    "X = df_encoded[all_features].copy()\n",
    "y = df_encoded[target].copy()\n",
    "\n",
    "# Convert target to binary numeric (0=LOW, 1=HIGH)\n",
    "y_numeric = (y == 'HIGH').astype(int)\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "print(f\"  - Samples: {X.shape[0]:,}\")\n",
    "print(f\"  - Features: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  - LOW (0):  {(y_numeric == 0).sum():,} ({(y_numeric == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_numeric == 1).sum():,} ({(y_numeric == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  - Imbalance ratio: {(y_numeric == 0).sum() / (y_numeric == 1).sum():.2f}:1\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY NO MISSING VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY FINAL CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_in_X = X.isnull().sum().sum()\n",
    "missing_in_y = y_numeric.isnull().sum()\n",
    "\n",
    "print(f\"\\nMissing values in features (X): {missing_in_X}\")\n",
    "print(f\"Missing values in target (y): {missing_in_y}\")\n",
    "\n",
    "if missing_in_X == 0 and missing_in_y == 0:\n",
    "    print(\"\\n No missing values - dataset is clean!\")\n",
    "else:\n",
    "    print(\"\\n WARNING: Missing values detected - need to handle before modeling\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE FEATURE NAMES FOR LATER USE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE INVENTORY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Total predictive features: {len(all_features)}\")\n",
    "print(f\"  - Numeric features: {len(numeric_features)}\")\n",
    "print(f\"  - Encoded categorical: {len(encoded_categorical_cols)}\")\n",
    "\n",
    "print(f\"\\n Feature preparation complete\")\n",
    "print(f\" Ready for train/validation/test split\")\n",
    "\n",
    "# Store for next cell\n",
    "feature_names = all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355b262c-d0d9-4148-8105-c95f084c3d03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAIN/VALIDATION/TEST SPLIT + SMOTE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 1: INITIAL SPLIT (Train 85% + Test 15%)\n",
      "======================================================================\n",
      "\n",
      "After first split:\n",
      "  - Temp set (train+val): 26,404 samples (85.0%)\n",
      "  - Test set:             4,660 samples (15.0%)\n",
      "\n",
      "Test set class distribution:\n",
      "  - LOW (0):  4,077 (87.49%)\n",
      "  - HIGH (1): 583 (12.51%)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: SPLIT TEMP INTO TRAIN (70%) + VALIDATION (15%)\n",
      "======================================================================\n",
      "\n",
      "After second split:\n",
      "  - Train set:      21,756 samples (70.0%)\n",
      "  - Validation set: 4,648 samples (15.0%)\n",
      "  - Test set:       4,660 samples (15.0%)\n",
      "\n",
      "Train set class distribution (BEFORE SMOTE):\n",
      "  - LOW (0):  19,036 (87.50%)\n",
      "  - HIGH (1): 2,720 (12.50%)\n",
      "  - Imbalance ratio: 7.00:1\n",
      "\n",
      "Validation set class distribution (natural):\n",
      "  - LOW (0):  4,067 (87.50%)\n",
      "  - HIGH (1): 581 (12.50%)\n",
      "\n",
      "======================================================================\n",
      "STEP 3: APPLY SMOTE TO TRAINING SET ONLY\n",
      "======================================================================\n",
      "\n",
      "  CRITICAL: SMOTE is applied ONLY to training data!\n",
      "   Validation and test sets keep natural class distribution.\n",
      "   This ensures realistic performance estimates.\n",
      "\n",
      "Training set AFTER SMOTE:\n",
      "  - Total samples: 38,072 (increased from 21,756)\n",
      "  - LOW (0):  19,036 (50.00%)\n",
      "  - HIGH (1): 19,036 (50.00%)\n",
      "  - Imbalance ratio: 1.00:1\n",
      "\n",
      "  SMOTE created 16,316 synthetic HIGH severity samples\n",
      "  Training set is now perfectly balanced (1:1 ratio)\n",
      "\n",
      "======================================================================\n",
      "FINAL DATASET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. TRAINING SET (SMOTE-balanced):\n",
      "   Shape: (38072, 41)\n",
      "   LOW:  19,036 samples\n",
      "   HIGH: 19,036 samples\n",
      "   Purpose: Model training with balanced classes\n",
      "\n",
      "2. VALIDATION SET (natural distribution):\n",
      "   Shape: (4648, 41)\n",
      "   LOW:  4,067 samples (87.50%)\n",
      "   HIGH: 581 samples (12.50%)\n",
      "   Purpose: Hyperparameter tuning and model selection\n",
      "\n",
      "3. TEST SET (natural distribution):\n",
      "   Shape: (4660, 41)\n",
      "   LOW:  4,077 samples (87.49%)\n",
      "   HIGH: 583 samples (12.51%)\n",
      "   Purpose: Final unbiased performance evaluation\n",
      "\n",
      "======================================================================\n",
      "KEY PRINCIPLES FOLLOWED\n",
      "======================================================================\n",
      "\n",
      " Stratified splitting maintains class proportions\n",
      " SMOTE applied ONLY to training set (prevents data leakage)\n",
      " Validation and test sets have natural class distribution\n",
      " Test set completely held out (never seen during training)\n",
      " Model will be evaluated on realistic class imbalance\n",
      "\n",
      " Data splitting and balancing complete!\n",
      " Ready to save datasets for modeling notebook\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 7: Train/Validation/Test Split + SMOTE\n",
    "\n",
    "Proper ML workflow requires:\n",
    "1. Split into Train (70%), Validation (15%), Test (15%)\n",
    "2. Apply SMOTE ONLY to training set (prevent data leakage)\n",
    "3. Keep validation and test sets with natural class distribution\n",
    "\n",
    "Why this matters:\n",
    "- Training set: Used to fit model (SMOTE applied here)\n",
    "- Validation set: Used to tune hyperparameters (natural distribution)\n",
    "- Test set: Final performance evaluation (natural distribution)\n",
    "\n",
    "SMOTE must NEVER touch validation or test sets, or we get artificially\n",
    "inflated performance estimates that won't work in real deployment.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT + SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: INITIAL SPLIT (Train vs Test)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: INITIAL SPLIT (Train 85% + Test 15%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First split: separate test set (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y_numeric,\n",
    "    test_size=0.15,\n",
    "    stratify=y_numeric,  # Maintain class proportions\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter first split:\")\n",
    "print(f\"  - Temp set (train+val): {len(X_temp):,} samples ({len(X_temp)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Test set:             {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(f\"  - LOW (0):  {(y_test == 0).sum():,} ({(y_test == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_test == 1).sum():,} ({(y_test == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: SPLIT TEMP INTO TRAIN AND VALIDATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: SPLIT TEMP INTO TRAIN (70%) + VALIDATION (15%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Second split: train (70% of total) vs validation (15% of total)\n",
    "# From temp (85%), we want: train=70/85=82.35%, val=15/85=17.65%\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.176,  # 15% of total = 17.6% of temp\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter second split:\")\n",
    "print(f\"  - Train set:      {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Validation set: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Test set:       {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain set class distribution (BEFORE SMOTE):\")\n",
    "print(f\"  - LOW (0):  {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  - Imbalance ratio: {(y_train == 0).sum() / (y_train == 1).sum():.2f}:1\")\n",
    "\n",
    "print(f\"\\nValidation set class distribution (natural):\")\n",
    "print(f\"  - LOW (0):  {(y_val == 0).sum():,} ({(y_val == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_val == 1).sum():,} ({(y_val == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: APPLY SMOTE TO TRAINING SET ONLY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: APPLY SMOTE TO TRAINING SET ONLY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n  CRITICAL: SMOTE is applied ONLY to training data!\")\n",
    "print(\"   Validation and test sets keep natural class distribution.\")\n",
    "print(\"   This ensures realistic performance estimates.\\n\")\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Training set AFTER SMOTE:\")\n",
    "print(f\"  - Total samples: {len(X_train_balanced):,} (increased from {len(X_train):,})\")\n",
    "print(f\"  - LOW (0):  {(y_train_balanced == 0).sum():,} ({(y_train_balanced == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_train_balanced == 1).sum():,} ({(y_train_balanced == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  - Imbalance ratio: {(y_train_balanced == 0).sum() / (y_train_balanced == 1).sum():.2f}:1\")\n",
    "\n",
    "synthetic_samples_added = len(X_train_balanced) - len(X_train)\n",
    "print(f\"\\n  SMOTE created {synthetic_samples_added:,} synthetic HIGH severity samples\")\n",
    "print(f\"  Training set is now perfectly balanced (1:1 ratio)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL DATASET SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. TRAINING SET (SMOTE-balanced):\")\n",
    "print(f\"   Shape: {X_train_balanced.shape}\")\n",
    "print(f\"   LOW:  {(y_train_balanced == 0).sum():,} samples\")\n",
    "print(f\"   HIGH: {(y_train_balanced == 1).sum():,} samples\")\n",
    "print(f\"   Purpose: Model training with balanced classes\")\n",
    "\n",
    "print(\"\\n2. VALIDATION SET (natural distribution):\")\n",
    "print(f\"   Shape: {X_val.shape}\")\n",
    "print(f\"   LOW:  {(y_val == 0).sum():,} samples ({(y_val == 0).mean()*100:.2f}%)\")\n",
    "print(f\"   HIGH: {(y_val == 1).sum():,} samples ({(y_val == 1).mean()*100:.2f}%)\")\n",
    "print(f\"   Purpose: Hyperparameter tuning and model selection\")\n",
    "\n",
    "print(\"\\n3. TEST SET (natural distribution):\")\n",
    "print(f\"   Shape: {X_test.shape}\")\n",
    "print(f\"   LOW:  {(y_test == 0).sum():,} samples ({(y_test == 0).mean()*100:.2f}%)\")\n",
    "print(f\"   HIGH: {(y_test == 1).sum():,} samples ({(y_test == 1).mean()*100:.2f}%)\")\n",
    "print(f\"   Purpose: Final unbiased performance evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY PRINCIPLES FOLLOWED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n Stratified splitting maintains class proportions\")\n",
    "print(\" SMOTE applied ONLY to training set (prevents data leakage)\")\n",
    "print(\" Validation and test sets have natural class distribution\")\n",
    "print(\" Test set completely held out (never seen during training)\")\n",
    "print(\" Model will be evaluated on realistic class imbalance\")\n",
    "\n",
    "print(\"\\n Data splitting and balancing complete!\")\n",
    "print(\" Ready to save datasets for modeling notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e81d99c8-0945-4541-a776-5e4db9f6b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING PROCESSED DATASETS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SAVING DATASETS AS CSV\n",
      "======================================================================\n",
      "\n",
      " Training set:   38,072 rows × 42 columns\n",
      " Validation set: 4,648 rows × 42 columns\n",
      " Test set:       4,660 rows × 42 columns\n",
      "\n",
      "======================================================================\n",
      "SAVING NUMPY ARRAYS\n",
      "======================================================================\n",
      "\n",
      "✓ Numpy arrays saved\n",
      "  - X_train_balanced.npy, y_train_balanced.npy\n",
      "  - X_val.npy, y_val.npy\n",
      "  - X_test.npy, y_test.npy\n",
      "\n",
      "======================================================================\n",
      "SAVING FEATURE METADATA\n",
      "======================================================================\n",
      "\n",
      "✓ Feature metadata saved\n",
      "  - 41 feature names\n",
      "  - Dataset sizes recorded\n",
      "\n",
      "======================================================================\n",
      "NOTEBOOK 02 COMPLETE\n",
      "======================================================================\n",
      "\n",
      " Data cleaning & feature engineering complete!\n",
      " 41 dispatch-time features created\n",
      " 38,072 training samples (SMOTE-balanced)\n",
      " 4,648 validation samples\n",
      " 4,660 test samples\n",
      "\n",
      " Ready for model training in Notebook 03!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 8: Save Processed Datasets\n",
    "\n",
    "Save prepared datasets for modeling notebook:\n",
    "1. Training set (SMOTE-balanced)\n",
    "2. Validation set (natural distribution)\n",
    "3. Test set (natural distribution)\n",
    "4. Feature metadata\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING PROCESSED DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create features directory if it doesn't exist\n",
    "os.makedirs('../data/features', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE DATASETS AS CSV FILES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING DATASETS AS CSV\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create DataFrames with feature names\n",
    "train_df = pd.DataFrame(X_train_balanced, columns=feature_names)\n",
    "train_df['severity_binary'] = y_train_balanced\n",
    "\n",
    "val_df = pd.DataFrame(X_val, columns=feature_names)\n",
    "val_df['severity_binary'] = y_val\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "test_df['severity_binary'] = y_test\n",
    "\n",
    "# Save to CSV\n",
    "train_df.to_csv('../data/features/train_balanced.csv', index=False)\n",
    "val_df.to_csv('../data/features/validation.csv', index=False)\n",
    "test_df.to_csv('../data/features/test.csv', index=False)\n",
    "\n",
    "print(f\"\\n Training set:   {train_df.shape[0]:,} rows × {train_df.shape[1]} columns\")\n",
    "print(f\" Validation set: {val_df.shape[0]:,} rows × {val_df.shape[1]} columns\")\n",
    "print(f\" Test set:       {test_df.shape[0]:,} rows × {test_df.shape[1]} columns\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE NUMPY ARRAYS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING NUMPY ARRAYS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.save('../data/features/X_train_balanced.npy', X_train_balanced)\n",
    "np.save('../data/features/y_train_balanced.npy', y_train_balanced)\n",
    "np.save('../data/features/X_val.npy', X_val)\n",
    "np.save('../data/features/y_val.npy', y_val)\n",
    "np.save('../data/features/X_test.npy', X_test)\n",
    "np.save('../data/features/y_test.npy', y_test)\n",
    "\n",
    "print(\"\\n✓ Numpy arrays saved\")\n",
    "print(\"  - X_train_balanced.npy, y_train_balanced.npy\")\n",
    "print(\"  - X_val.npy, y_val.npy\")\n",
    "print(\"  - X_test.npy, y_test.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE FEATURE METADATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING FEATURE METADATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "feature_metadata = {\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names),\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'n_train': len(X_train_balanced),\n",
    "    'n_val': len(X_val),\n",
    "    'n_test': len(X_test)\n",
    "}\n",
    "\n",
    "joblib.dump(feature_metadata, '../data/features/feature_metadata.pkl')\n",
    "\n",
    "print(f\"\\n✓ Feature metadata saved\")\n",
    "print(f\"  - {len(feature_names)} feature names\")\n",
    "print(f\"  - Dataset sizes recorded\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK 02 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Data cleaning & feature engineering complete!\")\n",
    "print(f\" 41 dispatch-time features created\")\n",
    "print(f\" {len(X_train_balanced):,} training samples (SMOTE-balanced)\")\n",
    "print(f\" {len(X_val):,} validation samples\")\n",
    "print(f\" {len(X_test):,} test samples\")\n",
    "print(\"\\n Ready for model training in Notebook 03!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a65a06-013c-4299-aa77-650c27ae3b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
