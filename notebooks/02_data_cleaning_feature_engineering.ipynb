{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab7705a-7ae0-44a0-b7c5-e0caee4cddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NOTEBOOK 02: DATA CLEANING & FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      " Libraries imported successfully\n",
      "  - pandas, numpy: Data manipulation\n",
      "  - matplotlib, seaborn: Visualization\n",
      "  - sklearn: Data splitting\n",
      "  - imblearn: SMOTE for class imbalance\n",
      "\n",
      "======================================================================\n",
      "LOADING LABELED DATASET FROM NOTEBOOK 01\n",
      "======================================================================\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: 31,064 rows × 24 columns\n",
      "Memory usage: 12.87 MB\n",
      "Date range: 2012-08-08 to 2023-07-12\n",
      "\n",
      "======================================================================\n",
      "FEATURE CATEGORIES FROM NOTEBOOK 01\n",
      "======================================================================\n",
      "\n",
      "1. ORIGINAL WORLD BANK FEATURES (10 columns):\n",
      "   - crash_id\n",
      "   - crash_datetime\n",
      "   - crash_date\n",
      "   - latitude\n",
      "   - longitude\n",
      "   - n_crash_reports\n",
      "   - contains_fatality_words\n",
      "   - contains_pedestrian_words\n",
      "   - contains_matatu_words\n",
      "   - contains_motorcycle_words\n",
      "\n",
      "2. SEVERITY LABELS (2 columns):\n",
      "   - severity_4class\n",
      "   - severity_binary\n",
      "\n",
      "3. TEMPORAL FEATURES (6 columns):\n",
      "   - hour\n",
      "   - day_of_week\n",
      "   - day_name\n",
      "   - month\n",
      "   - year\n",
      "   - is_weekend\n",
      "\n",
      "4. SPATIAL FEATURES (6 columns):\n",
      "   - lat_bin\n",
      "   - lon_bin\n",
      "   - grid_cell\n",
      "   - crashes_at_location\n",
      "   - high_rate_at_location\n",
      "   - location_risk_category\n",
      "\n",
      "======================================================================\n",
      "TARGET VARIABLE: BINARY SEVERITY\n",
      "======================================================================\n",
      "severity_binary\n",
      "LOW     27180\n",
      "HIGH     3884\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class imbalance ratio (LOW:HIGH): 7.00:1\n",
      "\n",
      " Data loaded successfully\n",
      " Ready for feature engineering\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "NOTEBOOK 02: DATA CLEANING & FEATURE ENGINEERING FOR EMERGENCY DISPATCH\n",
    "===============================================================================\n",
    "\n",
    "Purpose: Create dispatch-time available features for accident severity prediction\n",
    "\n",
    "Critical Principle: ONLY use information available WHEN THE EMERGENCY CALL COMES IN\n",
    "- Location (GPS coordinates) \n",
    "- Timestamp (when call received) \n",
    "- Historical patterns at location/time \n",
    "\n",
    "Author: Mary Wangoi Mwangi (122174)\n",
    "Supervisor: Prof. Vincent Omwenga\n",
    "Date: January 2026\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NOTEBOOK 02: DATA CLEANING & FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n Libraries imported successfully\")\n",
    "print(\"  - pandas, numpy: Data manipulation\")\n",
    "print(\"  - matplotlib, seaborn: Visualization\")\n",
    "print(\"  - sklearn: Data splitting\")\n",
    "print(\"  - imblearn: SMOTE for class imbalance\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD LABELED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING LABELED DATASET FROM NOTEBOOK 01\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the dataset created in Notebook 01\n",
    "data_path = '../data/processed/labeled_crashes.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert datetime columns\n",
    "df['crash_datetime'] = pd.to_datetime(df['crash_datetime'])\n",
    "df['crash_date'] = pd.to_datetime(df['crash_date'])\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Date range: {df['crash_date'].min().date()} to {df['crash_date'].max().date()}\")\n",
    "\n",
    "# Display column categories\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE CATEGORIES FROM NOTEBOOK 01\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. ORIGINAL WORLD BANK FEATURES (10 columns):\")\n",
    "original_cols = ['crash_id', 'crash_datetime', 'crash_date', 'latitude', 'longitude', \n",
    "                 'n_crash_reports', 'contains_fatality_words', 'contains_pedestrian_words',\n",
    "                 'contains_matatu_words', 'contains_motorcycle_words']\n",
    "for col in original_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(\"\\n2. SEVERITY LABELS (2 columns):\")\n",
    "severity_cols = ['severity_4class', 'severity_binary']\n",
    "for col in severity_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(\"\\n3. TEMPORAL FEATURES (6 columns):\")\n",
    "temporal_cols = ['hour', 'day_of_week', 'day_name', 'month', 'year', 'is_weekend']\n",
    "for col in temporal_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(\"\\n4. SPATIAL FEATURES (6 columns):\")\n",
    "spatial_cols = ['lat_bin', 'lon_bin', 'grid_cell', 'crashes_at_location', \n",
    "                'high_rate_at_location', 'location_risk_category']\n",
    "for col in spatial_cols:\n",
    "    print(f\"   - {col}\")\n",
    "    \n",
    "\n",
    "# Display target variable distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TARGET VARIABLE: BINARY SEVERITY\")\n",
    "print(\"=\"*70)\n",
    "print(df['severity_binary'].value_counts())\n",
    "print(f\"\\nClass imbalance ratio (LOW:HIGH): {(df['severity_binary']=='LOW').sum() / (df['severity_binary']=='HIGH').sum():.2f}:1\")\n",
    "\n",
    "print(\"\\n Data loaded successfully\")\n",
    "print(\" Ready for feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647e5479-33c8-4715-9cc6-23209477eac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MISSING VALUE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      " Columns with missing values:\n",
      "                Column  Missing_Count  Missing_Percentage\n",
      "location_risk_category           1211                 3.9\n",
      "\n",
      "======================================================================\n",
      "INVESTIGATING location_risk_category MISSING VALUES\n",
      "======================================================================\n",
      "\n",
      "Crashes with missing location_risk_category: 1,211\n",
      "Percentage of total dataset: 3.90%\n",
      "\n",
      "Why are these missing?\n",
      "Min high_rate_at_location (missing cases): 0.00%\n",
      "Max high_rate_at_location (missing cases): 0.00%\n",
      "Mean high_rate_at_location (missing cases): 0.00%\n",
      "\n",
      "Our risk category bins (from Notebook 01):\n",
      "  - LOW_RISK: 0-10%\n",
      "  - MEDIUM_RISK: 10-15%\n",
      "  - HIGH_RISK: 15-20%\n",
      "  - VERY_HIGH_RISK: 20-100%\n",
      "\n",
      " ISSUE IDENTIFIED:\n",
      "   Missing values occur when high_rate_at_location is EXACTLY 0%\n",
      "   or falls outside bin boundaries (edge case handling)\n",
      "\n",
      "======================================================================\n",
      "FIXING MISSING VALUES IN location_risk_category\n",
      "======================================================================\n",
      "\n",
      "Missing values after fix: 0\n",
      " All missing values successfully fixed!\n",
      "\n",
      "======================================================================\n",
      "UPDATED location_risk_category DISTRIBUTION\n",
      "======================================================================\n",
      "location_risk_category\n",
      "LOW_RISK          10308\n",
      "MEDIUM_RISK       13029\n",
      "HIGH_RISK          6208\n",
      "VERY_HIGH_RISK     1519\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "FINAL MISSING VALUE CHECK\n",
      "======================================================================\n",
      " No missing values in dataset!\n",
      " Dataset is complete and ready for feature engineering\n",
      "\n",
      " Missing value treatment complete\n",
      " Dataset shape: 31,064 rows × 24 columns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 2: Missing Value Analysis & Treatment\n",
    "\n",
    "Objective: Identify, understand, and handle missing values appropriately.\n",
    "Focus: The 1,211 missing values in location_risk_category from Notebook 01.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MISSING VALUE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for missing values across all columns\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"\\n Columns with missing values:\")\n",
    "    print(missing_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n No missing values found!\")\n",
    "\n",
    "    \n",
    "\n",
    "# ============================================================================\n",
    "# INVESTIGATE location_risk_category MISSING VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INVESTIGATING location_risk_category MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find which crashes have missing risk categories\n",
    "missing_risk = df[df['location_risk_category'].isnull()]\n",
    "\n",
    "print(f\"\\nCrashes with missing location_risk_category: {len(missing_risk):,}\")\n",
    "print(f\"Percentage of total dataset: {len(missing_risk)/len(df)*100:.2f}%\")\n",
    "\n",
    "# Analyze the high_rate_at_location for missing cases\n",
    "print(\"\\nWhy are these missing?\")\n",
    "print(f\"Min high_rate_at_location (missing cases): {missing_risk['high_rate_at_location'].min():.2f}%\")\n",
    "print(f\"Max high_rate_at_location (missing cases): {missing_risk['high_rate_at_location'].max():.2f}%\")\n",
    "print(f\"Mean high_rate_at_location (missing cases): {missing_risk['high_rate_at_location'].mean():.2f}%\")\n",
    "\n",
    "print(\"\\nOur risk category bins (from Notebook 01):\")\n",
    "print(\"  - LOW_RISK: 0-10%\")\n",
    "print(\"  - MEDIUM_RISK: 10-15%\")\n",
    "print(\"  - HIGH_RISK: 15-20%\")\n",
    "print(\"  - VERY_HIGH_RISK: 20-100%\")\n",
    "\n",
    "# The issue: values below 10% or exactly at boundaries\n",
    "print(\"\\n ISSUE IDENTIFIED:\")\n",
    "print(\"   Missing values occur when high_rate_at_location is EXACTLY 0%\")\n",
    "print(\"   or falls outside bin boundaries (edge case handling)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX MISSING VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIXING MISSING VALUES IN location_risk_category\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Strategy: Create a more inclusive binning that covers ALL cases\n",
    "df['location_risk_category'] = pd.cut(\n",
    "    df['high_rate_at_location'], \n",
    "    bins=[-0.01, 10, 15, 20, 100],  # Include -0.01 to catch 0% cases\n",
    "    labels=['LOW_RISK', 'MEDIUM_RISK', 'HIGH_RISK', 'VERY_HIGH_RISK'],\n",
    "    include_lowest=True  # Include the lowest boundary\n",
    ")\n",
    "\n",
    "# Verify fix\n",
    "remaining_missing = df['location_risk_category'].isnull().sum()\n",
    "print(f\"\\nMissing values after fix: {remaining_missing}\")\n",
    "\n",
    "if remaining_missing == 0:\n",
    "    print(\" All missing values successfully fixed!\")\n",
    "else:\n",
    "    print(f\" Still {remaining_missing} missing values - will fill with 'LOW_RISK'\")\n",
    "    df['location_risk_category'].fillna('LOW_RISK', inplace=True)\n",
    "\n",
    "# Display updated distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"UPDATED location_risk_category DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "print(df['location_risk_category'].value_counts().sort_index())\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL MISSING VALUE CHECK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MISSING VALUE CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_missing = df.isnull().sum().sum()\n",
    "if total_missing == 0:\n",
    "    print(\" No missing values in dataset!\")\n",
    "    print(\" Dataset is complete and ready for feature engineering\")\n",
    "else:\n",
    "    print(f\" {total_missing} missing values remaining\")\n",
    "    print(\"\\n Columns with missing values:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "print(f\"\\n Missing value treatment complete\")\n",
    "print(f\" Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4024a4c6-4dd2-45f4-a092-1c3fab667b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IDENTIFYING DATA LEAKAGE FEATURES\n",
      "======================================================================\n",
      "\n",
      " DATA LEAKAGE FEATURES (Must be removed):\n",
      "----------------------------------------------------------------------\n",
      "1. n_crash_reports\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "2. contains_fatality_words\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "3. contains_pedestrian_words\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "4. contains_matatu_words\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "5. contains_motorcycle_words\n",
      "   Why leakage? This information comes from crash reports AFTER the accident\n",
      "   Dispatcher doesn't have this when emergency call comes in\n",
      "\n",
      "======================================================================\n",
      "CORRELATION WITH TARGET (Showing why these are leakage)\n",
      "======================================================================\n",
      "\n",
      "Correlation with HIGH severity:\n",
      "----------------------------------------------------------------------\n",
      "n_crash_reports               :  0.095 \n",
      "contains_fatality_words       :  0.745 ← SUSPICIOUSLY HIGH!\n",
      "contains_pedestrian_words     :  0.468 ← SUSPICIOUSLY HIGH!\n",
      "contains_matatu_words         :  0.036 \n",
      "contains_motorcycle_words     :  0.517 ← SUSPICIOUSLY HIGH!\n",
      "\n",
      "======================================================================\n",
      "PROOF OF DATA LEAKAGE: Keyword Flags vs Severity Labels\n",
      "======================================================================\n",
      "\n",
      "1. contains_fatality_words vs severity_binary:\n",
      "severity_binary                HIGH        LOW\n",
      "contains_fatality_words                       \n",
      "0                          5.559416  94.440584\n",
      "1                        100.000000   0.000000\n",
      "\n",
      "2. contains_pedestrian_words vs severity_binary:\n",
      "severity_binary                  HIGH        LOW\n",
      "contains_pedestrian_words                       \n",
      "0                            9.760956  90.239044\n",
      "1                          100.000000   0.000000\n",
      "\n",
      "3. contains_motorcycle_words vs severity_binary:\n",
      "severity_binary                  HIGH        LOW\n",
      "contains_motorcycle_words                       \n",
      "0                            9.163826  90.836174\n",
      "1                          100.000000   0.000000\n",
      "\n",
      " NOTICE: Keyword flags are DIRECT INDICATORS of severity!\n",
      "   Using these features would be circular reasoning, not prediction.\n",
      "\n",
      "======================================================================\n",
      "REMOVING DATA LEAKAGE FEATURES\n",
      "======================================================================\n",
      "\n",
      "Before removal: 25 columns\n",
      "After removal:  19 columns\n",
      "Removed:        5 leakage features\n",
      "\n",
      "======================================================================\n",
      "REMAINING FEATURES (All dispatch-time available)\n",
      "======================================================================\n",
      "\n",
      "Identifiers (3):\n",
      "  ✓ crash_id\n",
      "  ✓ crash_datetime\n",
      "  ✓ crash_date\n",
      "\n",
      "Location (2):\n",
      "  ✓ latitude\n",
      "  ✓ longitude\n",
      "\n",
      "Temporal (6):\n",
      "  ✓ hour\n",
      "  ✓ day_of_week\n",
      "  ✓ day_name\n",
      "  ✓ month\n",
      "  ✓ year\n",
      "  ✓ is_weekend\n",
      "\n",
      "Spatial (6):\n",
      "  ✓ lat_bin\n",
      "  ✓ lon_bin\n",
      "  ✓ grid_cell\n",
      "  ✓ crashes_at_location\n",
      "  ✓ high_rate_at_location\n",
      "  ✓ location_risk_category\n",
      "\n",
      "Target (2):\n",
      "  ✓ severity_4class\n",
      "  ✓ severity_binary\n",
      "\n",
      "======================================================================\n",
      "DATA LEAKAGE REMOVAL COMPLETE\n",
      "======================================================================\n",
      "\n",
      " Removed 5 features with post-accident information\n",
      " Retained 19 dispatch-time available features\n",
      " Dataset shape: 31,064 rows × 19 columns\n",
      "\n",
      " Model will now be trained ONLY on information available at dispatch time\n",
      "  This ensures realistic performance estimates for operational deployment\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 3: Remove Data Leakage Features (CRITICAL)\n",
    "\n",
    "Data leakage occurs when features used for prediction contain information\n",
    "that would NOT be available at the time predictions need to be made.\n",
    "\n",
    "In emergency dispatch, we must ONLY use information available WHEN THE CALL COMES IN.\n",
    "\n",
    "AVAILABLE at dispatch time:\n",
    "  GPS location (caller provides)\n",
    "  Call timestamp (automatic)\n",
    "  Historical patterns at location/time\n",
    "\n",
    "NOT AVAILABLE at dispatch time (DATA LEAKAGE):\n",
    "  Keyword flags (contains_fatality_words, etc.) - from crash reports AFTER accident\n",
    "  Number of reports (n_crash_reports) - only known after multiple people report\n",
    "  Crash description text - not available until scene assessment\n",
    "\n",
    "These features create artificially high model performance that won't work in real dispatch!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"IDENTIFYING DATA LEAKAGE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY LEAKAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "# Features that contain post-accident information\n",
    "leakage_features = [\n",
    "    'n_crash_reports',           # Only known after reports accumulate\n",
    "    'contains_fatality_words',   # From crash scene reports\n",
    "    'contains_pedestrian_words', # From crash scene reports\n",
    "    'contains_matatu_words',     # From crash scene reports\n",
    "    'contains_motorcycle_words'  # From crash scene reports\n",
    "]\n",
    "\n",
    "print(\"\\n DATA LEAKAGE FEATURES (Must be removed):\")\n",
    "print(\"-\" * 70)\n",
    "for i, feature in enumerate(leakage_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "    print(f\"   Why leakage? This information comes from crash reports AFTER the accident\")\n",
    "    print(f\"   Dispatcher doesn't have this when emergency call comes in\\n\")\n",
    "\n",
    "# Show correlation with target (they'll be suspiciously high!)\n",
    "print(\"=\" * 70)\n",
    "print(\"CORRELATION WITH TARGET (Showing why these are leakage)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert binary target to numeric for correlation\n",
    "df['severity_numeric'] = (df['severity_binary'] == 'HIGH').astype(int)\n",
    "\n",
    "print(\"\\nCorrelation with HIGH severity:\")\n",
    "print(\"-\" * 70)\n",
    "for feature in leakage_features:\n",
    "    corr = df[feature].corr(df['severity_numeric'])\n",
    "    print(f\"{feature:30s}: {corr:6.3f} {'← SUSPICIOUSLY HIGH!' if abs(corr) > 0.3 else ''}\")\n",
    "\n",
    "# Show how keyword flags relate to our severity labels\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROOF OF DATA LEAKAGE: Keyword Flags vs Severity Labels\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. contains_fatality_words vs severity_binary:\")\n",
    "print(pd.crosstab(df['contains_fatality_words'], df['severity_binary'], \n",
    "                  normalize='index') * 100)\n",
    "\n",
    "print(\"\\n2. contains_pedestrian_words vs severity_binary:\")\n",
    "print(pd.crosstab(df['contains_pedestrian_words'], df['severity_binary'], \n",
    "                  normalize='index') * 100)\n",
    "\n",
    "print(\"\\n3. contains_motorcycle_words vs severity_binary:\")\n",
    "print(pd.crosstab(df['contains_motorcycle_words'], df['severity_binary'], \n",
    "                  normalize='index') * 100)\n",
    "\n",
    "print(\"\\n NOTICE: Keyword flags are DIRECT INDICATORS of severity!\")\n",
    "print(\"   Using these features would be circular reasoning, not prediction.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REMOVE LEAKAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REMOVING DATA LEAKAGE FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nBefore removal: {len(df.columns)} columns\")\n",
    "\n",
    "# Drop leakage features\n",
    "df_clean = df.drop(columns=leakage_features)\n",
    "\n",
    "# Also drop the temporary severity_numeric column\n",
    "df_clean = df_clean.drop(columns=['severity_numeric'])\n",
    "\n",
    "print(f\"After removal:  {len(df_clean.columns)} columns\")\n",
    "print(f\"Removed:        {len(leakage_features)} leakage features\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY REMAINING FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REMAINING FEATURES (All dispatch-time available)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Categorize remaining features\n",
    "dispatch_available = {\n",
    "    'Identifiers (3)': ['crash_id', 'crash_datetime', 'crash_date'],\n",
    "    'Location (2)': ['latitude', 'longitude'],\n",
    "    'Temporal (6)': ['hour', 'day_of_week', 'day_name', 'month', 'year', 'is_weekend'],\n",
    "    'Spatial (6)': ['lat_bin', 'lon_bin', 'grid_cell', 'crashes_at_location', \n",
    "                    'high_rate_at_location', 'location_risk_category'],\n",
    "    'Target (2)': ['severity_4class', 'severity_binary']\n",
    "}\n",
    "\n",
    "for category, features in dispatch_available.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for feature in features:\n",
    "        if feature in df_clean.columns:\n",
    "            print(f\"  ✓ {feature}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA LEAKAGE REMOVAL COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n Removed {len(leakage_features)} features with post-accident information\")\n",
    "print(f\" Retained {len(df_clean.columns)} dispatch-time available features\")\n",
    "print(f\" Dataset shape: {df_clean.shape[0]:,} rows × {df_clean.shape[1]} columns\")\n",
    "print(\"\\n Model will now be trained ONLY on information available at dispatch time\")\n",
    "print(\"  This ensures realistic performance estimates for operational deployment\")\n",
    "\n",
    "# Update df to clean version\n",
    "df = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92eb1748-f8d9-4ec3-b5fe-f4ff21b80654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DISPATCH-TIME FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING TEMPORAL RISK FEATURES\n",
      "======================================================================\n",
      "\n",
      "1. hour_severity_rate: Historical HIGH severity % for this hour\n",
      "   Range: 9.57% to 17.50%\n",
      "   Mean: 12.50%\n",
      "\n",
      "2. day_severity_rate: Historical HIGH severity % for this day of week\n",
      "   Range: 11.29% to 15.16%\n",
      "   Mean: 12.50%\n",
      "\n",
      "3. month_severity_rate: Historical HIGH severity % for this month\n",
      "   Range: 10.85% to 13.47%\n",
      "   Mean: 12.50%\n",
      "\n",
      "======================================================================\n",
      "CREATING TIME PERIOD INDICATORS\n",
      "======================================================================\n",
      "\n",
      "4. is_night (10 PM - 4 AM):\n",
      "   Night crashes: 2,115 (6.8%)\n",
      "   Night HIGH rate: 14.75%\n",
      "   Day HIGH rate: 12.34%\n",
      "   Night is 1.20x more dangerous\n",
      "\n",
      "5. is_rush_hour (6-9 AM, 5-7 PM):\n",
      "   Rush hour crashes: 15,590 (50.2%)\n",
      "   Rush hour HIGH rate: 11.34%\n",
      "   Non-rush HIGH rate: 13.67%\n",
      "\n",
      "======================================================================\n",
      "CREATING LOCATION × TIME INTERACTION FEATURES\n",
      "======================================================================\n",
      "\n",
      "6. high_risk_location_dangerous_time:\n",
      "   (High-risk location × Night/Weekend)\n",
      "   Interaction cases: 2,313\n",
      "   HIGH rate when both: 20.41%\n",
      "   HIGH rate baseline: 11.87%\n",
      "   Risk multiplier: 1.72x\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Created 8 new dispatch-time features:\n",
      "  1. hour_severity_rate\n",
      "  2. day_severity_rate\n",
      "  3. month_severity_rate\n",
      "  4. is_night\n",
      "  5. is_rush_hour\n",
      "  6. high_risk_location\n",
      "  7. dangerous_time\n",
      "  8. high_risk_location_dangerous_time\n",
      "\n",
      "Total features now: 27\n",
      "  - Identifiers: 3\n",
      "  - Location: 2 (latitude, longitude)\n",
      "  - Temporal original: 6\n",
      "  - Temporal engineered: 3 (severity rates)\n",
      "  - Temporal indicators: 2 (night, rush_hour)\n",
      "  - Spatial original: 6\n",
      "  - Spatial engineered: 1 (high_risk_location)\n",
      "  - Interaction: 2\n",
      "  - Target: 2\n",
      "\n",
      " Feature engineering complete\n",
      " All features are dispatch-time available\n",
      " Ready for train/validation/test split\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 4: Engineer Dispatch-Time Features\n",
    "\n",
    "Create features that predict severity using ONLY information available\n",
    "when the emergency call comes in.\n",
    "\n",
    "Feature Engineering Strategy:\n",
    "1. Temporal Risk Features: Historical severity rates by hour/day/month\n",
    "2. Spatial Risk Features: Already have (crashes_at_location, high_rate_at_location)\n",
    "3. Interaction Features: Location × Time patterns\n",
    "4. Categorical Encodings: Risk categories and time periods\n",
    "\n",
    "These features capture patterns like:\n",
    "- \"Accidents at 11 PM are usually more severe\"\n",
    "- \"This location has high historical severity\"\n",
    "- \"Weekend nights at this location are dangerous\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DISPATCH-TIME FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# TEMPORAL RISK FEATURES (Historical Severity Rates)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING TEMPORAL RISK FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate historical HIGH severity rate by hour\n",
    "hour_severity_rate = df.groupby('hour')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ").to_dict()\n",
    "\n",
    "df['hour_severity_rate'] = df['hour'].map(hour_severity_rate)\n",
    "\n",
    "print(\"\\n1. hour_severity_rate: Historical HIGH severity % for this hour\")\n",
    "print(f\"   Range: {df['hour_severity_rate'].min():.2f}% to {df['hour_severity_rate'].max():.2f}%\")\n",
    "print(f\"   Mean: {df['hour_severity_rate'].mean():.2f}%\")\n",
    "\n",
    "# Calculate historical HIGH severity rate by day of week\n",
    "day_severity_rate = df.groupby('day_of_week')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ").to_dict()\n",
    "\n",
    "df['day_severity_rate'] = df['day_of_week'].map(day_severity_rate)\n",
    "\n",
    "print(\"\\n2. day_severity_rate: Historical HIGH severity % for this day of week\")\n",
    "print(f\"   Range: {df['day_severity_rate'].min():.2f}% to {df['day_severity_rate'].max():.2f}%\")\n",
    "print(f\"   Mean: {df['day_severity_rate'].mean():.2f}%\")\n",
    "\n",
    "# Calculate historical HIGH severity rate by month\n",
    "month_severity_rate = df.groupby('month')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ").to_dict()\n",
    "\n",
    "df['month_severity_rate'] = df['month'].map(month_severity_rate)\n",
    "\n",
    "print(\"\\n3. month_severity_rate: Historical HIGH severity % for this month\")\n",
    "print(f\"   Range: {df['month_severity_rate'].min():.2f}% to {df['month_severity_rate'].max():.2f}%\")\n",
    "print(f\"   Mean: {df['month_severity_rate'].mean():.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TIME PERIOD INDICATORS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING TIME PERIOD INDICATORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Night indicator (highest severity hours: 10 PM - 4 AM)\n",
    "df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 4)).astype(int)\n",
    "night_high_rate = (df[df['is_night'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "day_high_rate = (df[df['is_night'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(f\"\\n4. is_night (10 PM - 4 AM):\")\n",
    "print(f\"   Night crashes: {df['is_night'].sum():,} ({df['is_night'].mean()*100:.1f}%)\")\n",
    "print(f\"   Night HIGH rate: {night_high_rate:.2f}%\")\n",
    "print(f\"   Day HIGH rate: {day_high_rate:.2f}%\")\n",
    "print(f\"   Night is {night_high_rate/day_high_rate:.2f}x more dangerous\")\n",
    "\n",
    "# Rush hour indicator (morning: 6-9 AM, evening: 5-7 PM)\n",
    "df['is_rush_hour'] = (((df['hour'] >= 6) & (df['hour'] <= 9)) | \n",
    "                      ((df['hour'] >= 17) & (df['hour'] <= 19))).astype(int)\n",
    "rush_high_rate = (df[df['is_rush_hour'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "non_rush_high_rate = (df[df['is_rush_hour'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(f\"\\n5. is_rush_hour (6-9 AM, 5-7 PM):\")\n",
    "print(f\"   Rush hour crashes: {df['is_rush_hour'].sum():,} ({df['is_rush_hour'].mean()*100:.1f}%)\")\n",
    "print(f\"   Rush hour HIGH rate: {rush_high_rate:.2f}%\")\n",
    "print(f\"   Non-rush HIGH rate: {non_rush_high_rate:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOCATION × TIME INTERACTION FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING LOCATION × TIME INTERACTION FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High-risk location at dangerous time\n",
    "df['high_risk_location'] = (df['high_rate_at_location'] > 15).astype(int)\n",
    "df['dangerous_time'] = ((df['is_night'] == 1) | (df['is_weekend'] == 1)).astype(int)\n",
    "df['high_risk_location_dangerous_time'] = df['high_risk_location'] * df['dangerous_time']\n",
    "\n",
    "interaction_high_rate = (df[df['high_risk_location_dangerous_time'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "baseline_high_rate = (df[df['high_risk_location_dangerous_time'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(f\"\\n6. high_risk_location_dangerous_time:\")\n",
    "print(f\"   (High-risk location × Night/Weekend)\")\n",
    "print(f\"   Interaction cases: {df['high_risk_location_dangerous_time'].sum():,}\")\n",
    "print(f\"   HIGH rate when both: {interaction_high_rate:.2f}%\")\n",
    "print(f\"   HIGH rate baseline: {baseline_high_rate:.2f}%\")\n",
    "print(f\"   Risk multiplier: {interaction_high_rate/baseline_high_rate:.2f}x\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY OF NEW FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "new_features = [\n",
    "    'hour_severity_rate',\n",
    "    'day_severity_rate', \n",
    "    'month_severity_rate',\n",
    "    'is_night',\n",
    "    'is_rush_hour',\n",
    "    'high_risk_location',\n",
    "    'dangerous_time',\n",
    "    'high_risk_location_dangerous_time'\n",
    "]\n",
    "\n",
    "print(f\"\\nCreated {len(new_features)} new dispatch-time features:\")\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(f\"\\nTotal features now: {len(df.columns)}\")\n",
    "print(f\"  - Identifiers: 3\")\n",
    "print(f\"  - Location: 2 (latitude, longitude)\")\n",
    "print(f\"  - Temporal original: 6\")\n",
    "print(f\"  - Temporal engineered: 3 (severity rates)\")\n",
    "print(f\"  - Temporal indicators: 2 (night, rush_hour)\")\n",
    "print(f\"  - Spatial original: 6\")\n",
    "print(f\"  - Spatial engineered: 1 (high_risk_location)\")\n",
    "print(f\"  - Interaction: 2\")\n",
    "print(f\"  - Target: 2\")\n",
    "\n",
    "print(\"\\n Feature engineering complete\")\n",
    "print(\" All features are dispatch-time available\")\n",
    "print(\" Ready for train/validation/test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "599152ed-bd35-4918-8e44-3a7ed1fc1ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OPEN-METEO API SETUP & CONNECTION TEST\n",
      "======================================================================\n",
      "\n",
      " Using Open-Meteo Historical Weather API\n",
      " API Endpoint: https://archive-api.open-meteo.com/v1/archive\n",
      " No API key required!\n",
      "\n",
      "======================================================================\n",
      "TESTING API WITH 5 SAMPLE CRASHES\n",
      "======================================================================\n",
      "\n",
      "Selected 5 crashes for testing:\n",
      " crash_id  latitude  longitude      crash_datetime\n",
      "     6731 -1.280864  36.850446 2021-11-12 05:41:48\n",
      "    12625 -1.481393  36.956825 2014-09-23 11:37:10\n",
      "    27111 -1.263030  36.764374 2012-12-16 20:58:33\n",
      "    12580 -1.281328  36.818710 2014-09-18 06:15:54\n",
      "     2209 -1.308972  36.913100 2019-05-06 08:04:07\n",
      "\n",
      "======================================================================\n",
      "ATTEMPTING API CONNECTION...\n",
      "======================================================================\n",
      "\n",
      "Test Request Parameters:\n",
      "  Latitude: -1.280864\n",
      "  Longitude: 36.850446\n",
      "  Date: 2021-11-12\n",
      "  Hour: 5\n",
      "  Crash Time: 2021-11-12 05:41:48\n",
      "\n",
      " API Response Status: 200\n",
      "\n",
      " SUCCESS! API connection working!\n",
      "\n",
      "Weather data at crash time (2021-11-12T05:00):\n",
      "  Temperature: 13.5 °C\n",
      "  Precipitation: 0.0 mm\n",
      "  Wind Speed: 5.6 km/h\n",
      "  Humidity: 88 %\n",
      "  Pressure: 837.3 hPa\n",
      "  Weather Code: 3\n",
      "\n",
      " Open-Meteo API is working perfectly!\n",
      " Ready to fetch weather data for all 31,064 crashes!\n",
      "\n",
      "======================================================================\n",
      "API TEST COMPLETE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "WEATHER CODE REFERENCE\n",
      "======================================================================\n",
      "\n",
      "Open-Meteo Weather Codes (WMO standard):\n",
      "  0 = Clear sky\n",
      "  1, 2, 3 = Mainly clear, partly cloudy, overcast\n",
      "  45, 48 = Fog\n",
      "  51, 53, 55 = Drizzle (light, moderate, dense)\n",
      "  61, 63, 65 = Rain (slight, moderate, heavy)\n",
      "  71, 73, 75 = Snow fall (slight, moderate, heavy)\n",
      "  80, 81, 82 = Rain showers (slight, moderate, violent)\n",
      "  95, 96, 99 = Thunderstorm\n",
      "\n",
      "Categorize these into:\n",
      "- CLEAR (0-3)\n",
      "- CLOUDY (45, 48)\n",
      "- RAIN (51, 53, 55, 61, 63, 65, 80, 81, 82)\n",
      "- SEVERE (71, 73, 75, 95, 96, 99)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 5A: OPEN-METEO API SETUP & TESTING\n",
    "\n",
    "Open-Meteo provides FREE historical weather data without requiring an API key!\n",
    "\n",
    "Advantages over OpenWeatherMap:\n",
    "- FREE forever (no credit card needed)\n",
    "- No API key required\n",
    "- 10,000 calls/day free tier\n",
    "- Historical data from 1940-present\n",
    "- Hourly weather data for exact crash times\n",
    "\n",
    "API: Open-Meteo Historical Weather API\n",
    "Endpoint: https://archive-api.open-meteo.com/v1/archive\n",
    "Documentation: https://open-meteo.com/en/docs/historical-weather-api\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"OPEN-METEO API SETUP & CONNECTION TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# API CONFIGURATION (NO API KEY NEEDED!)\n",
    "# ============================================================================\n",
    "BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "print(\"\\n Using Open-Meteo Historical Weather API\")\n",
    "print(f\" API Endpoint: {BASE_URL}\")\n",
    "print(\" No API key required!\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEST WITH 5 SAMPLE CRASHES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING API WITH 5 SAMPLE CRASHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select 5 random crashes for testing\n",
    "sample_crashes = df.sample(5, random_state=42)\n",
    "\n",
    "print(f\"\\nSelected {len(sample_crashes)} crashes for testing:\")\n",
    "print(sample_crashes[['crash_id', 'latitude', 'longitude', 'crash_datetime']].to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEST API CONNECTION WITH FIRST CRASH\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ATTEMPTING API CONNECTION...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_crash = sample_crashes.iloc[0]\n",
    "test_lat = test_crash['latitude']\n",
    "test_lon = test_crash['longitude']\n",
    "test_datetime = test_crash['crash_datetime']\n",
    "\n",
    "# Open-Meteo requires date in YYYY-MM-DD format\n",
    "test_date = test_datetime.strftime('%Y-%m-%d')\n",
    "test_hour = test_datetime.hour\n",
    "\n",
    "# Build API request parameters\n",
    "params = {\n",
    "    'latitude': test_lat,\n",
    "    'longitude': test_lon,\n",
    "    'start_date': test_date,\n",
    "    'end_date': test_date,\n",
    "    'hourly': 'temperature_2m,precipitation,wind_speed_10m,relative_humidity_2m,surface_pressure,weather_code',\n",
    "    'timezone': 'Africa/Nairobi'\n",
    "}\n",
    "\n",
    "print(f\"\\nTest Request Parameters:\")\n",
    "print(f\"  Latitude: {test_lat:.6f}\")\n",
    "print(f\"  Longitude: {test_lon:.6f}\")\n",
    "print(f\"  Date: {test_date}\")\n",
    "print(f\"  Hour: {test_hour}\")\n",
    "print(f\"  Crash Time: {test_datetime}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(BASE_URL, params=params, timeout=10)\n",
    "    \n",
    "    print(f\"\\n API Response Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"\\n SUCCESS! API connection working!\")\n",
    "        \n",
    "        # Extract weather data for the specific hour\n",
    "        if 'hourly' in data and 'time' in data['hourly']:\n",
    "            hourly_data = data['hourly']\n",
    "            \n",
    "            # Find the index for our specific hour\n",
    "            times = hourly_data['time']\n",
    "            target_time = f\"{test_date}T{test_hour:02d}:00\"\n",
    "            \n",
    "            if target_time in times:\n",
    "                idx = times.index(target_time)\n",
    "                \n",
    "                print(f\"\\nWeather data at crash time ({target_time}):\")\n",
    "                print(f\"  Temperature: {hourly_data['temperature_2m'][idx]:.1f} °C\")\n",
    "                print(f\"  Precipitation: {hourly_data['precipitation'][idx]:.1f} mm\")\n",
    "                print(f\"  Wind Speed: {hourly_data['wind_speed_10m'][idx]:.1f} km/h\")\n",
    "                print(f\"  Humidity: {hourly_data['relative_humidity_2m'][idx]:.0f} %\")\n",
    "                print(f\"  Pressure: {hourly_data['surface_pressure'][idx]:.1f} hPa\")\n",
    "                print(f\"  Weather Code: {hourly_data['weather_code'][idx]}\")\n",
    "                \n",
    "                print(\"\\n Open-Meteo API is working perfectly!\")\n",
    "                print(\" Ready to fetch weather data for all 31,064 crashes!\")\n",
    "            else:\n",
    "                print(f\"\\n Warning: Target time {target_time} not found in response\")\n",
    "                print(f\"  Available times: {times[:3]}...\")\n",
    "        else:\n",
    "            print(\"\\n Warning: Unexpected data structure in response\")\n",
    "            print(f\"  Keys in response: {list(data.keys())}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n ERROR: Unexpected response code {response.status_code}\")\n",
    "        print(f\"  Response: {response.text[:500]}\")\n",
    "        \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"\\n✗ ERROR: Request timed out\")\n",
    "    print(\"  Check your internet connection\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ ERROR: {str(e)}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"API TEST COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEATHER CODE REFERENCE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Open-Meteo Weather Codes (WMO standard):\n",
    "  0 = Clear sky\n",
    "  1, 2, 3 = Mainly clear, partly cloudy, overcast\n",
    "  45, 48 = Fog\n",
    "  51, 53, 55 = Drizzle (light, moderate, dense)\n",
    "  61, 63, 65 = Rain (slight, moderate, heavy)\n",
    "  71, 73, 75 = Snow fall (slight, moderate, heavy)\n",
    "  80, 81, 82 = Rain showers (slight, moderate, violent)\n",
    "  95, 96, 99 = Thunderstorm\n",
    "  \n",
    "Categorize these into:\n",
    "- CLEAR (0-3)\n",
    "- CLOUDY (45, 48)\n",
    "- RAIN (51, 53, 55, 61, 63, 65, 80, 81, 82)\n",
    "- SEVERE (71, 73, 75, 95, 96, 99)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debf3ace-8910-4192-b516-dc7af610b472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WEATHER DATA BATCH FETCHING SETUP\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ANALYZING CRASH DATE DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Total crashes: 31,064\n",
      "Unique dates: 3,889\n",
      "Average crashes per date: 8.0\n",
      "\n",
      " Most crashes on single date: 41\n",
      "Least crashes on single date: 1\n",
      "Median crashes per date: 7\n",
      "\n",
      " Optimization potential:\n",
      "  Instead of 31,064 API calls (1 per crash)\n",
      "  We only need 3,889 API calls (1 per date)\n",
      "  Reduction: 87.5%\n",
      "  Time needed: ~0.4 days at 10,000 calls/day\n",
      "\n",
      "======================================================================\n",
      "CREATING WEATHER FETCH FUNCTION\n",
      "======================================================================\n",
      "\n",
      " Weather fetching functions created\n",
      " Supports date-based batching\n",
      " Automatic retry on failures\n",
      " Rate limit handling\n",
      "\n",
      "======================================================================\n",
      "READY FOR BATCH WEATHER FETCHING\n",
      "======================================================================\n",
      "\n",
      "Next step: Fetch weather for 3,889 unique dates\n",
      "Estimated API calls: 3,889\n",
      "Free tier limit: 10,000 calls/day\n",
      "Estimated time: ~1 day(s)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 5B: BATCH WEATHER DATA FETCHING FUNCTION\n",
    "\n",
    "Strategy:\n",
    "- Open-Meteo allows 10,000 calls/day (free tier)\n",
    "- We have 31,064 crashes\n",
    "- This will take ~4 days if we fetch 1 crash per call\n",
    "\n",
    "Optimization:\n",
    "- Group crashes by date (many crashes happen on same day)\n",
    "- Fetch all hours for a date in ONE call\n",
    "- This reduces total API calls significantly\n",
    "\n",
    "Process:\n",
    "1. Group crashes by date\n",
    "2. Fetch weather for each unique date\n",
    "3. Extract data for specific hours\n",
    "4. Merge back to main dataframe\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"WEATHER DATA BATCH FETCHING SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYZE DATE DISTRIBUTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYZING CRASH DATE DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract just the date (without time)\n",
    "df['crash_date_only'] = df['crash_datetime'].dt.date\n",
    "\n",
    "# Count unique dates\n",
    "unique_dates = df['crash_date_only'].nunique()\n",
    "total_crashes = len(df)\n",
    "\n",
    "print(f\"\\nTotal crashes: {total_crashes:,}\")\n",
    "print(f\"Unique dates: {unique_dates:,}\")\n",
    "print(f\"Average crashes per date: {total_crashes/unique_dates:.1f}\")\n",
    "\n",
    "# Show date distribution\n",
    "crashes_per_date = df['crash_date_only'].value_counts()\n",
    "print(f\"\\n Most crashes on single date: {crashes_per_date.max()}\")\n",
    "print(f\"Least crashes on single date: {crashes_per_date.min()}\")\n",
    "print(f\"Median crashes per date: {crashes_per_date.median():.0f}\")\n",
    "\n",
    "print(f\"\\n Optimization potential:\")\n",
    "print(f\"  Instead of {total_crashes:,} API calls (1 per crash)\")\n",
    "print(f\"  We only need {unique_dates:,} API calls (1 per date)\")\n",
    "print(f\"  Reduction: {(1 - unique_dates/total_crashes)*100:.1f}%\")\n",
    "print(f\"  Time needed: ~{unique_dates/10000:.1f} days at 10,000 calls/day\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE WEATHER FETCHING FUNCTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING WEATHER FETCH FUNCTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def fetch_weather_for_date(date, latitude, longitude, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch weather data for a specific date and location.\n",
    "    \n",
    "    Returns hourly data for all 24 hours of that date.\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    \n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'start_date': date_str,\n",
    "        'end_date': date_str,\n",
    "        'hourly': 'temperature_2m,precipitation,wind_speed_10m,relative_humidity_2m,surface_pressure,weather_code',\n",
    "        'timezone': 'Africa/Nairobi'\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(BASE_URL, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:\n",
    "                # Rate limit hit, wait and retry\n",
    "                wait_time = (attempt + 1) * 5\n",
    "                print(f\"  Rate limit hit, waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"  Error {response.status_code} for {date_str}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Exception on attempt {attempt+1}: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_weather_for_hour(weather_data, target_hour):\n",
    "    \"\"\"\n",
    "    Extract weather values for a specific hour from the API response.\n",
    "    \"\"\"\n",
    "    if not weather_data or 'hourly' not in weather_data:\n",
    "        return None\n",
    "    \n",
    "    hourly = weather_data['hourly']\n",
    "    \n",
    "    try:\n",
    "        # Find the index for our target hour\n",
    "        times = hourly['time']\n",
    "        # Hour is already in 0-23 format\n",
    "        target_idx = target_hour  # Times are returned in order 00:00 to 23:00\n",
    "        \n",
    "        if target_idx < len(times):\n",
    "            return {\n",
    "                'temperature_c': hourly['temperature_2m'][target_idx],\n",
    "                'precipitation_mm': hourly['precipitation'][target_idx],\n",
    "                'wind_speed_kmh': hourly['wind_speed_10m'][target_idx],\n",
    "                'humidity_percent': hourly['relative_humidity_2m'][target_idx],\n",
    "                'pressure_hpa': hourly['surface_pressure'][target_idx],\n",
    "                'weather_code': hourly['weather_code'][target_idx]\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"  Error extracting hour {target_hour}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def categorize_weather_code(code):\n",
    "    \"\"\"\n",
    "    Categorize WMO weather code into simplified categories.\n",
    "    \"\"\"\n",
    "    if code <= 3:\n",
    "        return 'CLEAR'\n",
    "    elif code in [45, 48]:\n",
    "        return 'FOG'\n",
    "    elif code in [51, 53, 55, 61, 63, 65, 80, 81, 82]:\n",
    "        return 'RAIN'\n",
    "    elif code in [71, 73, 75, 95, 96, 99]:\n",
    "        return 'SEVERE'\n",
    "    else:\n",
    "        return 'CLOUDY'\n",
    "\n",
    "\n",
    "print(\"\\n Weather fetching functions created\")\n",
    "print(\" Supports date-based batching\")\n",
    "print(\" Automatic retry on failures\")\n",
    "print(\" Rate limit handling\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR BATCH WEATHER FETCHING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nNext step: Fetch weather for {unique_dates:,} unique dates\")\n",
    "print(f\"Estimated API calls: {unique_dates:,}\")\n",
    "print(f\"Free tier limit: 10,000 calls/day\")\n",
    "print(f\"Estimated time: ~{np.ceil(unique_dates/10000):.0f} day(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de9a1be-b2b3-4902-8d43-f9bf6b5b3883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHECKING FOR EXISTING WEATHER DATA\n",
      "======================================================================\n",
      "\n",
      "Looking for: D:\\Nairobi-Accident-Severity\\data\\raw\\weather_data_raw.pkl\n",
      "\n",
      "======================================================================\n",
      " EXISTING WEATHER DATA FOUND!\n",
      "======================================================================\n",
      "\n",
      " Loading from: D:\\Nairobi-Accident-Severity\\data\\raw\\weather_data_raw.pkl\n",
      " Loaded 3,889 weather records\n",
      " File size: 6.9 MB\n",
      "\n",
      " SKIPPED 76-MINUTE API FETCH!\n",
      " Dates covered: 3,889\n",
      " Ready to proceed to weather data processing\n",
      "\n",
      "======================================================================\n",
      "USING LOADED WEATHER DATA\n",
      "======================================================================\n",
      " Skipping fetch - data already available\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 5C: EXECUTE BATCH WEATHER DATA FETCHING (OPTIMIZED)\n",
    "\n",
    "AUTO-LOAD: If weather data exists in data/raw/, load it instead of fetching.\n",
    "Otherwise, fetch from Open-Meteo API (takes ~76 minutes).\n",
    "\n",
    "Fast version - no artificial delays!\n",
    "Open-Meteo has no strict rate limits for reasonable use.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-LOAD: Check if weather data already exists on disk\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"CHECKING FOR EXISTING WEATHER DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get paths (looking in data/raw/ folder)\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.dirname(notebook_dir)  # Go up to project root\n",
    "data_raw_dir = os.path.join(project_root, 'data', 'raw')\n",
    "weather_file = os.path.join(data_raw_dir, 'weather_data_raw.pkl')\n",
    "\n",
    "print(f\"\\nLooking for: {weather_file}\")\n",
    "\n",
    "if os.path.exists(weather_file):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" EXISTING WEATHER DATA FOUND!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n Loading from: {weather_file}\")\n",
    "    \n",
    "    with open(weather_file, 'rb') as f:\n",
    "        weather_results = pickle.load(f)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(weather_file) / (1024*1024)\n",
    "    \n",
    "    print(f\" Loaded {len(weather_results):,} weather records\")\n",
    "    print(f\" File size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"\\n SKIPPED 76-MINUTE API FETCH!\")\n",
    "    \n",
    "    # Create date_groups for compatibility with rest of code\n",
    "    date_groups = df.groupby('crash_date_only').agg({\n",
    "        'latitude': 'first',\n",
    "        'longitude': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(f\" Dates covered: {len(date_groups):,}\")\n",
    "    print(f\" Ready to proceed to weather data processing\")\n",
    "    \n",
    "    WEATHER_DATA_LOADED = True\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" NO EXISTING WEATHER DATA FOUND\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n Will proceed with API fetch (~76 minutes)...\")\n",
    "    WEATHER_DATA_LOADED = False\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ONLY RUN FETCH IF DATA WASN'T LOADED\n",
    "# ============================================================================\n",
    "if not WEATHER_DATA_LOADED:\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"BATCH WEATHER DATA FETCHING - FAST EXECUTION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ========================================================================\n",
    "    # PREPARE DATE-LOCATION GROUPS\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPARING FETCH QUEUE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Group crashes by date\n",
    "    date_groups = df.groupby('crash_date_only').agg({\n",
    "        'latitude': 'first',\n",
    "        'longitude': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    print(f\"\\n Total dates to fetch: {len(date_groups):,}\")\n",
    "    print(f\"Date range: {date_groups['crash_date_only'].min()} to {date_groups['crash_date_only'].max()}\")\n",
    "\n",
    "    # Create progress tracking\n",
    "    weather_results = []\n",
    "    failed_dates = []\n",
    "\n",
    "\n",
    "    # ========================================================================\n",
    "    # EXECUTE FAST BATCH FETCHING (NO DELAYS)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FETCHING WEATHER DATA (FAST MODE)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\n Starting fast batch fetch...\")\n",
    "    print(f\" Estimated time: 5-10 minutes\")\n",
    "    print(f\"\\n NO artificial delays - requesting at natural speed\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "\n",
    "    for idx in tqdm(range(len(date_groups)), desc=\"Fetching weather\"):\n",
    "        row = date_groups.iloc[idx]\n",
    "        date = row['crash_date_only']\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "        \n",
    "        # Fetch weather for this date (using function from previous cell)\n",
    "        weather_data = fetch_weather_for_date(date, lat, lon, max_retries=2)\n",
    "        \n",
    "        if weather_data:\n",
    "            weather_results.append({\n",
    "                'crash_date_only': date,\n",
    "                'weather_data': weather_data\n",
    "            })\n",
    "            success_count += 1\n",
    "        else:\n",
    "            failed_dates.append({\n",
    "                'crash_date_only': date,\n",
    "                'latitude': lat,\n",
    "                'longitude': lon\n",
    "            })\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Show progress every 500 dates\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (idx + 1) / elapsed\n",
    "            remaining = (len(date_groups) - idx - 1) / rate\n",
    "            print(f\"\\n  Progress: {idx+1}/{len(date_groups)} | Rate: {rate:.1f} dates/sec | ETA: {remaining/60:.1f} min\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BATCH FETCHING COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\n Total time: {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\" Successfully fetched: {success_count:,} dates ({success_count/len(date_groups)*100:.1f}%)\")\n",
    "    print(f\" Failed: {fail_count:,} dates ({fail_count/len(date_groups)*100:.1f}%)\")\n",
    "    print(f\" Average speed: {len(date_groups)/elapsed_time:.1f} dates/second\")\n",
    "\n",
    "    if fail_count > 0:\n",
    "        print(f\"\\n {fail_count} dates failed - will use fallback values\")\n",
    "\n",
    "    \n",
    "    # Save results immediately after fetching\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAVING FETCHED DATA TO DISK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open(weather_file, 'wb') as f:\n",
    "        pickle.dump(weather_results, f)\n",
    "    \n",
    "    print(f\" Saved to: {weather_file}\")\n",
    "    print(f\" Weather data ready: {len(weather_results):,} dates\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"USING LOADED WEATHER DATA\")\n",
    "    print(\"=\"*70)\n",
    "    print(\" Skipping fetch - data already available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3fd07cc-5b9d-452f-8aa9-66a8d0398e7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXTRACTING WEATHER DATA FOR EACH CRASH\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING WEATHER LOOKUP DICTIONARY\n",
      "======================================================================\n",
      "\n",
      " Created lookup dictionary for 3,889 dates\n",
      "\n",
      "======================================================================\n",
      "EXTRACTING HOURLY WEATHER FOR ALL CRASHES\n",
      "======================================================================\n",
      "\n",
      "Processing 31,064 crashes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37803f90ede145e78e8d9d8b5ffbfde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting weather:   0%|          | 0/31064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weather extracted for 31,064 crashes\n",
      " Complete data: 31,064 (100.0%)\n",
      " Missing data: 0 (0.0%)\n",
      "\n",
      "======================================================================\n",
      "ADDING RAW WEATHER FEATURES\n",
      "======================================================================\n",
      "\n",
      " Added 6 raw weather features:\n",
      "  1. actual_temperature_c\n",
      "  2. actual_precipitation_mm\n",
      "  3. actual_wind_speed_kmh\n",
      "  4. actual_humidity_percent\n",
      "  5. actual_pressure_hpa\n",
      "  6. weather_code\n",
      "\n",
      "======================================================================\n",
      "HANDLING MISSING VALUES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING DERIVED WEATHER FEATURES\n",
      "======================================================================\n",
      "\n",
      " Added 2 derived features:\n",
      "  7. weather_condition (categorical: CLEAR/CLOUDY/FOG/RAIN/SEVERE)\n",
      "  8. is_adverse_weather (boolean flag)\n",
      "\n",
      "======================================================================\n",
      "WEATHER FEATURES SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "1. Temperature (°C):\n",
      "   Range: 7.0 to 31.2\n",
      "   Mean: 19.3\n",
      "   Median: 19.0\n",
      "\n",
      "2. Precipitation (mm):\n",
      "   Range: 0.0 to 19.0\n",
      "   Mean: 0.12\n",
      "   Crashes with rain (>0mm): 6,983 (22.5%)\n",
      "\n",
      "3. Wind Speed (km/h):\n",
      "   Range: 0.0 to 27.3\n",
      "   Mean: 8.9\n",
      "\n",
      "4. Weather Conditions:\n",
      "weather_condition\n",
      "CLEAR    24081\n",
      "RAIN      6983\n",
      "\n",
      "5. Adverse Weather:\n",
      "   Adverse conditions: 6,983 (22.5%)\n",
      "   Normal conditions: 24,081 (77.5%)\n",
      "\n",
      "======================================================================\n",
      "SEVERITY ANALYSIS BY WEATHER CONDITIONS\n",
      "======================================================================\n",
      "\n",
      " HIGH severity rate by weather condition:\n",
      "  CLEAR     : 12.49%\n",
      "  RAIN      : 12.56%\n",
      "\n",
      "HIGH severity rate:\n",
      "  Adverse weather: 12.56%\n",
      "  Normal weather: 12.49%\n",
      "  Risk multiplier: 1.01x\n",
      "\n",
      "======================================================================\n",
      "WEATHER FEATURE EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "\n",
      " Total features in dataframe: 42\n",
      " Added 8 new weather features\n",
      " Ready to remove old weather proxies\n",
      "\n",
      "======================================================================\n",
      "CREATING DAYLIGHT STATUS FEATURE\n",
      "======================================================================\n",
      "\n",
      " daylight_status created:\n",
      "daylight_status\n",
      "DAYLIGHT    20756\n",
      "DARKNESS    10308\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   HIGH severity rate by visibility:\n",
      "   - DAYLIGHT  : 12.21%\n",
      "   - DARKNESS  : 13.10%\n",
      "\n",
      " Daylight feature added\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 5D: EXTRACT WEATHER DATA FOR EACH CRASH HOUR\n",
    "\n",
    "Weather data for 3,889 dates (24 hours each),\n",
    "Need to extract the specific hour for each of the 31,064 crashes.\n",
    "\n",
    "Process:\n",
    "1. For each crash, find its date in weather_results\n",
    "2. Extract weather for that specific hour\n",
    "3. Create 8 new weather features\n",
    "4. Add to main dataframe\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXTRACTING WEATHER DATA FOR EACH CRASH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE WEATHER LOOKUP DICTIONARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING WEATHER LOOKUP DICTIONARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert weather_results list to dictionary for fast lookup\n",
    "weather_dict = {}\n",
    "for item in weather_results:\n",
    "    date = item['crash_date_only']\n",
    "    weather_dict[date] = item['weather_data']\n",
    "\n",
    "print(f\"\\n Created lookup dictionary for {len(weather_dict):,} dates\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT WEATHER FOR EACH CRASH HOUR\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTRACTING HOURLY WEATHER FOR ALL CRASHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize lists for new weather features\n",
    "temperatures = []\n",
    "precipitations = []\n",
    "wind_speeds = []\n",
    "humidities = []\n",
    "pressures = []\n",
    "weather_codes = []\n",
    "\n",
    "missing_count = 0\n",
    "\n",
    "print(f\"\\nProcessing {len(df):,} crashes...\")\n",
    "\n",
    "for idx in tqdm(range(len(df)), desc=\"Extracting weather\"):\n",
    "    crash_date = df.iloc[idx]['crash_date_only']\n",
    "    crash_hour = df.iloc[idx]['hour']\n",
    "    \n",
    "    # Look up weather for this date\n",
    "    if crash_date in weather_dict:\n",
    "        weather_data = weather_dict[crash_date]\n",
    "        \n",
    "        # Extract data for specific hour\n",
    "        weather_hour = extract_weather_for_hour(weather_data, crash_hour)\n",
    "        \n",
    "        if weather_hour:\n",
    "            temperatures.append(weather_hour['temperature_c'])\n",
    "            precipitations.append(weather_hour['precipitation_mm'])\n",
    "            wind_speeds.append(weather_hour['wind_speed_kmh'])\n",
    "            humidities.append(weather_hour['humidity_percent'])\n",
    "            pressures.append(weather_hour['pressure_hpa'])\n",
    "            weather_codes.append(weather_hour['weather_code'])\n",
    "        else:\n",
    "            # Missing hour data - use NaN\n",
    "            temperatures.append(np.nan)\n",
    "            precipitations.append(np.nan)\n",
    "            wind_speeds.append(np.nan)\n",
    "            humidities.append(np.nan)\n",
    "            pressures.append(np.nan)\n",
    "            weather_codes.append(np.nan)\n",
    "            missing_count += 1\n",
    "    else:\n",
    "        # Missing date - use NaN\n",
    "        temperatures.append(np.nan)\n",
    "        precipitations.append(np.nan)\n",
    "        wind_speeds.append(np.nan)\n",
    "        humidities.append(np.nan)\n",
    "        pressures.append(np.nan)\n",
    "        weather_codes.append(np.nan)\n",
    "        missing_count += 1\n",
    "\n",
    "print(f\"\\n Weather extracted for {len(df):,} crashes\")\n",
    "print(f\" Complete data: {len(df) - missing_count:,} ({(len(df)-missing_count)/len(df)*100:.1f}%)\")\n",
    "print(f\" Missing data: {missing_count:,} ({missing_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ADD RAW WEATHER FEATURES TO DATAFRAME\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADDING RAW WEATHER FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df['actual_temperature_c'] = temperatures\n",
    "df['actual_precipitation_mm'] = precipitations\n",
    "df['actual_wind_speed_kmh'] = wind_speeds\n",
    "df['actual_humidity_percent'] = humidities\n",
    "df['actual_pressure_hpa'] = pressures\n",
    "df['weather_code'] = weather_codes\n",
    "\n",
    "print(\"\\n Added 6 raw weather features:\")\n",
    "print(\"  1. actual_temperature_c\")\n",
    "print(\"  2. actual_precipitation_mm\")\n",
    "print(\"  3. actual_wind_speed_kmh\")\n",
    "print(\"  4. actual_humidity_percent\")\n",
    "print(\"  5. actual_pressure_hpa\")\n",
    "print(\"  6. weather_code\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HANDLE MISSING VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"\\nFilling {missing_count:,} missing values with median/mode...\")\n",
    "    \n",
    "    # Fill with median for numeric features\n",
    "    df['actual_temperature_c'].fillna(df['actual_temperature_c'].median(), inplace=True)\n",
    "    df['actual_precipitation_mm'].fillna(0, inplace=True)  # Assume no rain if missing\n",
    "    df['actual_wind_speed_kmh'].fillna(df['actual_wind_speed_kmh'].median(), inplace=True)\n",
    "    df['actual_humidity_percent'].fillna(df['actual_humidity_percent'].median(), inplace=True)\n",
    "    df['actual_pressure_hpa'].fillna(df['actual_pressure_hpa'].median(), inplace=True)\n",
    "    df['weather_code'].fillna(df['weather_code'].mode()[0], inplace=True)\n",
    "    \n",
    "    print(\" Missing values filled with median/mode\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DERIVED WEATHER FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING DERIVED WEATHER FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Categorize weather code into simplified categories\n",
    "df['weather_condition'] = df['weather_code'].apply(categorize_weather_code)\n",
    "\n",
    "# Create adverse weather flag (rain, fog, or severe)\n",
    "df['is_adverse_weather'] = df['weather_condition'].isin(['RAIN', 'FOG', 'SEVERE']).astype(int)\n",
    "\n",
    "print(\"\\n Added 2 derived features:\")\n",
    "print(\"  7. weather_condition (categorical: CLEAR/CLOUDY/FOG/RAIN/SEVERE)\")\n",
    "print(\"  8. is_adverse_weather (boolean flag)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEATHER FEATURES SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Temperature (°C):\")\n",
    "print(f\"   Range: {df['actual_temperature_c'].min():.1f} to {df['actual_temperature_c'].max():.1f}\")\n",
    "print(f\"   Mean: {df['actual_temperature_c'].mean():.1f}\")\n",
    "print(f\"   Median: {df['actual_temperature_c'].median():.1f}\")\n",
    "\n",
    "print(\"\\n2. Precipitation (mm):\")\n",
    "print(f\"   Range: {df['actual_precipitation_mm'].min():.1f} to {df['actual_precipitation_mm'].max():.1f}\")\n",
    "print(f\"   Mean: {df['actual_precipitation_mm'].mean():.2f}\")\n",
    "print(f\"   Crashes with rain (>0mm): {(df['actual_precipitation_mm'] > 0).sum():,} ({(df['actual_precipitation_mm'] > 0).mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n3. Wind Speed (km/h):\")\n",
    "print(f\"   Range: {df['actual_wind_speed_kmh'].min():.1f} to {df['actual_wind_speed_kmh'].max():.1f}\")\n",
    "print(f\"   Mean: {df['actual_wind_speed_kmh'].mean():.1f}\")\n",
    "\n",
    "print(\"\\n4. Weather Conditions:\")\n",
    "print(df['weather_condition'].value_counts().to_string())\n",
    "\n",
    "print(\"\\n5. Adverse Weather:\")\n",
    "print(f\"   Adverse conditions: {df['is_adverse_weather'].sum():,} ({df['is_adverse_weather'].mean()*100:.1f}%)\")\n",
    "print(f\"   Normal conditions: {(1-df['is_adverse_weather']).sum():,} ({(1-df['is_adverse_weather']).mean()*100:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYZE SEVERITY BY WEATHER\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEVERITY ANALYSIS BY WEATHER CONDITIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "weather_severity = df.groupby('weather_condition')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ")\n",
    "\n",
    "print(\"\\n HIGH severity rate by weather condition:\")\n",
    "for condition in ['CLEAR', 'CLOUDY', 'FOG', 'RAIN', 'SEVERE']:\n",
    "    if condition in weather_severity.index:\n",
    "        print(f\"  {condition:10s}: {weather_severity[condition]:.2f}%\")\n",
    "\n",
    "adverse_high = (df[df['is_adverse_weather'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "normal_high = (df[df['is_adverse_weather'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(f\"\\nHIGH severity rate:\")\n",
    "print(f\"  Adverse weather: {adverse_high:.2f}%\")\n",
    "print(f\"  Normal weather: {normal_high:.2f}%\")\n",
    "if normal_high > 0:\n",
    "    print(f\"  Risk multiplier: {adverse_high/normal_high:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEATHER FEATURE EXTRACTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Total features in dataframe: {len(df.columns)}\")\n",
    "print(f\" Added 8 new weather features\")\n",
    "print(f\" Ready to remove old weather proxies\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DAYLIGHT STATUS FEATURE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING DAYLIGHT STATUS FEATURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def get_daylight_status(hour):\n",
    "    \"\"\"\n",
    "    Determine daylight vs darkness based on hour\n",
    "    Nairobi (near equator): sunrise ~6:15 AM, sunset ~6:45 PM (very consistent)\n",
    "    \"\"\"\n",
    "    if 7 <= hour <= 18:\n",
    "        return 'DAYLIGHT'\n",
    "    else:\n",
    "        return 'DARKNESS'\n",
    "\n",
    "df['daylight_status'] = df['hour'].apply(get_daylight_status)\n",
    "\n",
    "print(\"\\n daylight_status created:\")\n",
    "print(df['daylight_status'].value_counts())\n",
    "\n",
    "# Analyze severity by daylight\n",
    "daylight_severity = df.groupby('daylight_status')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ")\n",
    "print(\"\\n   HIGH severity rate by visibility:\")\n",
    "for status in ['DAYLIGHT', 'DARKNESS']:\n",
    "    if status in daylight_severity.index:\n",
    "        print(f\"   - {status:10s}: {daylight_severity[status]:.2f}%\")\n",
    "\n",
    "print(\"\\n Daylight feature added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59b2df58-d977-4f86-9ab4-3daf2a428f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ROAD INFRASTRUCTURE FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ROAD TYPE CLASSIFICATION (Based on Crash Patterns)\n",
      "======================================================================\n",
      "\n",
      "1. road_type_proxy (derived from crash volume):\n",
      "road_type_proxy\n",
      "MAIN_ROAD          4752\n",
      "MAJOR_HIGHWAY     15376\n",
      "RESIDENTIAL        2601\n",
      "SECONDARY_ROAD     8335\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   HIGH severity rate by road type:\n",
      "   - RESIDENTIAL      :  13.30%\n",
      "   - SECONDARY_ROAD   :  12.11%\n",
      "   - MAIN_ROAD        :  12.44%\n",
      "   - MAJOR_HIGHWAY    :  12.60%\n",
      "\n",
      "======================================================================\n",
      "INTERSECTION DETECTION (Based on Spatial Clustering)\n",
      "======================================================================\n",
      "\n",
      "2. likely_intersection (crash density >50):\n",
      "   Intersection locations: 24,979 crashes (80.4%)\n",
      "   Intersection HIGH rate: 12.33%\n",
      "   Non-intersection HIGH rate: 13.23%\n",
      "\n",
      "======================================================================\n",
      "SPEED-RELATED RISK INDICATORS\n",
      "======================================================================\n",
      "\n",
      "3. high_speed_road (major highways + main roads):\n",
      "   High-speed road crashes: 20,128 (64.8%)\n",
      "   High-speed HIGH rate: 12.56%\n",
      "   Low-speed HIGH rate: 12.39%\n",
      "\n",
      "======================================================================\n",
      "GEOGRAPHIC RISK ZONES\n",
      "======================================================================\n",
      "\n",
      "4. geographic_zone (distance from Nairobi CBD):\n",
      "geographic_zone\n",
      "CBD_CORE          9905\n",
      "INNER_SUBURBS    14771\n",
      "OUTER_SUBURBS     4155\n",
      "PERIPHERAL        2233\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   HIGH severity rate by zone:\n",
      "   - CBD_CORE         :  12.67%\n",
      "   - INNER_SUBURBS    :  12.13%\n",
      "   - OUTER_SUBURBS    :  12.06%\n",
      "   - PERIPHERAL       :  15.09%\n",
      "\n",
      "5. distance_from_cbd_km (continuous):\n",
      "   Range: 0.02 to 225.07 km\n",
      "   Mean: 11.65 km\n",
      "   Median: 7.55 km\n",
      "\n",
      "======================================================================\n",
      "COMPOUND INFRASTRUCTURE RISK\n",
      "======================================================================\n",
      "\n",
      "6. high_risk_infrastructure (intersection × high-speed road):\n",
      "   High-risk infrastructure: 20,128 crashes\n",
      "   HIGH rate with risk: 12.56%\n",
      "   HIGH rate baseline: 12.39%\n",
      "   Risk multiplier: 1.01x\n",
      "\n",
      "======================================================================\n",
      "ROAD INFRASTRUCTURE FEATURES SUMMARY\n",
      "======================================================================\n",
      "\n",
      " Created 6 road infrastructure features:\n",
      "  1. road_type_proxy\n",
      "  2. likely_intersection\n",
      "  3. high_speed_road\n",
      "  4. geographic_zone\n",
      "  5. distance_from_cbd_km\n",
      "  6. high_risk_infrastructure\n",
      "\n",
      " Road infrastructure features engineered\n",
      " Total features now: 43\n",
      " Feature engineering complete!\n",
      "\n",
      " Ready for train/validation/test split\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 5B: Road Infrastructure Feature Engineering\n",
    "\n",
    "Road infrastructure significantly impacts accident severity:\n",
    "- Major highways → higher speeds → more severe crashes\n",
    "- Intersections → conflict points → higher crash risk\n",
    "- Residential roads → lower speeds → less severe crashes\n",
    "\n",
    "Data Source Strategy:\n",
    "Derive road characteristics from:\n",
    "1. Crash density patterns (high-volume roads vs low-volume)\n",
    "2. Location clustering (intersections have more crashes)\n",
    "3. Geographic patterns (highway corridors vs neighborhoods)\n",
    "\n",
    "In operational deployment, these would come from:\n",
    "- OpenStreetMap API (road type, intersection presence)\n",
    "- Google Maps API (speed limits, traffic patterns)\n",
    "- Pre-loaded GIS database\n",
    "\n",
    "For this analysis, I used crash patterns as proxies for road types.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ROAD INFRASTRUCTURE FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# CRASH VOLUME AS ROAD TYPE PROXY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROAD TYPE CLASSIFICATION (Based on Crash Patterns)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def classify_road_type(crashes_at_location):\n",
    "    \"\"\"\n",
    "    Classify road type based on crash volume at location\n",
    "    \n",
    "    Logic:\n",
    "    - Very high crash volume (>200) → Major highway/arterial\n",
    "    - High crash volume (100-200) → Main road\n",
    "    - Medium crash volume (20-100) → Secondary road\n",
    "    - Low crash volume (<20) → Residential/minor road\n",
    "    \"\"\"\n",
    "    if crashes_at_location >= 200:\n",
    "        return 'MAJOR_HIGHWAY'\n",
    "    elif crashes_at_location >= 100:\n",
    "        return 'MAIN_ROAD'\n",
    "    elif crashes_at_location >= 20:\n",
    "        return 'SECONDARY_ROAD'\n",
    "    else:\n",
    "        return 'RESIDENTIAL'\n",
    "\n",
    "df['road_type_proxy'] = df['crashes_at_location'].apply(classify_road_type)\n",
    "\n",
    "print(\"\\n1. road_type_proxy (derived from crash volume):\")\n",
    "print(df['road_type_proxy'].value_counts().sort_index())\n",
    "\n",
    "# Analyze severity by road type\n",
    "road_type_severity = df.groupby('road_type_proxy')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ")\n",
    "print(\"\\n   HIGH severity rate by road type:\")\n",
    "for road_type in ['RESIDENTIAL', 'SECONDARY_ROAD', 'MAIN_ROAD', 'MAJOR_HIGHWAY']:\n",
    "    if road_type in road_type_severity.index:\n",
    "        print(f\"   - {road_type:17s}: {road_type_severity[road_type]:6.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTERSECTION INDICATOR (Crash Clustering)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERSECTION DETECTION (Based on Spatial Clustering)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Locations with very high crash density likely indicate intersections\n",
    "# We already have crashes_at_location from Notebook 01\n",
    "\n",
    "# Intersection proxy: locations with >50 crashes (statistical hotspot)\n",
    "df['likely_intersection'] = (df['crashes_at_location'] > 50).astype(int)\n",
    "\n",
    "intersection_high_rate = (df[df['likely_intersection'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "non_intersection_high_rate = (df[df['likely_intersection'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(\"\\n2. likely_intersection (crash density >50):\")\n",
    "print(f\"   Intersection locations: {df['likely_intersection'].sum():,} crashes ({df['likely_intersection'].mean()*100:.1f}%)\")\n",
    "print(f\"   Intersection HIGH rate: {intersection_high_rate:.2f}%\")\n",
    "print(f\"   Non-intersection HIGH rate: {non_intersection_high_rate:.2f}%\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SPEED-RELATED RISK (High-Volume Roads)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPEED-RELATED RISK INDICATORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High-speed road indicator (major highways + main roads)\n",
    "df['high_speed_road'] = df['road_type_proxy'].isin(['MAJOR_HIGHWAY', 'MAIN_ROAD']).astype(int)\n",
    "\n",
    "high_speed_high_rate = (df[df['high_speed_road'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "low_speed_high_rate = (df[df['high_speed_road'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(\"\\n3. high_speed_road (major highways + main roads):\")\n",
    "print(f\"   High-speed road crashes: {df['high_speed_road'].sum():,} ({df['high_speed_road'].mean()*100:.1f}%)\")\n",
    "print(f\"   High-speed HIGH rate: {high_speed_high_rate:.2f}%\")\n",
    "print(f\"   Low-speed HIGH rate: {low_speed_high_rate:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# GEOGRAPHIC RISK ZONES (Central vs Peripheral)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GEOGRAPHIC RISK ZONES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate distance from Nairobi CBD (approximately -1.2864, 36.8172)\n",
    "nairobi_cbd_lat = -1.2864\n",
    "nairobi_cbd_lon = 36.8172\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate distance between two points in kilometers\n",
    "    \"\"\"\n",
    "    from math import radians, sin, cos, sqrt, atan2\n",
    "    \n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "df['distance_from_cbd_km'] = df.apply(\n",
    "    lambda row: haversine_distance(row['latitude'], row['longitude'], \n",
    "                                   nairobi_cbd_lat, nairobi_cbd_lon),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Classify into zones\n",
    "def classify_zone(distance_km):\n",
    "    \"\"\"\n",
    "    Classify location into geographic zones\n",
    "    \"\"\"\n",
    "    if distance_km < 5:\n",
    "        return 'CBD_CORE'\n",
    "    elif distance_km < 15:\n",
    "        return 'INNER_SUBURBS'\n",
    "    elif distance_km < 30:\n",
    "        return 'OUTER_SUBURBS'\n",
    "    else:\n",
    "        return 'PERIPHERAL'\n",
    "\n",
    "df['geographic_zone'] = df['distance_from_cbd_km'].apply(classify_zone)\n",
    "\n",
    "print(\"\\n4. geographic_zone (distance from Nairobi CBD):\")\n",
    "print(df['geographic_zone'].value_counts().sort_index())\n",
    "\n",
    "# Analyze severity by zone\n",
    "zone_severity = df.groupby('geographic_zone')['severity_binary'].apply(\n",
    "    lambda x: (x == 'HIGH').sum() / len(x) * 100\n",
    ")\n",
    "print(\"\\n   HIGH severity rate by zone:\")\n",
    "for zone in ['CBD_CORE', 'INNER_SUBURBS', 'OUTER_SUBURBS', 'PERIPHERAL']:\n",
    "    if zone in zone_severity.index:\n",
    "        print(f\"   - {zone:17s}: {zone_severity[zone]:6.2f}%\")\n",
    "\n",
    "print(\"\\n5. distance_from_cbd_km (continuous):\")\n",
    "print(f\"   Range: {df['distance_from_cbd_km'].min():.2f} to {df['distance_from_cbd_km'].max():.2f} km\")\n",
    "print(f\"   Mean: {df['distance_from_cbd_km'].mean():.2f} km\")\n",
    "print(f\"   Median: {df['distance_from_cbd_km'].median():.2f} km\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPOUND INFRASTRUCTURE RISK\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPOUND INFRASTRUCTURE RISK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High-risk infrastructure: intersection + high-speed road\n",
    "df['high_risk_infrastructure'] = ((df['likely_intersection'] == 1) & \n",
    "                                   (df['high_speed_road'] == 1)).astype(int)\n",
    "\n",
    "infra_risk_high_rate = (df[df['high_risk_infrastructure'] == 1]['severity_binary'] == 'HIGH').mean() * 100\n",
    "baseline_infra_rate = (df[df['high_risk_infrastructure'] == 0]['severity_binary'] == 'HIGH').mean() * 100\n",
    "\n",
    "print(\"\\n6. high_risk_infrastructure (intersection × high-speed road):\")\n",
    "print(f\"   High-risk infrastructure: {df['high_risk_infrastructure'].sum():,} crashes\")\n",
    "print(f\"   HIGH rate with risk: {infra_risk_high_rate:.2f}%\")\n",
    "print(f\"   HIGH rate baseline: {baseline_infra_rate:.2f}%\")\n",
    "if baseline_infra_rate > 0:\n",
    "    print(f\"   Risk multiplier: {infra_risk_high_rate/baseline_infra_rate:.2f}x\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY OF ROAD INFRASTRUCTURE FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROAD INFRASTRUCTURE FEATURES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "road_features = [\n",
    "    'road_type_proxy',\n",
    "    'likely_intersection',\n",
    "    'high_speed_road',\n",
    "    'geographic_zone',\n",
    "    'distance_from_cbd_km',\n",
    "    'high_risk_infrastructure'\n",
    "]\n",
    "\n",
    "print(f\"\\n Created {len(road_features)} road infrastructure features:\")\n",
    "for i, feature in enumerate(road_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(f\"\\n Road infrastructure features engineered\")\n",
    "print(f\" Total features now: {len(df.columns)}\")\n",
    "print(f\" Feature engineering complete!\")\n",
    "print(f\"\\n Ready for train/validation/test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8066baf5-7fc0-4ef8-b44c-e1a623a5118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE COUNT VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "Current total features: 43\n",
      "\n",
      "Feature categories:\n",
      "  Weather features: 9\n",
      "  Road features: 6\n",
      "  Temporal features: 14\n",
      "  Spatial features: 9\n",
      "\n",
      " Feature engineering complete\n",
      " Ready to prepare final feature set\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFICATION: Feature Count Check\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE COUNT VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nCurrent total features: {len(df.columns)}\")\n",
    "\n",
    "print(f\"\\nFeature categories:\")\n",
    "\n",
    "# Weather features\n",
    "weather_cols = [c for c in df.columns if 'weather' in c.lower() or \n",
    "                'temperature' in c.lower() or 'precipitation' in c.lower() or\n",
    "                'wind' in c.lower() or 'humidity' in c.lower() or \n",
    "                'pressure' in c.lower() or 'daylight' in c.lower()]\n",
    "print(f\"  Weather features: {len(weather_cols)}\")\n",
    "\n",
    "# Road features  \n",
    "road_cols = [c for c in df.columns if 'road' in c.lower() or \n",
    "             'intersection' in c.lower() or 'infrastructure' in c.lower() or \n",
    "             'distance' in c.lower() or 'zone' in c.lower()]\n",
    "print(f\"  Road features: {len(road_cols)}\")\n",
    "\n",
    "# Temporal features\n",
    "temporal_cols = [c for c in df.columns if 'hour' in c.lower() or \n",
    "                 'day' in c.lower() or 'month' in c.lower() or \n",
    "                 'weekend' in c.lower() or 'rush' in c.lower() or\n",
    "                 'night' in c.lower() or 'time' in c.lower()]\n",
    "print(f\"  Temporal features: {len(temporal_cols)}\")\n",
    "\n",
    "# Spatial features\n",
    "spatial_cols = [c for c in df.columns if 'lat' in c.lower() or \n",
    "                'lon' in c.lower() or 'location' in c.lower() or\n",
    "                'crash' in c.lower() and 'at_location' in c.lower()]\n",
    "print(f\"  Spatial features: {len(spatial_cols)}\")\n",
    "\n",
    "print(f\"\\n Feature engineering complete\")\n",
    "print(f\" Ready to prepare final feature set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0c5c997-af43-4571-95c5-0e131959ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPARING FINAL FEATURE SET FOR MODELING\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "FEATURE CATEGORIZATION\n",
      "======================================================================\n",
      "\n",
      " Features to EXCLUDE: 9\n",
      "  - crash_id\n",
      "  - crash_datetime\n",
      "  - crash_date\n",
      "  - crash_date_only\n",
      "  - day_name\n",
      "  - lat_bin\n",
      "  - lon_bin\n",
      "  - grid_cell\n",
      "  - severity_4class\n",
      "\n",
      " NUMERIC features: 28\n",
      "   (Showing categories, not listing all 28)\n",
      "   - Location: 2\n",
      "   - Temporal: 10\n",
      "   - Spatial: 5\n",
      "   - Weather: 7 (NEW real weather data)\n",
      "   - Road: 4\n",
      "\n",
      " CATEGORICAL features: 5\n",
      "   - location_risk_category (4 categories)\n",
      "   - daylight_status (2 categories)\n",
      "   - weather_condition (2 categories)\n",
      "   - road_type_proxy (4 categories)\n",
      "   - geographic_zone (4 categories)\n",
      "\n",
      " TARGET variable: severity_binary\n",
      "\n",
      "======================================================================\n",
      "ENCODING CATEGORICAL FEATURES\n",
      "======================================================================\n",
      "\n",
      " Before encoding: 43 columns\n",
      "After encoding:  54 columns\n",
      "\n",
      "Encoding details:\n",
      "  location_risk_category: 4 categories → 4 binary columns\n",
      "  daylight_status: 2 categories → 2 binary columns\n",
      "  weather_condition: 2 categories → 2 binary columns\n",
      "  road_type_proxy: 4 categories → 4 binary columns\n",
      "  geographic_zone: 4 categories → 4 binary columns\n",
      "\n",
      "======================================================================\n",
      "CREATING FINAL FEATURE MATRIX\n",
      "======================================================================\n",
      "\n",
      " Final feature matrix shape: (31064, 44)\n",
      "   - Samples: 31,064\n",
      "   - Features: 44\n",
      "\n",
      " Target distribution:\n",
      "   - LOW (0):  27,180 (87.50%)\n",
      "   - HIGH (1): 3,884 (12.50%)\n",
      "   - Imbalance ratio: 7.00:1\n",
      "\n",
      "======================================================================\n",
      "DATA QUALITY FINAL CHECK\n",
      "======================================================================\n",
      "\n",
      " Missing values in features (X): 0\n",
      "Missing values in target (y): 0\n",
      "\n",
      " No missing values - dataset is clean!\n",
      "\n",
      "======================================================================\n",
      "FEATURE INVENTORY\n",
      "======================================================================\n",
      "\n",
      " Total predictive features: 44\n",
      "   - Numeric features: 28\n",
      "   - Encoded categorical: 16\n",
      "\n",
      " Feature preparation complete!\n",
      " Ready for train/validation/test split\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 6: Prepare Final Feature Set for Modeling\n",
    "\n",
    "1. Select only predictive features (remove identifiers)\n",
    "2. Encode categorical variables\n",
    "3. Verify no data leakage\n",
    "4. Prepare for train/validation/test split\n",
    "\n",
    "Final feature set will include:\n",
    "- Numeric features (continuous and binary indicators)\n",
    "- Encoded categorical features (one-hot encoding)\n",
    "- Target variable (severity_binary)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING FINAL FEATURE SET FOR MODELING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY FEATURE CATEGORIES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE CATEGORIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Features to EXCLUDE (identifiers, not predictive)\n",
    "exclude_features = [\n",
    "    'crash_id',\n",
    "    'crash_datetime',\n",
    "    'crash_date',\n",
    "    'crash_date_only',     # Added from weather fetching\n",
    "    'day_name',\n",
    "    'lat_bin',\n",
    "    'lon_bin',\n",
    "    'grid_cell',\n",
    "    'severity_4class'\n",
    "]\n",
    "\n",
    "# Numeric features (already in usable format)\n",
    "numeric_features = [\n",
    "    # Location\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    \n",
    "    # Temporal original\n",
    "    'hour',\n",
    "    'day_of_week',\n",
    "    'month',\n",
    "    'year',\n",
    "    'is_weekend',\n",
    "    \n",
    "    # Temporal engineered\n",
    "    'hour_severity_rate',\n",
    "    'day_severity_rate',\n",
    "    'month_severity_rate',\n",
    "    'is_night',\n",
    "    'is_rush_hour',\n",
    "    \n",
    "    # Spatial original\n",
    "    'crashes_at_location',\n",
    "    'high_rate_at_location',\n",
    "    \n",
    "    # Spatial engineered\n",
    "    'high_risk_location',\n",
    "    'dangerous_time',\n",
    "    'high_risk_location_dangerous_time',\n",
    "    \n",
    "    # Weather (UPDATED - Real weather data)\n",
    "    'actual_temperature_c',\n",
    "    'actual_precipitation_mm',\n",
    "    'actual_wind_speed_kmh',\n",
    "    'actual_humidity_percent',\n",
    "    'actual_pressure_hpa',\n",
    "    'weather_code',\n",
    "    'is_adverse_weather',\n",
    "    \n",
    "    # Road infrastructure\n",
    "    'likely_intersection',\n",
    "    'high_speed_road',\n",
    "    'distance_from_cbd_km',\n",
    "    'high_risk_infrastructure'\n",
    "]\n",
    "\n",
    "# Categorical features (need encoding)\n",
    "categorical_features = [\n",
    "    'location_risk_category',  # 4 categories\n",
    "    'daylight_status',         # 2 categories (KEPT from old weather)\n",
    "    'weather_condition',       # NEW - 5 categories (CLEAR/CLOUDY/FOG/RAIN/SEVERE)\n",
    "    'road_type_proxy',         # 4 categories\n",
    "    'geographic_zone'          # 4 categories\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'severity_binary'\n",
    "\n",
    "print(f\"\\n Features to EXCLUDE: {len(exclude_features)}\")\n",
    "for feature in exclude_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\n NUMERIC features: {len(numeric_features)}\")\n",
    "print(f\"   (Showing categories, not listing all {len(numeric_features)})\")\n",
    "print(f\"   - Location: 2\")\n",
    "print(f\"   - Temporal: 10\")\n",
    "print(f\"   - Spatial: 5\")\n",
    "print(f\"   - Weather: 7 (NEW real weather data)\")\n",
    "print(f\"   - Road: 4\")\n",
    "\n",
    "print(f\"\\n CATEGORICAL features: {len(categorical_features)}\")\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"   - {feature} ({df[feature].nunique()} categories)\")\n",
    "    else:\n",
    "        print(f\"   - {feature} (NOT FOUND - ERROR!)\")\n",
    "\n",
    "print(f\"\\n TARGET variable: {target}\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENCODE CATEGORICAL FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=False)\n",
    "\n",
    "print(f\"\\n Before encoding: {len(df.columns)} columns\")\n",
    "print(f\"After encoding:  {len(df_encoded.columns)} columns\")\n",
    "\n",
    "print(\"\\nEncoding details:\")\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        original_categories = df[feature].nunique()\n",
    "    else:\n",
    "        original_categories = 0\n",
    "    encoded_cols = [col for col in df_encoded.columns if col.startswith(feature + '_')]\n",
    "    print(f\"  {feature}: {original_categories} categories → {len(encoded_cols)} binary columns\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE FINAL FEATURE SET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING FINAL FEATURE MATRIX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get all encoded categorical column names\n",
    "encoded_categorical_cols = []\n",
    "for feature in categorical_features:\n",
    "    encoded_categorical_cols.extend([col for col in df_encoded.columns if col.startswith(feature + '_')])\n",
    "\n",
    "# Combine numeric and encoded categorical features\n",
    "all_features = numeric_features + encoded_categorical_cols\n",
    "\n",
    "# Create final feature matrix X and target y\n",
    "X = df_encoded[all_features].copy()\n",
    "y = df_encoded[target].copy()\n",
    "\n",
    "# Convert target to binary numeric (0=LOW, 1=HIGH)\n",
    "y_numeric = (y == 'HIGH').astype(int)\n",
    "\n",
    "print(f\"\\n Final feature matrix shape: {X.shape}\")\n",
    "print(f\"   - Samples: {X.shape[0]:,}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\n Target distribution:\")\n",
    "print(f\"   - LOW (0):  {(y_numeric == 0).sum():,} ({(y_numeric == 0).mean()*100:.2f}%)\")\n",
    "print(f\"   - HIGH (1): {(y_numeric == 1).sum():,} ({(y_numeric == 1).mean()*100:.2f}%)\")\n",
    "print(f\"   - Imbalance ratio: {(y_numeric == 0).sum() / (y_numeric == 1).sum():.2f}:1\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY NO MISSING VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY FINAL CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_in_X = X.isnull().sum().sum()\n",
    "missing_in_y = y_numeric.isnull().sum()\n",
    "\n",
    "print(f\"\\n Missing values in features (X): {missing_in_X}\")\n",
    "print(f\"Missing values in target (y): {missing_in_y}\")\n",
    "\n",
    "if missing_in_X == 0 and missing_in_y == 0:\n",
    "    print(\"\\n No missing values - dataset is clean!\")\n",
    "else:\n",
    "    print(\"\\n WARNING: Missing values detected - need to handle before modeling\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE INVENTORY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE INVENTORY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n Total predictive features: {len(all_features)}\")\n",
    "print(f\"   - Numeric features: {len(numeric_features)}\")\n",
    "print(f\"   - Encoded categorical: {len(encoded_categorical_cols)}\")\n",
    "\n",
    "print(f\"\\n Feature preparation complete!\")\n",
    "print(f\" Ready for train/validation/test split\")\n",
    "\n",
    "# Store for next cell\n",
    "feature_names = all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "355b262c-d0d9-4148-8105-c95f084c3d03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAIN/VALIDATION/TEST SPLIT + SMOTE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 1: INITIAL SPLIT (Train 85% + Test 15%)\n",
      "======================================================================\n",
      "\n",
      "After first split:\n",
      "  - Temp set (train+val): 26,404 samples (85.0%)\n",
      "  - Test set:             4,660 samples (15.0%)\n",
      "\n",
      "Test set class distribution:\n",
      "  - LOW (0):  4,077 (87.49%)\n",
      "  - HIGH (1): 583 (12.51%)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: SPLIT TEMP INTO TRAIN (70%) + VALIDATION (15%)\n",
      "======================================================================\n",
      "\n",
      "After second split:\n",
      "  - Train set:      21,756 samples (70.0%)\n",
      "  - Validation set: 4,648 samples (15.0%)\n",
      "  - Test set:       4,660 samples (15.0%)\n",
      "\n",
      "Train set class distribution (BEFORE SMOTE):\n",
      "  - LOW (0):  19,036 (87.50%)\n",
      "  - HIGH (1): 2,720 (12.50%)\n",
      "  - Imbalance ratio: 7.00:1\n",
      "\n",
      "Validation set class distribution (natural):\n",
      "  - LOW (0):  4,067 (87.50%)\n",
      "  - HIGH (1): 581 (12.50%)\n",
      "\n",
      "======================================================================\n",
      "STEP 3: APPLY SMOTE TO TRAINING SET ONLY\n",
      "======================================================================\n",
      "\n",
      "  CRITICAL: SMOTE is applied ONLY to training data!\n",
      "   Validation and test sets keep natural class distribution.\n",
      "   This ensures realistic performance estimates.\n",
      "\n",
      "Training set AFTER SMOTE:\n",
      "  - Total samples: 38,072 (increased from 21,756)\n",
      "  - LOW (0):  19,036 (50.00%)\n",
      "  - HIGH (1): 19,036 (50.00%)\n",
      "  - Imbalance ratio: 1.00:1\n",
      "\n",
      "  SMOTE created 16,316 synthetic HIGH severity samples\n",
      "  Training set is now perfectly balanced (1:1 ratio)\n",
      "\n",
      "======================================================================\n",
      "FINAL DATASET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. TRAINING SET (SMOTE-balanced):\n",
      "   Shape: (38072, 44)\n",
      "   LOW:  19,036 samples\n",
      "   HIGH: 19,036 samples\n",
      "   Purpose: Model training with balanced classes\n",
      "\n",
      "2. VALIDATION SET (natural distribution):\n",
      "   Shape: (4648, 44)\n",
      "   LOW:  4,067 samples (87.50%)\n",
      "   HIGH: 581 samples (12.50%)\n",
      "   Purpose: Hyperparameter tuning and model selection\n",
      "\n",
      "3. TEST SET (natural distribution):\n",
      "   Shape: (4660, 44)\n",
      "   LOW:  4,077 samples (87.49%)\n",
      "   HIGH: 583 samples (12.51%)\n",
      "   Purpose: Final unbiased performance evaluation\n",
      "\n",
      "======================================================================\n",
      "KEY PRINCIPLES FOLLOWED\n",
      "======================================================================\n",
      "\n",
      " Stratified splitting maintains class proportions\n",
      " SMOTE applied ONLY to training set (prevents data leakage)\n",
      " Validation and test sets have natural class distribution\n",
      " Test set completely held out (never seen during training)\n",
      " Model will be evaluated on realistic class imbalance\n",
      "\n",
      " Data splitting and balancing complete!\n",
      " Ready to save datasets for modeling notebook\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 7: Train/Validation/Test Split + SMOTE\n",
    "\n",
    "Proper ML workflow requires:\n",
    "1. Split into Train (70%), Validation (15%), Test (15%)\n",
    "2. Apply SMOTE ONLY to training set (prevent data leakage)\n",
    "3. Keep validation and test sets with natural class distribution\n",
    "\n",
    "Why this matters:\n",
    "- Training set: Used to fit model (SMOTE applied here)\n",
    "- Validation set: Used to tune hyperparameters (natural distribution)\n",
    "- Test set: Final performance evaluation (natural distribution)\n",
    "\n",
    "SMOTE must NEVER touch validation or test sets, or we get artificially\n",
    "inflated performance estimates that won't work in real deployment.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT + SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: INITIAL SPLIT (Train vs Test)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: INITIAL SPLIT (Train 85% + Test 15%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First split: separate test set (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y_numeric,\n",
    "    test_size=0.15,\n",
    "    stratify=y_numeric,  # Maintain class proportions\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter first split:\")\n",
    "print(f\"  - Temp set (train+val): {len(X_temp):,} samples ({len(X_temp)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Test set:             {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(f\"  - LOW (0):  {(y_test == 0).sum():,} ({(y_test == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_test == 1).sum():,} ({(y_test == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: SPLIT TEMP INTO TRAIN AND VALIDATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: SPLIT TEMP INTO TRAIN (70%) + VALIDATION (15%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Second split: train (70% of total) vs validation (15% of total)\n",
    "# From temp (85%), we want: train=70/85=82.35%, val=15/85=17.65%\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.176,  # 15% of total = 17.6% of temp\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter second split:\")\n",
    "print(f\"  - Train set:      {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Validation set: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Test set:       {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTrain set class distribution (BEFORE SMOTE):\")\n",
    "print(f\"  - LOW (0):  {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  - Imbalance ratio: {(y_train == 0).sum() / (y_train == 1).sum():.2f}:1\")\n",
    "\n",
    "print(f\"\\nValidation set class distribution (natural):\")\n",
    "print(f\"  - LOW (0):  {(y_val == 0).sum():,} ({(y_val == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_val == 1).sum():,} ({(y_val == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: APPLY SMOTE TO TRAINING SET ONLY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: APPLY SMOTE TO TRAINING SET ONLY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n  CRITICAL: SMOTE is applied ONLY to training data!\")\n",
    "print(\"   Validation and test sets keep natural class distribution.\")\n",
    "print(\"   This ensures realistic performance estimates.\\n\")\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Training set AFTER SMOTE:\")\n",
    "print(f\"  - Total samples: {len(X_train_balanced):,} (increased from {len(X_train):,})\")\n",
    "print(f\"  - LOW (0):  {(y_train_balanced == 0).sum():,} ({(y_train_balanced == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  - HIGH (1): {(y_train_balanced == 1).sum():,} ({(y_train_balanced == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  - Imbalance ratio: {(y_train_balanced == 0).sum() / (y_train_balanced == 1).sum():.2f}:1\")\n",
    "\n",
    "synthetic_samples_added = len(X_train_balanced) - len(X_train)\n",
    "print(f\"\\n  SMOTE created {synthetic_samples_added:,} synthetic HIGH severity samples\")\n",
    "print(f\"  Training set is now perfectly balanced (1:1 ratio)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL DATASET SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. TRAINING SET (SMOTE-balanced):\")\n",
    "print(f\"   Shape: {X_train_balanced.shape}\")\n",
    "print(f\"   LOW:  {(y_train_balanced == 0).sum():,} samples\")\n",
    "print(f\"   HIGH: {(y_train_balanced == 1).sum():,} samples\")\n",
    "print(f\"   Purpose: Model training with balanced classes\")\n",
    "\n",
    "print(\"\\n2. VALIDATION SET (natural distribution):\")\n",
    "print(f\"   Shape: {X_val.shape}\")\n",
    "print(f\"   LOW:  {(y_val == 0).sum():,} samples ({(y_val == 0).mean()*100:.2f}%)\")\n",
    "print(f\"   HIGH: {(y_val == 1).sum():,} samples ({(y_val == 1).mean()*100:.2f}%)\")\n",
    "print(f\"   Purpose: Hyperparameter tuning and model selection\")\n",
    "\n",
    "print(\"\\n3. TEST SET (natural distribution):\")\n",
    "print(f\"   Shape: {X_test.shape}\")\n",
    "print(f\"   LOW:  {(y_test == 0).sum():,} samples ({(y_test == 0).mean()*100:.2f}%)\")\n",
    "print(f\"   HIGH: {(y_test == 1).sum():,} samples ({(y_test == 1).mean()*100:.2f}%)\")\n",
    "print(f\"   Purpose: Final unbiased performance evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY PRINCIPLES FOLLOWED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n Stratified splitting maintains class proportions\")\n",
    "print(\" SMOTE applied ONLY to training set (prevents data leakage)\")\n",
    "print(\" Validation and test sets have natural class distribution\")\n",
    "print(\" Test set completely held out (never seen during training)\")\n",
    "print(\" Model will be evaluated on realistic class imbalance\")\n",
    "\n",
    "print(\"\\n Data splitting and balancing complete!\")\n",
    "print(\" Ready to save datasets for modeling notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e81d99c8-0945-4541-a776-5e4db9f6b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING PROCESSED DATASETS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SAVING DATASETS AS CSV\n",
      "======================================================================\n",
      "\n",
      " Training set:   38,072 rows × 45 columns\n",
      " Validation set: 4,648 rows × 45 columns\n",
      " Test set:       4,660 rows × 45 columns\n",
      "\n",
      "======================================================================\n",
      "SAVING NUMPY ARRAYS\n",
      "======================================================================\n",
      "\n",
      " Numpy arrays saved\n",
      "  - X_train_balanced.npy, y_train_balanced.npy\n",
      "  - X_val.npy, y_val.npy\n",
      "  - X_test.npy, y_test.npy\n",
      "\n",
      "======================================================================\n",
      "SAVING FEATURE METADATA\n",
      "======================================================================\n",
      "\n",
      " Feature metadata saved\n",
      "  - 44 feature names\n",
      "  - Dataset sizes recorded\n",
      "\n",
      "======================================================================\n",
      "NOTEBOOK 02 COMPLETE\n",
      "======================================================================\n",
      "\n",
      " Data cleaning & feature engineering complete!\n",
      " 41 dispatch-time features created\n",
      " 38,072 training samples (SMOTE-balanced)\n",
      " 4,648 validation samples\n",
      " 4,660 test samples\n",
      "\n",
      " Ready for model training in Notebook 03!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 8: Save Processed Datasets\n",
    "\n",
    "Save prepared datasets for modeling notebook:\n",
    "1. Training set (SMOTE-balanced)\n",
    "2. Validation set (natural distribution)\n",
    "3. Test set (natural distribution)\n",
    "4. Feature metadata\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING PROCESSED DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create features directory if it doesn't exist\n",
    "os.makedirs('../data/features', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE DATASETS AS CSV FILES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING DATASETS AS CSV\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create DataFrames with feature names\n",
    "train_df = pd.DataFrame(X_train_balanced, columns=feature_names)\n",
    "train_df['severity_binary'] = y_train_balanced\n",
    "\n",
    "val_df = pd.DataFrame(X_val, columns=feature_names)\n",
    "val_df['severity_binary'] = y_val\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "test_df['severity_binary'] = y_test\n",
    "\n",
    "# Save to CSV\n",
    "train_df.to_csv('../data/features/train_balanced.csv', index=False)\n",
    "val_df.to_csv('../data/features/validation.csv', index=False)\n",
    "test_df.to_csv('../data/features/test.csv', index=False)\n",
    "\n",
    "print(f\"\\n Training set:   {train_df.shape[0]:,} rows × {train_df.shape[1]} columns\")\n",
    "print(f\" Validation set: {val_df.shape[0]:,} rows × {val_df.shape[1]} columns\")\n",
    "print(f\" Test set:       {test_df.shape[0]:,} rows × {test_df.shape[1]} columns\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE NUMPY ARRAYS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING NUMPY ARRAYS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.save('../data/features/X_train_balanced.npy', X_train_balanced)\n",
    "np.save('../data/features/y_train_balanced.npy', y_train_balanced)\n",
    "np.save('../data/features/X_val.npy', X_val)\n",
    "np.save('../data/features/y_val.npy', y_val)\n",
    "np.save('../data/features/X_test.npy', X_test)\n",
    "np.save('../data/features/y_test.npy', y_test)\n",
    "\n",
    "print(\"\\n Numpy arrays saved\")\n",
    "print(\"  - X_train_balanced.npy, y_train_balanced.npy\")\n",
    "print(\"  - X_val.npy, y_val.npy\")\n",
    "print(\"  - X_test.npy, y_test.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE FEATURE METADATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING FEATURE METADATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "feature_metadata = {\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names),\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'n_train': len(X_train_balanced),\n",
    "    'n_val': len(X_val),\n",
    "    'n_test': len(X_test)\n",
    "}\n",
    "\n",
    "joblib.dump(feature_metadata, '../data/features/feature_metadata.pkl')\n",
    "\n",
    "print(f\"\\n Feature metadata saved\")\n",
    "print(f\"  - {len(feature_names)} feature names\")\n",
    "print(f\"  - Dataset sizes recorded\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK 02 COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n Data cleaning & feature engineering complete!\")\n",
    "print(f\" 41 dispatch-time features created\")\n",
    "print(f\" {len(X_train_balanced):,} training samples (SMOTE-balanced)\")\n",
    "print(f\" {len(X_val):,} validation samples\")\n",
    "print(f\" {len(X_test):,} test samples\")\n",
    "print(\"\\n Ready for model training in Notebook 03!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a65a06-013c-4299-aa77-650c27ae3b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
