{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd23c929-5b3c-4333-bd3f-ebc4dfa21fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING: TRAFFIC ACCIDENT SEVERITY PREDICTION\n",
      "================================================================================\n",
      "Start Time: 2026-01-28 14:40:54\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING: Traffic Accident Severity Prediction\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING: TRAFFIC ACCIDENT SEVERITY PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8509af-ff6a-48d8-a232-456eae876af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading dataset...\n",
      " Dataset loaded successfully!\n",
      "   Shape: (31064, 10)\n",
      "   Records: 31,064\n",
      "   Columns: 10\n"
     ]
    }
   ],
   "source": [
    "# ========== LOAD DATA ==========\n",
    "\n",
    "print(\"\\n Loading dataset...\")\n",
    "\n",
    "crashes = pd.read_csv(r'D:\\Nairobi-Accident-Severity\\data\\raw\\ma3route_crashes_2012_2023\\ma3route_crashes_algorithmcode.csv')\n",
    "\n",
    "print(f\" Dataset loaded successfully!\")\n",
    "print(f\"   Shape: {crashes.shape}\")\n",
    "print(f\"   Records: {len(crashes):,}\")\n",
    "print(f\"   Columns: {len(crashes.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7916c2fa-2c93-4eb7-9d10-417f6108bd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Parsing datetime columns...\n",
      " Datetime columns parsed!\n"
     ]
    }
   ],
   "source": [
    "# ========== PARSE DATETIME ==========\n",
    "\n",
    "print(\"\\n Parsing datetime columns...\")\n",
    "\n",
    "crashes['crash_datetime'] = pd.to_datetime(crashes['crash_datetime'])\n",
    "crashes['crash_date'] = pd.to_datetime(crashes['crash_date'])\n",
    "\n",
    "print(\" Datetime columns parsed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0fea3db-c1bb-4f98-9468-d9371e060647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating severity labels...\n",
      " Severity labels created!\n",
      "\n",
      "Severity distribution:\n",
      "severity\n",
      "MINOR       25059\n",
      "FATAL        2284\n",
      "MODERATE     2121\n",
      "SEVERE       1600\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      " DATA LOADED AND READY FOR FEATURE ENGINEERING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== CREATE SEVERITY LABELS ==========\n",
    "\n",
    "print(\"\\n Creating severity labels...\")\n",
    "\n",
    "def classify_severity(row):\n",
    "    \"\"\"\n",
    "    Classify crash severity based on keyword indicators\n",
    "    \"\"\"\n",
    "    if row['contains_fatality_words'] == 1:\n",
    "        return 'FATAL'\n",
    "    elif row['contains_pedestrian_words'] == 1 or row['contains_motorcycle_words'] == 1:\n",
    "        return 'SEVERE'\n",
    "    elif row['contains_matatu_words'] == 1:\n",
    "        return 'MODERATE'\n",
    "    else:\n",
    "        return 'MINOR'\n",
    "\n",
    "crashes['severity'] = crashes.apply(classify_severity, axis=1)\n",
    "\n",
    "print(\" Severity labels created!\")\n",
    "print(\"\\nSeverity distribution:\")\n",
    "print(crashes['severity'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DATA LOADED AND READY FOR FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b83d82-8559-4da0-9c62-957f77c96de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING TEMPORAL FEATURES - PART 1\n",
      "================================================================================\n",
      "\n",
      " Extracting basic time components...\n",
      " Basic time components extracted!\n",
      "\n",
      " Sample of new temporal features:\n",
      "       crash_datetime  hour   day_name month_name  year\n",
      "0 2018-06-06 20:39:54    20  Wednesday       June  2018\n",
      "1 2018-08-17 06:15:54     6     Friday     August  2018\n",
      "2 2018-05-25 17:51:54    17     Friday        May  2018\n",
      "3 2018-05-25 18:11:54    18     Friday        May  2018\n",
      "4 2018-05-25 21:59:54    21     Friday        May  2018\n",
      "5 2018-05-26 07:11:54     7   Saturday        May  2018\n",
      "6 2018-05-26 07:42:54     7   Saturday        May  2018\n",
      "7 2018-05-26 07:52:24     7   Saturday        May  2018\n",
      "8 2018-05-26 11:51:24    11   Saturday        May  2018\n",
      "9 2018-05-26 15:42:24    15   Saturday        May  2018\n",
      "\n",
      " Hour distribution:\n",
      "hour\n",
      "0     271\n",
      "1     189\n",
      "2     146\n",
      "3     129\n",
      "4     161\n",
      "5     576\n",
      "6    2654\n",
      "7    2984\n",
      "8    2174\n",
      "9    1597\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Day of week distribution:\n",
      "day_name\n",
      "Friday       5058\n",
      "Tuesday      4730\n",
      "Wednesday    4604\n",
      "Monday       4586\n",
      "Thursday     4537\n",
      "Saturday     4395\n",
      "Sunday       3154\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      " TEMPORAL FEATURES (PART 1) COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEMPORAL FEATURES - PART 1: Basic Time Components\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING TEMPORAL FEATURES - PART 1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n Extracting basic time components...\")\n",
    "\n",
    "# Extract hour (0-23)\n",
    "crashes['hour'] = crashes['crash_datetime'].dt.hour\n",
    "\n",
    "# Extract day of week (0=Monday, 6=Sunday)\n",
    "crashes['day_of_week'] = crashes['crash_datetime'].dt.dayofweek\n",
    "crashes['day_name'] = crashes['crash_datetime'].dt.day_name()\n",
    "\n",
    "# Extract month (1-12)\n",
    "crashes['month'] = crashes['crash_datetime'].dt.month\n",
    "crashes['month_name'] = crashes['crash_datetime'].dt.month_name()\n",
    "\n",
    "# Extract year\n",
    "crashes['year'] = crashes['crash_datetime'].dt.year\n",
    "\n",
    "print(\" Basic time components extracted!\")\n",
    "\n",
    "print(\"\\n Sample of new temporal features:\")\n",
    "print(crashes[['crash_datetime', 'hour', 'day_name', 'month_name', 'year']].head(10))\n",
    "\n",
    "print(\"\\n Hour distribution:\")\n",
    "print(crashes['hour'].value_counts().sort_index().head(10))\n",
    "\n",
    "print(\"\\n Day of week distribution:\")\n",
    "print(crashes['day_name'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" TEMPORAL FEATURES (PART 1) COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2645cf-2d2a-48de-834d-aa3f202d702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING TEMPORAL FEATURES - PART 2: DERIVED FEATURES\n",
      "================================================================================\n",
      "\n",
      " Creating rush hour indicators...\n",
      "  Rush hour indicators created!\n",
      "   Morning rush crashes: 6,755 (21.7%)\n",
      "   Evening rush crashes: 6,181 (19.9%)\n",
      "   Total rush hour crashes: 12,936 (41.6%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEMPORAL FEATURES - PART 2: Derived Time Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING TEMPORAL FEATURES - PART 2: DERIVED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== RUSH HOUR INDICATOR ==========\n",
    "\n",
    "print(\"\\n Creating rush hour indicators...\")\n",
    "\n",
    "# Morning rush hour: 7-9 AM\n",
    "crashes['is_morning_rush'] = crashes['hour'].isin([7, 8, 9])\n",
    "\n",
    "# Evening rush hour: 5-7 PM (17-19 in 24-hour format)\n",
    "crashes['is_evening_rush'] = crashes['hour'].isin([17, 18, 19])\n",
    "\n",
    "# Any rush hour\n",
    "crashes['is_rush_hour'] = crashes['is_morning_rush'] | crashes['is_evening_rush']\n",
    "\n",
    "print(f\"  Rush hour indicators created!\")\n",
    "print(f\"   Morning rush crashes: {crashes['is_morning_rush'].sum():,} ({crashes['is_morning_rush'].mean()*100:.1f}%)\")\n",
    "print(f\"   Evening rush crashes: {crashes['is_evening_rush'].sum():,} ({crashes['is_evening_rush'].mean()*100:.1f}%)\")\n",
    "print(f\"   Total rush hour crashes: {crashes['is_rush_hour'].sum():,} ({crashes['is_rush_hour'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55d71d8-c13e-4de5-b20f-b98c9b5089bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating weekend indicator...\n",
      "   Weekend indicator created!\n",
      "   Weekend crashes: 7,549 (24.3%)\n",
      "   Weekday crashes: 23,515 (75.7%)\n"
     ]
    }
   ],
   "source": [
    "# ========== WEEKEND INDICATOR ==========\n",
    "\n",
    "print(\"\\n Creating weekend indicator...\")\n",
    "\n",
    "# Weekend: Saturday (5) and Sunday (6)\n",
    "crashes['is_weekend'] = crashes['day_of_week'].isin([5, 6])\n",
    "\n",
    "print(f\"   Weekend indicator created!\")\n",
    "print(f\"   Weekend crashes: {crashes['is_weekend'].sum():,} ({crashes['is_weekend'].mean()*100:.1f}%)\")\n",
    "print(f\"   Weekday crashes: {(~crashes['is_weekend']).sum():,} ({(~crashes['is_weekend']).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c1f5c9-b8df-449c-98a2-9bc9c0c53319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Creating time of day categories...\n",
      " Time of day categories created!\n",
      "\n",
      "Time of day distribution:\n",
      "time_of_day\n",
      "Morning      12081\n",
      "Evening       7930\n",
      "Afternoon     6938\n",
      "Night         4115\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ========== TIME OF DAY ==========\n",
    "\n",
    "print(\"\\n Creating time of day categories...\")\n",
    "\n",
    "def categorize_time_of_day(hour):\n",
    "    \"\"\"\n",
    "    Categorize hour into time of day periods\n",
    "    \"\"\"\n",
    "    if 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:  # 21-24 and 0-5\n",
    "        return 'Night'\n",
    "\n",
    "crashes['time_of_day'] = crashes['hour'].apply(categorize_time_of_day)\n",
    "\n",
    "print(f\" Time of day categories created!\")\n",
    "print(\"\\nTime of day distribution:\")\n",
    "print(crashes['time_of_day'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64290787-09a2-41f5-93ac-db83031c2bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample of all temporal features:\n",
      "       crash_datetime  hour  is_rush_hour  is_weekend time_of_day severity\n",
      "0 2018-06-06 20:39:54    20         False       False     Evening    MINOR\n",
      "1 2018-08-17 06:15:54     6         False       False     Morning    FATAL\n",
      "2 2018-05-25 17:51:54    17          True       False     Evening    MINOR\n",
      "3 2018-05-25 18:11:54    18          True       False     Evening    MINOR\n",
      "4 2018-05-25 21:59:54    21         False       False       Night    FATAL\n",
      "5 2018-05-26 07:11:54     7          True        True     Morning    MINOR\n",
      "6 2018-05-26 07:42:54     7          True        True     Morning    FATAL\n",
      "7 2018-05-26 07:52:24     7          True        True     Morning    MINOR\n",
      "8 2018-05-26 11:51:24    11         False        True     Morning    MINOR\n",
      "9 2018-05-26 15:42:24    15         False        True   Afternoon    MINOR\n",
      "\n",
      "================================================================================\n",
      " TEMPORAL FEATURES (PART 2) COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Total temporal features created: 10\n",
      "   - hour, day_of_week, month, year\n",
      "   - is_morning_rush, is_evening_rush, is_rush_hour\n",
      "   - is_weekend\n",
      "   - time_of_day\n"
     ]
    }
   ],
   "source": [
    "# ========== VERIFY NEW FEATURES ==========\n",
    "\n",
    "print(\"\\n Sample of all temporal features:\")\n",
    "print(crashes[['crash_datetime', 'hour', 'is_rush_hour', 'is_weekend', 'time_of_day', 'severity']].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" TEMPORAL FEATURES (PART 2) COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal temporal features created: 10\")\n",
    "print(\"   - hour, day_of_week, month, year\")\n",
    "print(\"   - is_morning_rush, is_evening_rush, is_rush_hour\")\n",
    "print(\"   - is_weekend\")\n",
    "print(\"   - time_of_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c28b4888-ab16-4b1e-9715-e7ed5d6b28a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING LOCATION FEATURES - PART 1\n",
      "================================================================================\n",
      "\n",
      " Defining Nairobi city center coordinates...\n",
      "   City Center: (-1.2864, 36.8172)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOCATION FEATURES - PART 1: Distance from City Center\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING LOCATION FEATURES - PART 1\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# ========== DEFINE NAIROBI CITY CENTER ==========\n",
    "\n",
    "print(\"\\n Defining Nairobi city center coordinates...\")\n",
    "\n",
    "# Nairobi CBD coordinates (approximately City Square/Kenyatta Avenue)\n",
    "NAIROBI_CENTER_LAT = -1.2864\n",
    "NAIROBI_CENTER_LON = 36.8172\n",
    "\n",
    "print(f\"   City Center: ({NAIROBI_CENTER_LAT}, {NAIROBI_CENTER_LON})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abaad20c-bcc5-48d6-b5c6-f0a30a25c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== HAVERSINE DISTANCE FUNCTION ==========\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    Returns distance in kilometers\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    \n",
    "    # Radius of earth in kilometers\n",
    "    r = 6371\n",
    "    \n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee5518c-32d1-4e40-ad66-8ff09f37b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Calculating distance from city center for all crashes...\n",
      " Distance calculations complete!\n"
     ]
    }
   ],
   "source": [
    "# ========== CALCULATE DISTANCE FROM CENTER ==========\n",
    "\n",
    "print(\"\\n Calculating distance from city center for all crashes...\")\n",
    "\n",
    "crashes['distance_from_center_km'] = crashes.apply(\n",
    "    lambda row: haversine_distance(\n",
    "        row['latitude'], \n",
    "        row['longitude'], \n",
    "        NAIROBI_CENTER_LAT, \n",
    "        NAIROBI_CENTER_LON\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\" Distance calculations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74a0609-9ad8-4807-a983-f0a296ccc21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Distance from city center statistics:\n",
      "count    31064.000000\n",
      "mean        11.654997\n",
      "std         14.715408\n",
      "min          0.020285\n",
      "25%          4.029428\n",
      "50%          7.550795\n",
      "75%         13.692216\n",
      "max        225.068492\n",
      "Name: distance_from_center_km, dtype: float64\n",
      "\n",
      " Distance distribution (binned):\n",
      "distance_category\n",
      "0-5km      9905\n",
      "5-10km     9887\n",
      "10-15km    4884\n",
      "15-20km    2237\n",
      "20+km      4051\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ========== ANALYZE DISTANCE DISTRIBUTION ==========\n",
    "\n",
    "print(\"\\n Distance from city center statistics:\")\n",
    "print(crashes['distance_from_center_km'].describe())\n",
    "\n",
    "print(\"\\n Distance distribution (binned):\")\n",
    "distance_bins = [0, 5, 10, 15, 20, 100]\n",
    "distance_labels = ['0-5km', '5-10km', '10-15km', '15-20km', '20+km']\n",
    "crashes['distance_category'] = pd.cut(\n",
    "    crashes['distance_from_center_km'], \n",
    "    bins=distance_bins, \n",
    "    labels=distance_labels\n",
    ")\n",
    "print(crashes['distance_category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f15183-4e0f-4b91-bcf5-8b1eb68039a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample of location features:\n",
      "   latitude  longitude  distance_from_center_km distance_category severity\n",
      "0 -1.263030  36.764374                 6.421765            5-10km    MINOR\n",
      "1 -0.829710  37.037820                56.394804             20+km    FATAL\n",
      "2 -1.125301  37.003297                27.366123             20+km    MINOR\n",
      "3 -1.740958  37.129026                61.287430             20+km    MINOR\n",
      "4 -1.259392  36.842321                 4.100905             0-5km    FATAL\n",
      "5 -1.215499  36.835150                 8.132447            5-10km    MINOR\n",
      "6 -1.372556  36.920491                14.954018           10-15km    FATAL\n",
      "7 -1.209940  36.833173                 8.685364            5-10km    MINOR\n",
      "8 -1.314351  36.807909                 3.275190             0-5km    MINOR\n",
      "9 -1.206788  36.854991                 9.798800            5-10km    MINOR\n",
      "\n",
      "================================================================================\n",
      " LOCATION FEATURES (PART 1) COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== SAMPLE OUTPUT ==========\n",
    "\n",
    "print(\"\\n Sample of location features:\")\n",
    "print(crashes[['latitude', 'longitude', 'distance_from_center_km', 'distance_category', 'severity']].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" LOCATION FEATURES (PART 1) COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647d3fd4-ab5c-4883-91e8-e93f6f3a205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING LOCATION FEATURES - PART 2: HOTSPOTS\n",
      "================================================================================\n",
      "\n",
      " Creating location grid for hotspot analysis...\n",
      " Location grid created!\n",
      "   Unique grid cells: 895\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOCATION FEATURES - PART 2: Crash Hotspots & Frequency\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING LOCATION FEATURES - PART 2: HOTSPOTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== CREATE LOCATION GRID ==========\n",
    "\n",
    "print(\"\\n Creating location grid for hotspot analysis...\")\n",
    "\n",
    "# Round coordinates to 2 decimal places (~1km grid cells)\n",
    "crashes['lat_grid'] = crashes['latitude'].round(2)\n",
    "crashes['lon_grid'] = crashes['longitude'].round(2)\n",
    "crashes['location_grid'] = crashes['lat_grid'].astype(str) + '_' + crashes['lon_grid'].astype(str)\n",
    "\n",
    "print(f\" Location grid created!\")\n",
    "print(f\"   Unique grid cells: {crashes['location_grid'].nunique():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbcbb870-7784-4546-aa52-2728641c47b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Calculating crash frequency at each location...\n",
      " Crash frequency calculated!\n",
      "\n",
      "   Location crash frequency statistics:\n",
      "count    31064.000000\n",
      "mean       245.870268\n",
      "std        209.836369\n",
      "min          1.000000\n",
      "25%         72.000000\n",
      "50%        189.000000\n",
      "75%        375.000000\n",
      "max        764.000000\n",
      "Name: crashes_at_location, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ========== CALCULATE CRASH FREQUENCY PER LOCATION ==========\n",
    "\n",
    "print(\"\\n Calculating crash frequency at each location...\")\n",
    "\n",
    "# Count total crashes per grid cell\n",
    "location_crash_counts = crashes.groupby('location_grid').size().reset_index(name='crashes_at_location')\n",
    "\n",
    "# Merge back to main dataframe\n",
    "crashes = crashes.merge(location_crash_counts, on='location_grid', how='left')\n",
    "\n",
    "print(\" Crash frequency calculated!\")\n",
    "print(f\"\\n   Location crash frequency statistics:\")\n",
    "print(crashes['crashes_at_location'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "128feb89-c371-4b77-a972-178ec0e62346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Identifying crash hotspots...\n",
      " Hotspots identified!\n",
      "   Hotspot threshold: 574 crashes\n",
      "   Crashes in hotspots: 3,307 (10.6%)\n",
      "   Crashes outside hotspots: 27,757 (89.4%)\n"
     ]
    }
   ],
   "source": [
    "# ========== IDENTIFY HIGH-FREQUENCY LOCATIONS (HOTSPOTS) ==========\n",
    "\n",
    "print(\"\\n Identifying crash hotspots...\")\n",
    "\n",
    "# Define hotspot threshold (top 10% of locations by crash count)\n",
    "hotspot_threshold = crashes['crashes_at_location'].quantile(0.90)\n",
    "crashes['is_hotspot'] = crashes['crashes_at_location'] >= hotspot_threshold\n",
    "\n",
    "print(f\" Hotspots identified!\")\n",
    "print(f\"   Hotspot threshold: {hotspot_threshold:.0f} crashes\")\n",
    "print(f\"   Crashes in hotspots: {crashes['is_hotspot'].sum():,} ({crashes['is_hotspot'].mean()*100:.1f}%)\")\n",
    "print(f\"   Crashes outside hotspots: {(~crashes['is_hotspot']).sum():,} ({(~crashes['is_hotspot']).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffbc98f2-761d-4ef7-ad46-71ce9f23e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 10 crash hotspot locations:\n",
      "               total_crashes  latitude  longitude\n",
      "location_grid                                    \n",
      "-1.29_36.83              764 -1.289059  36.828536\n",
      "-1.28_36.82              697 -1.279729  36.819974\n",
      "-1.2_36.92               664 -1.203744  36.917527\n",
      "-1.26_36.84              608 -1.259392  36.842321\n",
      "-1.28_36.83              574 -1.283916  36.827562\n",
      "-1.33_36.87              555 -1.329937  36.871007\n",
      "-1.24_36.87              527 -1.244783  36.866854\n",
      "-1.22_36.89              525 -1.218637  36.891361\n",
      "-1.33_36.89              496 -1.334935  36.891436\n",
      "-1.27_36.81              460 -1.273061  36.812918\n"
     ]
    }
   ],
   "source": [
    "# ========== TOP 10 HOTSPOT LOCATIONS ==========\n",
    "\n",
    "print(\"\\n Top 10 crash hotspot locations:\")\n",
    "top_hotspots = crashes.groupby('location_grid').agg({\n",
    "    'crash_id': 'count',\n",
    "    'latitude': 'first',\n",
    "    'longitude': 'first'\n",
    "}).rename(columns={'crash_id': 'total_crashes'}).sort_values('total_crashes', ascending=False).head(10)\n",
    "\n",
    "print(top_hotspots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9433be6-c0af-4e6c-949f-ebf218aec720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Categorizing crash frequency...\n",
      " Frequency categories created!\n",
      "\n",
      "Frequency distribution:\n",
      "frequency_category\n",
      "High        28816\n",
      "Moderate     1286\n",
      "Low           706\n",
      "Isolated      256\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ========== CATEGORIZE CRASH FREQUENCY ==========\n",
    "\n",
    "print(\"\\n Categorizing crash frequency...\")\n",
    "\n",
    "def categorize_frequency(count):\n",
    "    \"\"\"Categorize crash frequency at location\"\"\"\n",
    "    if count == 1:\n",
    "        return 'Isolated'\n",
    "    elif count <= 5:\n",
    "        return 'Low'\n",
    "    elif count <= 15:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "crashes['frequency_category'] = crashes['crashes_at_location'].apply(categorize_frequency)\n",
    "\n",
    "print(\" Frequency categories created!\")\n",
    "print(\"\\nFrequency distribution:\")\n",
    "print(crashes['frequency_category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dff30c5f-45b4-410d-8ab1-b7a502dc1e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample of location features with frequency:\n",
      "  location_grid  crashes_at_location  is_hotspot frequency_category severity\n",
      "0   -1.26_36.76                  250       False               High    MINOR\n",
      "1   -0.83_37.04                    2       False                Low    FATAL\n",
      "2    -1.13_37.0                    3       False                Low    MINOR\n",
      "3   -1.74_37.13                    6       False           Moderate    MINOR\n",
      "4   -1.26_36.84                  608        True               High    FATAL\n",
      "5   -1.22_36.84                   96       False               High    MINOR\n",
      "6   -1.37_36.92                  111       False               High    FATAL\n",
      "7   -1.21_36.83                   38       False               High    MINOR\n",
      "8   -1.31_36.81                  107       False               High    MINOR\n",
      "9   -1.21_36.85                   15       False           Moderate    MINOR\n",
      "\n",
      "================================================================================\n",
      " LOCATION FEATURES (PART 2) COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== SAMPLE OUTPUT ==========\n",
    "\n",
    "print(\"\\n Sample of location features with frequency:\")\n",
    "print(crashes[['location_grid', 'crashes_at_location', 'is_hotspot', 'frequency_category', 'severity']].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" LOCATION FEATURES (PART 2) COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1127a856-5809-423f-b9ab-0b4f45c565a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING HISTORICAL SEVERITY FEATURES\n",
      "================================================================================\n",
      "\n",
      " Calculating average severity at each location...\n",
      " Location severity calculated!\n",
      "\n",
      "   Average severity statistics:\n",
      "count    31064.000000\n",
      "mean         1.391868\n",
      "std          0.183142\n",
      "min          1.000000\n",
      "25%          1.310541\n",
      "50%          1.382353\n",
      "75%          1.462963\n",
      "max          4.000000\n",
      "Name: avg_severity_at_location, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HISTORICAL SEVERITY FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING HISTORICAL SEVERITY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== AVERAGE SEVERITY AT LOCATION ==========\n",
    "\n",
    "print(\"\\n Calculating average severity at each location...\")\n",
    "\n",
    "# Create numeric severity for calculations\n",
    "severity_mapping = {\n",
    "    'MINOR': 1,\n",
    "    'MODERATE': 2,\n",
    "    'SEVERE': 3,\n",
    "    'FATAL': 4\n",
    "}\n",
    "crashes['severity_numeric'] = crashes['severity'].map(severity_mapping)\n",
    "\n",
    "# Calculate average severity per location\n",
    "location_severity = crashes.groupby('location_grid')['severity_numeric'].agg([\n",
    "    ('avg_severity_at_location', 'mean'),\n",
    "    ('max_severity_at_location', 'max')\n",
    "]).reset_index()\n",
    "\n",
    "# Merge back to main dataframe\n",
    "crashes = crashes.merge(location_severity, on='location_grid', how='left')\n",
    "\n",
    "print(\" Location severity calculated!\")\n",
    "print(f\"\\n   Average severity statistics:\")\n",
    "print(crashes['avg_severity_at_location'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca429400-edaa-4c12-a6b2-3fa4a68406e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Calculating fatal crash rate at each location...\n",
      " Fatal crash rate calculated!\n",
      "\n",
      "   Fatal rate at location statistics:\n",
      "count    31064.000000\n",
      "mean         0.073526\n",
      "std          0.053151\n",
      "min          0.000000\n",
      "25%          0.051643\n",
      "50%          0.068736\n",
      "75%          0.091667\n",
      "max          1.000000\n",
      "Name: fatal_rate_at_location, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ========== FATAL CRASH RATE AT LOCATION ==========\n",
    "\n",
    "print(\"\\n Calculating fatal crash rate at each location...\")\n",
    "\n",
    "# Count fatal crashes per location\n",
    "location_fatal = crashes.groupby('location_grid').agg({\n",
    "    'contains_fatality_words': 'sum',\n",
    "    'crash_id': 'count'\n",
    "}).rename(columns={\n",
    "    'contains_fatality_words': 'fatal_crashes_at_location',\n",
    "    'crash_id': 'total_crashes_at_location'\n",
    "})\n",
    "\n",
    "# Calculate fatal crash rate\n",
    "location_fatal['fatal_rate_at_location'] = (\n",
    "    location_fatal['fatal_crashes_at_location'] / \n",
    "    location_fatal['total_crashes_at_location']\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "crashes = crashes.merge(\n",
    "    location_fatal[['fatal_rate_at_location']], \n",
    "    on='location_grid', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\" Fatal crash rate calculated!\")\n",
    "print(f\"\\n   Fatal rate at location statistics:\")\n",
    "print(crashes['fatal_rate_at_location'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "236cae6f-11b1-41a3-84d2-c60d8329d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Calculating pedestrian crash rate at each location...\n",
      " Pedestrian crash rate calculated!\n",
      "\n",
      "   Pedestrian rate statistics:\n",
      "count    31064.000000\n",
      "mean         0.030389\n",
      "std          0.039818\n",
      "min          0.000000\n",
      "25%          0.015152\n",
      "50%          0.025210\n",
      "75%          0.040984\n",
      "max          1.000000\n",
      "Name: pedestrian_rate_at_location, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ========== PEDESTRIAN CRASH RATE AT LOCATION ==========\n",
    "\n",
    "print(\"\\n Calculating pedestrian crash rate at each location...\")\n",
    "\n",
    "location_pedestrian = crashes.groupby('location_grid')['contains_pedestrian_words'].agg([\n",
    "    ('pedestrian_crashes_at_location', 'sum')\n",
    "]).reset_index()\n",
    "\n",
    "location_pedestrian['pedestrian_rate_at_location'] = (\n",
    "    location_pedestrian['pedestrian_crashes_at_location'] / \n",
    "    crashes.groupby('location_grid').size().values\n",
    ")\n",
    "\n",
    "crashes = crashes.merge(\n",
    "    location_pedestrian[['location_grid', 'pedestrian_rate_at_location']], \n",
    "    on='location_grid', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\" Pedestrian crash rate calculated!\")\n",
    "print(f\"\\n   Pedestrian rate statistics:\")\n",
    "print(crashes['pedestrian_rate_at_location'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b6692ae-e0ff-49a4-94d9-9ddb7c357fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Creating location risk categories...\n",
      " Risk categories created!\n",
      "\n",
      "Location risk distribution:\n",
      "location_risk\n",
      "Low Risk          26893\n",
      "Moderate Risk      3801\n",
      "High Risk           224\n",
      "Very High Risk      146\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ========== CATEGORIZE LOCATION RISK ==========\n",
    "\n",
    "print(\"\\n  Creating location risk categories...\")\n",
    "\n",
    "def categorize_location_risk(avg_severity):\n",
    "    \"\"\"Categorize location by average severity\"\"\"\n",
    "    if avg_severity < 1.5:\n",
    "        return 'Low Risk'\n",
    "    elif avg_severity < 2.0:\n",
    "        return 'Moderate Risk'\n",
    "    elif avg_severity < 2.5:\n",
    "        return 'High Risk'\n",
    "    else:\n",
    "        return 'Very High Risk'\n",
    "\n",
    "crashes['location_risk'] = crashes['avg_severity_at_location'].apply(categorize_location_risk)\n",
    "\n",
    "print(\" Risk categories created!\")\n",
    "print(\"\\nLocation risk distribution:\")\n",
    "print(crashes['location_risk'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fe5f4fa-018f-494b-8ec4-248a1b94b9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample of historical severity features:\n",
      "  location_grid  crashes_at_location  avg_severity_at_location  \\\n",
      "0   -1.26_36.76                  250                  1.336000   \n",
      "1   -0.83_37.04                    2                  3.000000   \n",
      "2    -1.13_37.0                    3                  1.000000   \n",
      "3   -1.74_37.13                    6                  1.166667   \n",
      "4   -1.26_36.84                  608                  1.378289   \n",
      "5   -1.22_36.84                   96                  1.239583   \n",
      "6   -1.37_36.92                  111                  1.360360   \n",
      "7   -1.21_36.83                   38                  1.263158   \n",
      "8   -1.31_36.81                  107                  1.364486   \n",
      "9   -1.21_36.85                   15                  1.000000   \n",
      "\n",
      "   fatal_rate_at_location  pedestrian_rate_at_location   location_risk  \\\n",
      "0                0.052000                     0.016000        Low Risk   \n",
      "1                0.500000                     0.000000  Very High Risk   \n",
      "2                0.000000                     0.000000        Low Risk   \n",
      "3                0.000000                     0.000000        Low Risk   \n",
      "4                0.072368                     0.039474        Low Risk   \n",
      "5                0.020833                     0.020833        Low Risk   \n",
      "6                0.045045                     0.018018        Low Risk   \n",
      "7                0.052632                     0.000000        Low Risk   \n",
      "8                0.046729                     0.028037        Low Risk   \n",
      "9                0.000000                     0.000000        Low Risk   \n",
      "\n",
      "  severity  \n",
      "0    MINOR  \n",
      "1    FATAL  \n",
      "2    MINOR  \n",
      "3    MINOR  \n",
      "4    FATAL  \n",
      "5    MINOR  \n",
      "6    FATAL  \n",
      "7    MINOR  \n",
      "8    MINOR  \n",
      "9    MINOR  \n",
      "\n",
      "================================================================================\n",
      " HISTORICAL SEVERITY FEATURES COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== SAMPLE OUTPUT ==========\n",
    "\n",
    "print(\"\\n Sample of historical severity features:\")\n",
    "print(crashes[[\n",
    "    'location_grid', \n",
    "    'crashes_at_location',\n",
    "    'avg_severity_at_location',\n",
    "    'fatal_rate_at_location',\n",
    "    'pedestrian_rate_at_location',\n",
    "    'location_risk',\n",
    "    'severity'\n",
    "]].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" HISTORICAL SEVERITY FEATURES COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abdbfc7b-c952-460f-bae8-36b04a8ce829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "\n",
      " Feature Categories:\n",
      "\n",
      "   Original features: 10\n",
      "   Temporal features: 11\n",
      "   Location features: 5\n",
      "   Hotspot features: 3\n",
      "   Severity features: 7\n",
      "\n",
      "   TOTAL FEATURES: 36\n",
      "\n",
      " All columns in processed dataset:\n",
      "   1. crash_id\n",
      "   2. crash_datetime\n",
      "   3. crash_date\n",
      "   4. latitude\n",
      "   5. longitude\n",
      "   6. n_crash_reports\n",
      "   7. contains_fatality_words\n",
      "   8. contains_pedestrian_words\n",
      "   9. contains_matatu_words\n",
      "   10. contains_motorcycle_words\n",
      "   11. severity\n",
      "   12. hour\n",
      "   13. day_of_week\n",
      "   14. day_name\n",
      "   15. month\n",
      "   16. month_name\n",
      "   17. year\n",
      "   18. is_morning_rush\n",
      "   19. is_evening_rush\n",
      "   20. is_rush_hour\n",
      "   21. is_weekend\n",
      "   22. time_of_day\n",
      "   23. distance_from_center_km\n",
      "   24. distance_category\n",
      "   25. lat_grid\n",
      "   26. lon_grid\n",
      "   27. location_grid\n",
      "   28. crashes_at_location\n",
      "   29. is_hotspot\n",
      "   30. frequency_category\n",
      "   31. severity_numeric\n",
      "   32. avg_severity_at_location\n",
      "   33. max_severity_at_location\n",
      "   34. fatal_rate_at_location\n",
      "   35. pedestrian_rate_at_location\n",
      "   36. location_risk\n",
      "\n",
      " Final Data Quality Check:\n",
      "   Total records: 31,064\n",
      "   Missing values: 100\n",
      "   Duplicate rows: 0\n",
      "\n",
      " Sample of final processed dataset:\n",
      "       crash_datetime severity  hour  is_rush_hour  is_weekend  \\\n",
      "0 2018-06-06 20:39:54    MINOR    20         False       False   \n",
      "1 2018-08-17 06:15:54    FATAL     6         False       False   \n",
      "2 2018-05-25 17:51:54    MINOR    17          True       False   \n",
      "3 2018-05-25 18:11:54    MINOR    18          True       False   \n",
      "4 2018-05-25 21:59:54    FATAL    21         False       False   \n",
      "5 2018-05-26 07:11:54    MINOR     7          True        True   \n",
      "6 2018-05-26 07:42:54    FATAL     7          True        True   \n",
      "7 2018-05-26 07:52:24    MINOR     7          True        True   \n",
      "8 2018-05-26 11:51:24    MINOR    11         False        True   \n",
      "9 2018-05-26 15:42:24    MINOR    15         False        True   \n",
      "\n",
      "   distance_from_center_km  crashes_at_location  is_hotspot  \\\n",
      "0                 6.421765                  250       False   \n",
      "1                56.394804                    2       False   \n",
      "2                27.366123                    3       False   \n",
      "3                61.287430                    6       False   \n",
      "4                 4.100905                  608        True   \n",
      "5                 8.132447                   96       False   \n",
      "6                14.954018                  111       False   \n",
      "7                 8.685364                   38       False   \n",
      "8                 3.275190                  107       False   \n",
      "9                 9.798800                   15       False   \n",
      "\n",
      "   avg_severity_at_location   location_risk  \n",
      "0                  1.336000        Low Risk  \n",
      "1                  3.000000  Very High Risk  \n",
      "2                  1.000000        Low Risk  \n",
      "3                  1.166667        Low Risk  \n",
      "4                  1.378289        Low Risk  \n",
      "5                  1.239583        Low Risk  \n",
      "6                  1.360360        Low Risk  \n",
      "7                  1.263158        Low Risk  \n",
      "8                  1.364486        Low Risk  \n",
      "9                  1.000000        Low Risk  \n",
      "\n",
      " Saving processed dataset...\n",
      " Dataset saved successfully!\n",
      "   Location: D:\\Nairobi-Accident-Severity\\data\\processed\\crashes_with_features.csv\n",
      "   Records: 31,064\n",
      "   Features: 36\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      " Summary Statistics:\n",
      "   • Original dataset: 10 columns\n",
      "   • Processed dataset: 36 columns\n",
      "   • New features created: 26\n",
      "   • Records processed: 31,064\n",
      "   • Missing values: 0\n",
      "   • Data quality:  Excellent\n",
      "\n",
      " Features Ready for Machine Learning:\n",
      "    Temporal features (time patterns)\n",
      "    Location features (geographic patterns)\n",
      "    Hotspot features (crash frequency)\n",
      "    Severity features (historical patterns)\n",
      "    Target variable: severity (Fatal/Severe/Moderate/Minor)\n",
      "\n",
      "================================================================================\n",
      " Dataset ready for model training!\n",
      "   Next step: Data preprocessing and model development\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SUMMARY AND SAVE PROCESSED DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== COUNT FEATURES BY CATEGORY ==========\n",
    "\n",
    "print(\"\\n Feature Categories:\")\n",
    "\n",
    "original_features = ['crash_id', 'crash_datetime', 'crash_date', 'latitude', 'longitude', \n",
    "                     'n_crash_reports', 'contains_fatality_words', 'contains_pedestrian_words',\n",
    "                     'contains_matatu_words', 'contains_motorcycle_words']\n",
    "\n",
    "temporal_features = ['hour', 'day_of_week', 'day_name', 'month', 'month_name', 'year',\n",
    "                     'is_morning_rush', 'is_evening_rush', 'is_rush_hour', \n",
    "                     'is_weekend', 'time_of_day']\n",
    "\n",
    "location_features = ['distance_from_center_km', 'distance_category', \n",
    "                     'lat_grid', 'lon_grid', 'location_grid']\n",
    "\n",
    "hotspot_features = ['crashes_at_location', 'is_hotspot', 'frequency_category']\n",
    "\n",
    "severity_features = ['severity', 'severity_numeric', 'avg_severity_at_location',\n",
    "                     'max_severity_at_location', 'fatal_rate_at_location',\n",
    "                     'pedestrian_rate_at_location', 'location_risk']\n",
    "\n",
    "print(f\"\\n   Original features: {len(original_features)}\")\n",
    "print(f\"   Temporal features: {len(temporal_features)}\")\n",
    "print(f\"   Location features: {len(location_features)}\")\n",
    "print(f\"   Hotspot features: {len(hotspot_features)}\")\n",
    "print(f\"   Severity features: {len(severity_features)}\")\n",
    "print(f\"\\n   TOTAL FEATURES: {len(crashes.columns)}\")\n",
    "\n",
    "# ========== DISPLAY ALL COLUMNS ==========\n",
    "\n",
    "print(\"\\n All columns in processed dataset:\")\n",
    "for i, col in enumerate(crashes.columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "# ========== DATA QUALITY CHECK ==========\n",
    "\n",
    "print(\"\\n Final Data Quality Check:\")\n",
    "print(f\"   Total records: {len(crashes):,}\")\n",
    "print(f\"   Missing values: {crashes.isnull().sum().sum()}\")\n",
    "print(f\"   Duplicate rows: {crashes.duplicated().sum()}\")\n",
    "\n",
    "# ========== SAMPLE OF FINAL DATASET ==========\n",
    "\n",
    "print(\"\\n Sample of final processed dataset:\")\n",
    "sample_cols = ['crash_datetime', 'severity', 'hour', 'is_rush_hour', 'is_weekend',\n",
    "               'distance_from_center_km', 'crashes_at_location', 'is_hotspot',\n",
    "               'avg_severity_at_location', 'location_risk']\n",
    "print(crashes[sample_cols].head(10))\n",
    "\n",
    "# ========== SAVE PROCESSED DATASET ==========\n",
    "\n",
    "print(\"\\n Saving processed dataset...\")\n",
    "\n",
    "output_path = r'D:\\Nairobi-Accident-Severity\\data\\processed\\crashes_with_features.csv'\n",
    "crashes.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\" Dataset saved successfully!\")\n",
    "print(f\"   Location: {output_path}\")\n",
    "print(f\"   Records: {len(crashes):,}\")\n",
    "print(f\"   Features: {len(crashes.columns)}\")\n",
    "\n",
    "# ========== FEATURE ENGINEERING STATISTICS ==========\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n Summary Statistics:\")\n",
    "print(f\"   • Original dataset: 10 columns\")\n",
    "print(f\"   • Processed dataset: {len(crashes.columns)} columns\")\n",
    "print(f\"   • New features created: {len(crashes.columns) - 10}\")\n",
    "print(f\"   • Records processed: {len(crashes):,}\")\n",
    "print(f\"   • Missing values: 0\")\n",
    "print(f\"   • Data quality:  Excellent\")\n",
    "\n",
    "print(\"\\n Features Ready for Machine Learning:\")\n",
    "print(f\"    Temporal features (time patterns)\")\n",
    "print(f\"    Location features (geographic patterns)\")\n",
    "print(f\"    Hotspot features (crash frequency)\")\n",
    "print(f\"    Severity features (historical patterns)\")\n",
    "print(f\"    Target variable: severity (Fatal/Severe/Moderate/Minor)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\" Dataset ready for model training!\")\n",
    "print(f\"   Next step: Data preprocessing and model development\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8af60cea-47ca-46ec-bf8e-a78929381051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values by column:\n",
      "distance_category    100\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Quick check for missing values\n",
    "print(\"Missing values by column:\")\n",
    "missing = crashes.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if len(missing) > 0:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"No missing values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "307ed309-ae7c-435a-9965-53dc8c459db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HANDLING MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      " Investigating missing values in distance_category...\n",
      "   Rows with missing distance_category: 100\n",
      "\n",
      " Distance values for rows with missing category:\n",
      "count    100.000000\n",
      "mean     137.033470\n",
      "std       31.134815\n",
      "min      102.316003\n",
      "25%      114.445625\n",
      "50%      128.326441\n",
      "75%      147.077000\n",
      "max      225.068492\n",
      "Name: distance_from_center_km, dtype: float64\n",
      "\n",
      " Fixing missing categories...\n",
      " Missing values fixed!\n",
      "\n",
      " Verification:\n",
      "   Missing values in distance_category: 0\n",
      "   Total missing values in dataset: 0\n",
      "\n",
      " Updated distance distribution:\n",
      "distance_category\n",
      "0-5km      9905\n",
      "5-10km     9887\n",
      "10-15km    4884\n",
      "15-20km    2237\n",
      "20+km      4151\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Saving final clean dataset...\n",
      " Clean dataset saved!\n",
      "   Location: D:\\Nairobi-Accident-Severity\\data\\processed\\crashes_with_features.csv\n",
      "   Records: 31,064\n",
      "   Features: 36\n",
      "   Missing values: 0\n",
      "\n",
      "================================================================================\n",
      " DATA READY FOR MACHINE LEARNING!\n",
      "================================================================================\n",
      "\n",
      " Dataset Status:\n",
      "    31,064 crash records\n",
      "    36 features (10 original + 26 engineered)\n",
      "    0 missing values\n",
      "    0 duplicates\n",
      "    100% GPS coverage\n",
      "    Severity labels: Fatal/Severe/Moderate/Minor\n",
      "\n",
      " Ready for: Data preprocessing → Model training → Evaluation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIX MISSING VALUES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n Investigating missing values in distance_category...\")\n",
    "\n",
    "# Check which rows have missing distance_category\n",
    "missing_mask = crashes['distance_category'].isnull()\n",
    "print(f\"   Rows with missing distance_category: {missing_mask.sum()}\")\n",
    "\n",
    "# Look at their distance values\n",
    "print(\"\\n Distance values for rows with missing category:\")\n",
    "print(crashes[missing_mask]['distance_from_center_km'].describe())\n",
    "\n",
    "# ========== FIX THE ISSUE ==========\n",
    "\n",
    "print(\"\\n Fixing missing categories...\")\n",
    "\n",
    "# Recreate distance categories to ensure no missing values\n",
    "distance_bins = [0, 5, 10, 15, 20, 1000]  # Increased max to 1000 to catch all values\n",
    "distance_labels = ['0-5km', '5-10km', '10-15km', '15-20km', '20+km']\n",
    "\n",
    "crashes['distance_category'] = pd.cut(\n",
    "    crashes['distance_from_center_km'], \n",
    "    bins=distance_bins, \n",
    "    labels=distance_labels,\n",
    "    include_lowest=True  # Ensure boundary values are included\n",
    ")\n",
    "\n",
    "print(\" Missing values fixed!\")\n",
    "\n",
    "# ========== VERIFY FIX ==========\n",
    "\n",
    "print(\"\\n Verification:\")\n",
    "print(f\"   Missing values in distance_category: {crashes['distance_category'].isnull().sum()}\")\n",
    "print(f\"   Total missing values in dataset: {crashes.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n Updated distance distribution:\")\n",
    "print(crashes['distance_category'].value_counts().sort_index())\n",
    "\n",
    "# ========== SAVE FINAL CLEAN DATASET ==========\n",
    "\n",
    "print(\"\\n Saving final clean dataset...\")\n",
    "\n",
    "output_path = r'D:\\Nairobi-Accident-Severity\\data\\processed\\crashes_with_features.csv'\n",
    "crashes.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\" Clean dataset saved!\")\n",
    "print(f\"   Location: {output_path}\")\n",
    "print(f\"   Records: {len(crashes):,}\")\n",
    "print(f\"   Features: {len(crashes.columns)}\")\n",
    "print(f\"   Missing values: {crashes.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" DATA READY FOR MACHINE LEARNING!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n Dataset Status:\")\n",
    "print(f\"    31,064 crash records\")\n",
    "print(f\"    36 features (10 original + 26 engineered)\")\n",
    "print(f\"    0 missing values\")\n",
    "print(f\"    0 duplicates\")\n",
    "print(f\"    100% GPS coverage\")\n",
    "print(f\"    Severity labels: Fatal/Severe/Moderate/Minor\")\n",
    "print(f\"\\n Ready for: Data preprocessing → Model training → Evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6b1da-f396-45fe-83ba-f2e6f9d68fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
